file_name,content
Harim Lee et al 2021 - Generation of Modern Satellite Data from Galileo Sunspot Drawings in 1612 by Deep Learning.pdf,"# Generation of Modern Satellite Data from Galileo Sunspot Drawings in 1612 by Deep Learning 

Harim Lee ${ }^{1}$ (D) , Eunsu Park ${ }^{1}$ (D) , and Yong-Jae Moon ${ }^{1,2}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2020 August 14; revised 2020 November 23; accepted 2020 November 25; published 2021 February 5


#### Abstract

Historical sunspot drawings are very important resources for understanding past solar activity. We generate solar magnetograms and EUV images from Galileo sunspot drawings using a deep learning model based on conditional generative adversarial networks. We train the model using pairs of sunspot drawings from the Mount Wilson Observatory and their corresponding magnetograms (or UV/EUV images) from 2011 to 2015 except for every June and December by the Solar Dynamic Observatory satellite. We evaluate the model by comparing pairs of actual magnetograms (or UV/EUV images) and the corresponding AI-generated ones in June and December. Our results show that bipolar structures of the AI-generated magnetograms are consistent with those of the original ones and their unsigned magnetic fluxes (or intensities) are consistent with those of the original ones. Applying this model to the Galileo sunspot drawings in 1612, we generate Helioseismic and Magnetic Imager-like magnetograms and Atmospheric Imaging Assembly-like EUV images of the sunspots. We hope that the EUV intensities can be used for estimating solar EUV irradiance at long-term historical times.


Unified Astronomy Thesaurus concepts: The Sun (1693); Sunspots (1653); Sunspot cycle (1650); Solar active regions (1974); Solar magnetic fields (1503)

## 1. Introduction

Sunspot drawing is an important resource for understanding the long-term variation of solar activity (Vaquero 2007; Arlt \& Vaquero 2020). In particular, a record of sunspot numbers and their locations played an important role in discovering the 11 yr solar cycle and solar rotation (Hoyt \& Schatten 1998a, 1998b; Usoskin et al. 2003a; Casas et al. 2006). Furthermore, the relationship between solar activity and the Earth's climate has been studied by estimating magnetic field strength and solar irradiance through simple physical models using historical sunspot number and area information (Lockwood et al. 1999; Usoskin et al. 2003b; Solanki \& Krivova 2004; Zharkov \& Zharkova 2006; Krivova et al. 2007; Nagovitsyn et al. 2016; Pevtsov et al. 2019).

Nowadays, it is possible to conduct extensive research on solar activity using high-quality solar satellite observations (Solanki 2003; Munoz-Jaramillo \& Vaquero 2019). We propose a new way to conduct solar long-term variation studies using a deep learning model that transforms historical sunspot drawings into modern satellite images and their physical parameters. This study is expected to serve as a bridge between historical sunspot drawing data and modern satellite data.

For this we adopt a deep learning model, which is a general purposed solution for the image-to-image translation, named ""pix2pix"" (Isola et al. 2016), to sunspot drawings, in order to generate solar magnetograms and UV/EUV images. For this we make eight deep learning models for image translations. For training and evaluation data sets, we consider pairs of the Mount Wilson Observatory (MWO) sunspot drawings and Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) line-of-sight magnetograms from 2011 to 2015. We also consider seven sets of data: pairs of the sunspot drawings and Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) seven wavelength images.

This study is organized as follows. The data will be described in Section 2, and the method in Section 3. Results
and a discussion are present in Section 4 and our conclusion is given in Section 5.

## 2. Data

We use the MWO sunspot drawings from 2011 to 2015 for input data. First, we align the sunspot drawings with the corresponding SDO magnetograms and EUV/UV images. Then, we manually remove all letters and lines except for sunspots. In order to use relatively clear sunspot drawings, we make 8 bit scale images as follows (Figure 1(a)): 255 for solar disk and 0 for umbra, penumbra, and outside of solar disk.

We use SDO/HMI line-of-sight magnetograms and SDO/ AIA seven wavelength images ( $94,131,171,193,211,304$, and $335 \AA$ ) for target data. We first make level 1.5 images by calibrating, rotating, and centering the images. We divide the data numbers of all AIA images by exposure time to make a homogeneous exposure condition ( $\mathrm{DN} \mathrm{s}^{-1}$ ). To compensate for the instrument degradation over time for seven AIA EUV passbands (Boerner et al. 2012), we apply their degrading factors to the images using a SolarSoft routine (aia_get_response.pro) with a reference date of 2011 January 1. Also, we coalign the AIA and HMI images by fixing the solar disk size, and downsample the images to 512 by 512 . Magnetic flux densities are considered within $\pm 1000$ Gauss. Since sunspot drawings only represent strong magnetic fields, we calculate the total unsigned magnetic fluxes (TUMFs) of magnetograms for only strong field areas whose absolute field strengths are larger than 50 Gauss, which approximately correspond to 5 times of noise levels (Liu et al. 2012).

We use an AIA image with the range of $0 \mathrm{DN} \mathrm{s}^{-1}$ for minimum and $2^{6}-1 \sim 2^{13}-1\left(\mathrm{DN} \mathrm{s}^{-1}\right)$ for maximum. The maximum value of each passband is determined by the brightness in active regions without flares (see Table 5 of Boerner et al. 2012).

We exclude image pairs with poor quality: e.g., images that are too noisy because of solar flares, incorrect header information,
![img-0.jpeg](img-0.jpeg)

Figure 1. An example of input and target images for test and application. (a) Mount Wilson sunspot drawing, which is an input image on 2014 June 8. (b) The corresponding SDO/HMI image as the target, which is byte-scaled with $\pm 500$ Gauss for only display. (c) Galileo sunspot drawing with rotation correction on 1612 June 2. (d) Its modified image for input data.
atypical images due to reasons such as the eclipse of a planet. We adopt SDO data, which were observed within $\pm 36$ minutes of the observation time of each sunspot drawing, in order to minimize the effect of solar rotation. This time corresponds to 1.5 pixels by solar rotation. As a result, we make eight data sets, which include 1250 pairs of each data set: MWO sunspot drawings and SDO images. For training we use 1046 pairs from 2011 to 2015 except for every June and December. For evaluating our model, the remaining 204 pairs are used. Figure 1 shows an example of input image, target one, and Galileo sunspot drawing. Figure 1(a) is an MWO sunspot drawing (input) on 2014 June 8. Figure 1(b) is the corresponding SDO/HMI image (target) at 15:45 UT on 2014 June 8.

For application of our model, we use the 35 Galileo sunspot drawings (Galilei et al. 1613) processed by Al Van Helden and Owen Gingerich from 1612 June 2 to July 8 (images are available at galileo.rice.edu). First, we align the Galileo sunspot drawings with the MWO sunspot drawings, after correcting the rotational axis so north is up. We use the rotational degree from the electronic supplementary material of Vokhmyanin \& Zolotova (2018). Then,
we remove the letters and make 8 bit images as we did for our training and test data. Figure 1(c) is the Galileo sunspot drawing with rotation correction on 1612 June 2 and Figure 1(d) is an input image for our model.

## 3. Method

We adopt a deep learning model based on pix2pix. The pix2pix is based on the generative adversarial network (GAN; Goodfellow et al. 2014), which is an novel deep learning algorithm for the generation tasks. The pix2pix is a combination of the conditional generative adversarial network (cGAN; Mirza \& Osindero 2014) and the deep convolutional generative adversarial network (DCGAN; Radford et al. 2015). Our model consists of two networks: one is a generator and the other is a discriminator. The rule of the generator is to generate a target-like image from an input image by minimizing the difference between the target image and the generated one. The rule of the discriminator is to distinguish the real pair from the generated pair. The real pair
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparison between real SDO images and the generated ones at 15:45 UT on 2014 June 8. The first columns represent SDO/HMI and SDO/AIA images, the second columns are AI-generated images. Here two magnetograms are byte-scaled by ±500 Gauss for display only.

consists of the input image and the target one. The generated pair consists of the input image and the one generated by our model.

The pix2pix, proposed by Isola et al. (2016), uses 256 × 256 size images for training. We modify the data pipeline and depth of the generator network because the size of our data is 512 × 512. The loss function and other hyper parameters are the same as those of Isola et al. (2016). For each translation from sunspot drawings to a specific type of solar image, we make one deep learning model.

We save the generator (and the discriminator) every 10,000 iterations to check the training process, to avoid over-training, and to find the best model. The best model is taken when it gives the highest mean correlation coefficient (CC) value for the evaluation data set. We empirically find that the models are sufficiently trained before 210,000 iterations (~200 epochs). Here one iteration is when one pair of images is trained in our model, and one epoch is when an entire training data set of 1046 pairs is done in our model.

### 4. Results and Discussion

Figure 2 shows eight pairs of target images and their corresponding AI-generated ones at 15:45 UT on 2014 June 8.
![img-2.jpeg](img-2.jpeg)

**Figure 3.** Temporal variations of the total unsigned magnetic flux (a) and full-disk count rates (b) and (c) from 2011 to 2015. The solid lines show variations from the real images and dashed lines correspond to those from the generated ones. Small vertical space is given the period of training.

A comparison between target and AI-generated magnetograms shows that the bipolar structures of the HMI magnetograms are approximately restored. Even though we do not have any prior conditions such as preceding or following sunspots for sunspot drawings, bipolar structures in AI-generated magnetograms mostly follow Hale's law (Hale & Nicholson 1925). Our model learns the polarity pattern in the training step, then reproduces such a pattern in the evaluation and the generation step. It makes sense in that most of the active regions follow Hale's law. However, our model does not successfully generate active regions that do not follow Hale's law. Note that the polarity of the solar magnetic field is reversed cycle by cycle. Since all data are from the twenty-fourth solar cycle, there is no problem producing the Hale's law pattern in this cycle. Hence, our model would be
![img-3.jpeg](img-3.jpeg)

Figure 4. Results of our model when the Galileo sunspot drawings are used for input data. (a) AI-generated magnetogram and UV/EUV images from the Galileo sunspot drawing on 1612 June 2. (b) Total unsigned magnetic flux and full-disk count rates estimated from the AI-generated Galileo magnetogram and UV/EUV images from 1612 June 2 to July 8.

Effective for even solar cycles, but should be tested or the reversed polarity for odd cycles. For further discussion on this issue, please refer to Kim et al. (2019). As seen in the figure, the UV/EUV brightness at active regions of the AIA images is mostly restored. Other detailed structures are not reproduced well, because sunspot drawings do not have information on other structures such as filaments and coronal holes. These results mean that our model can only reproduce active regions.

We estimate TUMF and full-disk count rates (CR) of the evaluation data for both generated and real images and their temporal variations are given in Figure 3. We only consider pixels within 0.98 solar radius for all images to avoid uncertainties near the limb. The average CC of TUMF and CR between generated and real ones are 0.82 for magnetograms, 0.74 for 94 Å, 0.75 for 131 Å, 0.56 for 171 Å, 0.65 for 193 Å, 0.67 for 211 Å, 0.70 for 304 Å, and 0.76 for 335 Å.
respectively. The average CC is the highest for magnetograms because the sunspot drawings trace photospheric magnetic field distributions. It is the lowest for $171 \AA$, thus it seems that sunspot drawings cannot figure out the detailed configuration of EUV coronal loops, which are most evident for this passband image. The normalized rms error of TUMF and CR ranges from 0.08 to 0.37 . The TUMF and CR from generated images are comparable or slightly underestimated to those from real ones. This may be caused by the fact that the model generates active regions well, but not for quiet regions and/or EUV coronal structures. Nevertheless, their overall trends from both data sets are approximately consistent with each other.

Now we apply our model to Galileo sunspot drawings in 1612, in order to generate modern satellite solar images. Figure 4(a) shows the SDO/HMI-like magnetogram and SDO/ AIA-like images on 1612 June 2. Noting that 1755 is the first year of the first solar cycle and the solar cycle period is 11 years, we assume that 1612 belongs to an even solar cycle. Therefore our model generates the polarity pattern of the active regions in the AI-generated magnetogram like twenty-fourth solar cycle. We can see the EUV brightness of active regions in the AI-generated UV/EUV images. Since other structures such as coronal loops, coronal hole, and filaments are not well reproduced, we admit that the present model cannot produce the detailed morphology of solar corona. However, it is possible to estimate magnetic flux and EUV intensity of solar active regions. Figure 4(b) shows the temporal variation of TUMF and CR from 1612 June 2 to July 8. From this result, we can see the temporal variation of magnetic flux and EUV intensity over a month in 1612.

## 5. Conclusion

In this study, we proposed a new attempt to generate modern satellite data and their related physical parameters from historical sunspot drawings. We demonstrated the validity of this attempt using modern data sets: Mount Wilson sunspot drawings and SDO data. Finally, our model produces modern satellite images and related physical values from Galileo sunspot drawings. This study is expected to offer more information on the long-term evolution of solar magnetic fields and their related studies such as long-term variation of solar irradiance.

This study includes data from the synoptic program at the 150-Foot Solar Tower of the Mt. Wilson Observatory. The Mt. Wilson 150-Foot Solar Tower is operated by UCLA, with funding from NASA, ONR, and NSF, under agreement with the Mt. Wilson Institute. The data used here is proprietary of the Mount Wilson Observatory and the Galileo Project. We thank all the observers who made the drawings of sunspots. We thank the numerous team members who have contributed to the success of the SDO mission. This work was supported by the

BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea,the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2019R1A2C1002634, NRF2019R1C1C1004778, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: NumPy (numpy.org), Keras (keras.io), TensorFlow (tensorflow.org), and SunPy (sunpy.org).

## ORCID iDs

Harim Lee (2) https://orcid.org/0000-0002-9300-8073
Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944

## References

Arlt, R., \& Vaquero, J. M. 2020, LRSP, 17, 1
Boerner, P., Edwards, C., Lemen, J., et al. 2012, SoPh, 275, 41
Casas, R., Vaquero, J. M., \& Vazquez, M. 2006, SoPh, 234, 379
Galilei, G., Welser, M., \& de Filiis, A. 1613, Istoria E dimostrazioni intorno alle macchie solari E loro accidenti comprese in tre lettere scritte all'illvstrissimo signor Marco Velseri (Rome: G. Mascadi)
Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Hale, G. E., \& Nicholson, S. B. 1925, ApJ, 62, 270
Hoyt, D. V., \& Schatten, K. H. 1998a, SoPh, 171, 189
Hoyt, D. V., \& Schatten, K. H. 1998b, SoPh, 171, 491
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2016, arXiv:1611.07004
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Krivova, N. A., Balmaceda, L., \& Solanki, S. K. 2007, A\&A, 467, 335
Lemen, J. R., Title, A. M., \& Waltham, N. 2012, SoPh, 275, 17
Liu, Y., Hoeksema, J. T., Scherrer, P. H., et al. 2012, SoPh, 279, 295
Lockwood, M., Stamper, R., \& Wild, M. N. 1999, Natur, 399, 437
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Munoz-Jaramillo, A., \& Vaquero, J. M. 2019, NatAs, 3, 205
Nagovitsyn, Y. A., Tlatov, A. G., \& Nagovitsyna, E. Y. 2016, ARep, 60, 831
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Pevtsov, A. A., Tlatova, K. A., Pevtsov, A. A., et al. 2019, A\&A, 628, A103
Radford, A., Metz, L., \& Chintala, S. 2015, arXiv:1511.06434
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Solanki, S. K. 2003, A\&ARv, 11, 153
Solanki, S. K., \& Krivova, N. A. 2004, SoPh, 224, 197
Usoskin, I. G., Mursula, K., \& Kovaltsov, G. A. 2003a, SoPh, 218, 295
Usoskin, I. G., Solanki, S. K., Schussler, M., Mursula, K., \& Alanki, K. 2003b, PhRvL, 91, 211101
Vaquero, J. M. 2007, AdSpR, 40, 929
Vokhmyanin, M. V., \& Zolotova, N. V. 2018, SoPh, 293, 31
Zharkov, S. I., \& Zharkova, V. V. 2006, AdSpR, 38, 868"
Kangwoo Yi et al 2021 - Visual Explanation of a Deep Learning Solar Flare Forecast Model and Its Relationship to Physical Parameters.pdf,"# Visual Explanation of a Deep Learning Solar Flare Forecast Model and Its Relationship to Physical Parameters 

Kangwoo $\mathrm{Yi}^{1}$ (1), Yong-Jae Moon ${ }^{1,2}$ (D), Daye Lim ${ }^{2}$ (D), Eunsu Park ${ }^{2}$ (D), and Harim Lee ${ }^{2}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea

Received 2020 May 27; revised 2021 January 18; accepted 2021 January 19; published 2021 March 22


#### Abstract

In this study, we present a visual explanation of a deep learning solar flare forecast model and its relationship to physical parameters of solar active regions (ARs). For this, we use full-disk magnetograms at 00:00 UT from the Solar and Heliospheric Observatory/Michelson Doppler Imager and the Solar Dynamics Observatory/ Helioseismic and Magnetic Imager, physical parameters from the Space-weather HMI Active Region Patch (SHARP), and Geostationary Operational Environmental Satellite X-ray flare data. Our deep learning flare forecast model based on the Convolutional Neural Network (CNN) predicts ""Yes"" or ""No"" for the daily occurrence of C-, M-, and X-class flares. We interpret the model using two CNN attribution methods (guided backpropagation and Gradient-weighted Class Activation Mapping [Grad-CAM]) that provide quantitative information on explaining the model. We find that our deep learning flare forecasting model is intimately related to AR physical properties that have also been distinguished in previous studies as holding significant predictive ability. Major results of this study are as follows. First, we successfully apply our deep learning models to the forecast of daily solar flare occurrence with TSS $=0.65$, without any preprocessing to extract features from data. Second, using the attribution methods, we find that the polarity inversion line is an important feature for the deep learning flare forecasting model. Third, the ARs with high Grad-CAM values produce more flares than those with low Grad-CAM values. Fourth, nine SHARP parameters such as total unsigned vertical current, total unsigned current helicity, total unsigned flux, and total photospheric magnetic free energy density are well correlated with Grad-CAM values.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar flares (1496); Convolutional neural networks (1938)

## 1. Introduction

Solar flares, one of the most energetic activities of the Sun, are known to occur in active regions (ARs) which are magnetically concentrated locations (Priest \& Forbes 2002; Shibata \& Magara 2011). Many researchers have suggested that flares are related to magnetic characteristics such as non-potentiality and magnetic complexity (Schrijver 2007, 2016; Sharykin et al. 2017; Toriumi \& Takasao 2017; Vasantharaju et al. 2018). Characteristics of the polarity inversion line (PIL) have been studied for flare prediction (Mason \& Hoeksema 2010; Falconer et al. 2011, 2012, 2014; Sadykov \& Kosovichev 2017). Many studies have considered Space-weather HMI Active Region Patch (SHARP; Bobra et al. 2014) data which indicate magnetic characteristics of ARs (Bobra \& Couvidat 2015; Liu et al. 2017; Sadykov \& Kosovichev 2017; Lim et al. 2019a, 2019b). Recently, several studies have applied deep learning methods to the flare forecast using magnetic features of the Sun (Huang et al. 2018; Nishizuka et al. 2018; Park et al. 2018; Chen et al. 2019; Liu et al. 2019; Li et al. 2020). However, the question of how deep learning models predict flare occurrence is still not answered clearly. Liu et al. (2019) used magnetic parameters for their flare prediction model which is based on the Long ShortTerm Memory (LSTM; Hochreiter \& Schmidhuber 1997). They investigated the relationship between input parameters and the performance of the model by searching for a subset of inputs that make prediction scores high or low. This study is a good answer to the question of what inputs make the forecasting score high but not a proper quantitative information of the deep learning flare model. Huang et al. (2018) applied the Convolution Neural

Network (CNN; Lecun et al. 1998) to the flare forecast using patches of ARs of solar line-of-sight magnetograms. They extracted CNN feature maps from the interior layers in the model and presented that their model pays attention to the area of the PIL. However, the feature map, which is just a result of the calculation between the input image and CNN kernels, does not indicate important areas of the input image for prediction results. The highlighted area in the feature map could be fade-out in subsequent layers due to inactive kernels, low weights, and/or the merging of spatial information. In addition, feature extraction cannot guarantee which ones out of a number of features in the layer are important.

Deep learning interpretation is an important issue in computer science and related fields. For CNN, attribution methods such as guided backpropagation (Springenberg et al. 2015) and Gradientweighted Class Activation Mapping (Grad-CAM; Selvaraju et al. 2017) have been mainly used for visual explanations. Guided backpropagation and Grad-CAM are based on the gradient for each weight (input pixel, CNN kernel, and fully connected layer weight), which indicates the change in the result if the weight is increased by a tiny amount (Simonyan et al. 2014; Lecun et al. 2015).

In this letter, for the first time, we present a visual explanation of a deep learning flare forecast model by attribution methods and its relationship to physical parameters such as SHARP. For this, we develop a new deep learning flare forecasting model, which is based on CNN, that predicts ""Yes or No"" for the daily occurrence of C-, M-, and X-class flares. Input data are the solar full-disk line-of-sight magnetogram at 00:00 UT from the Solar and Heliospheric Observatory
(SOHO; Domingo et al. 1995)/Michelson Doppler Imager (MDI; Scherrer et al. 1995) and Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Helioseismic and Magnetic Imager (HMI; Schou et al. 2012). Using Guided backpropagation and Grad-CAM, we measure the importance weight of the area in the input and compare them with physical parameters taken from SHARP.

This paper is organized as follows. The data are described in Section 2. Our model and attribution methods used for this research are described in Section 3. The results of model interpretation and comparison with physical parameters are given in Section 4. A brief conclusion and summary are presented in Section 5.

## 2. Data

We consider solar full-disk line-of-sight magnetograms at 00:00 UT from $\mathrm{SOHO} / \mathrm{MDI}$ and $\mathrm{SDO} / \mathrm{HMI}$ for model training and test. When a magnetogram is not available at 00:00 UT, we use the nearest one taken on the previous day. Most of the data are in the range of two hours before 00:00 UTC. SOHO, launched in 1995 December, is a space mission to study the Sun, and a joint venture between the European Space Agency and the National Aeronautics and Space Administration (NASA). MDI, which is one of the scientific instruments of SOHO , provides $1024 \times 1024$ full-disk solar magnetograms with $1 .{ }^{\prime} 98$ per pixel at a cadence of 96 minutes. SDO, launched in 2010 February, is a NASA space mission to provide data for predicting solar activities. HMI, which is one of the scientific instruments of SDO, supplants MDI. HMI produces $4096 \times 4096$ full-disk magnetograms with $0 .{ }^{\prime \prime} 5$ per pixel at a 720 s cadence.

We obtained binned magnetogram image data from the SOHO data archive ${ }^{3}$ in JPG format with $512 \times 512$ resolution for training and test. MDI full-disk magnetograms in the archive were used for ASAP flare forecast (Colak \& Qahwaji 2009). HMI full-disk magnetograms in the archive were produced by quick-look methods which are useful for real-time forecasts.

In order to verify the scientific reliability of JPG data, we calculate the pixel correlation coefficient between it and actual magnetogram data using a randomly selected $10 \%$ of MDI and HMI data that are resized into $256 \times 256$ sizes by block average. The average pixel correlation coefficient of MDI is 0.91 and that of HMI is 0.87 with the actual magnetogram data with the bytescale of $\pm 100$ Gauss, which shows the JPG data are in good agreement with the actual magnetogram data.

We evaluate transferability between MDI and HMI data using the data within 30 minutes of time difference during 2011 January in two ways. First, we calculate Complex Wavelet Structural Similarity (CW-SSIM; Sampat et al. 2009) between MDI and HMI, which is the measure index of image similarity. CW-SSIM is an extension of the Structural Similarity (SSIM; Wang et al. 2004), which is generally used to measure the similarity between two images to the complex wavelet domain. It is less sensitive to small geometric distortions such as small rotations, translations, and small differences in scale than SSIM.

$$
\operatorname{CW}-\operatorname{SSIM}\left(c_{x}, c_{y}\right)=\frac{2\left|\sum_{i=1}^{N} c_{x, i} c_{y, i}^{*}\right|+L}{\sum_{i=1}^{N}\left|c_{x, i}\right|^{2}+\sum_{i=1}^{N}\left|c_{y, i}\right|^{2}+L}
$$

[^0]where $c_{x}$ and $c_{y}$ represent the sets of coefficients (extracted at the same spatial location in the same wavelet subbands) in the complex wavelet transform domain (e.g., the complex version of the steer pyramid decomposition Portilla \& Simoncelli 2000) of the two images being compared, respectively. The ${ }^{*}$ denotes the complex conjugate of $c$ and $L$ is a small positive constant. CW-SSIM is in the range $0-1$ and results in a higher value for higher similarity. The small geometric distortion is the main difference between MDI and HMI data, but its effect on solar activity is not as significant compared to large distortion. For this, we use re-shaped data by cubic-interpolation for a fair evaluation considering a solar disk radius in pixels. The average CW-SSIM is 0.82 , indicating that MDI and HMI data are structurally transferable. Second, we calculate the pixel correlation coefficient between MDI and HMI data in units of Gauss. For this we use rebinned MDI and HMI data into 256 sizes by block average. The average correlation coefficient is 0.9 , which is high enough to transfer between MDI and HMI. This result is slightly higher than Liu et al. (2012; 0.82), which may be due to denoising by block average, and different comparison areas (center-to-limb angle of $0^{\circ}-60^{\circ}$ for Liu et al. 2012 and full-disk for ours).

For physical parameters we adopt definitive SHARP data with a 720 s cadence, developed by the HMI team (Bobra et al. 2014). These data are given for HMI Active Region Patches (HARPs) which are automatically identified. For each HARP, magnetic parameters that are derived from the size, distribution, and non-potentiality of vector magnetic fields are provided.

We label the magnetograms separately with the flaring event day ( $\geq \mathrm{C} 1.0$ Class) or non-flare event day ( $<\mathrm{C} 1.0$ Class) using Geostationary Operational Environmental Satellite (GOES) X-ray flare data which are automatically detected by algorithm or manually recorded by the National Oceanic and Atmospheric Administration (NOAA; Ryan et al. 2016; NOAA GOES X-ray flux ${ }^{4}$ ).

To select training and test data, we use the chronological data separation in which we sequentially separate the data into training and test according to its observation time. By using the chronological data separation, we avoid an impossible forecasting condition for the model evaluation, i.e., carrying out current forecasting using future data, not past data.

As a result, 4298 MDI data from 1996 May to 2008 December are used for training and 683 MDI data from 2009 January to 2010 December and 2360 HMI data from 2011 January to 2017 June are used for testing.

## 3. Model and Methods

### 3.1. Flare Model

Our model follows the technique of the dense connection (Huang et al. 2017). The number of layers and optimized hyperparameters in our model are different from theirs. The data that we use are solar magnetograms so that we train our model independently. Figure 1 shows a structure of our deep learning flare model. The model consists of an initial block, five dense blocks, and a last block, in order. The initial block contains a convolution layer ( $3 \times 3$ kernel, 1 stride, 26 features) and a max pooling layer ( $2 \times 2$ kernel, 2 strides), in order. Dense blocks 1-5 consists of a batch normalization (BN; Ioffe \& Szegedy 2015), a

[^1]
[^0]:    ${ }^{3}$ https://sohowww.nascom.nasa.gov/data/REPROCESSING/

[^1]:    4 https://www.swpc.noaa.gov/products/goes-x-ray-flux
![img-0.jpeg](img-0.jpeg)

Figure 1. Structure of our model. *K* is kernel size. *D* is the number of feature dimensions and is displayed in the case where its number is changed in the layer.

rectified linear unit (ReLU; Nair & Hinton 2010), a convolution layer (1 × 1 kernel, 1 stride, 13 × *n* features; *n* = number of the block), a BN, a ReLU, a convolution layer (3 × 3 kernel, 1 stride, 39 features), a concatenation layer that concatenates features from the previous layer and the input of the dense block, and an average pooling layer (2 × 2 kernel, 2 strides), in order. The last block includes a BN, an average pooling layer (2 × 2 kernel, 2 strides), and a fully connected layer, in order. The last layer produces two values; one is a score for the flaring event day and the other for the non-flare event day. The prediction of the model selects a higher score.

We compare our model with previous flare models. To evaluate models, we consider statistical scores using the contingency table that consists of four components: hit (H; flare predicted and occurred), false alarm (F; flare predicted but did not occur), miss (M; no flare predicted but occurred), and null (N; no flare predicted and none occurred). The statistical scores, Accuracy (ACC), the Heidke Skill Score (HSS; Heidke 1926), the Appleman Skill Score (ApSS; Appleman 1960), and the True Skill Statistics (TSS; Allouche et al. 2006), are described as

$$
\text{ACC} = \frac{H + N}{H + F + M + N}, \tag{2}
$$

$$
\text{HSS} = \frac{2[(H \times N) - (M \times F)]}{(H + M) \times (M + N) + (H + F) \times (F + N)}, \tag{3}
$$

$$
\text{ApSS} = \left\{ \frac{H - F}{H + M}, \text{ if event rate} < 0.5 \right\}, \tag{4}
$$

$$
\text{TSS} = \frac{H}{H + M} - \frac{F}{F + N}, \tag{5}
$$

where the event rate is the rate between the number of flare events and the total number of data, in the test set. Barnes et al. (2016) suggested that ApSS is a good index for method evaluation because it treats the cost of each type of error (miss and false alarm) as equal. TSS has to be understood with event rate and frequency bias (FB; the number of events in forecasting divided by the number of events on observation, Leka et al. 2019). In detail, with an event rate lower than 0.5, overforecasting (FB > 1) attains a high TSS while underforecasting (FB < 1) is less likely to. With an event rate higher than 0.5, that is the opposite. The performance of the forecasting model could be changed with the difference in data splitting (Nishizuka et al. 2017). We compare our model with the other models (deep learning models and one statistical model) based on the chronological data separation method.

Cinto et al. (2020) separated flare prediction models into two groups, operationally evaluated systems and non-operationally evaluated systems. Operationally evaluated systems satisfy the following four criteria:

(i) The model has been evaluated by truly unseen data which are split before any treatment used for designing the output model. Distinguishing test data from training data gives a true estimate of the model.

(ii) The model has been evaluated using ARs at any location in the disk, including far from the center (at the limb). The evaluation only including ARs near the solar disk center increases uncertainty about model performance.

(iii) For the ≥ M-class flare prediction, the model has been evaluated using ARs including those not linked to any sort of flares. Some models only include ARs linked with ≥ C-class flares. This approach also raises uncertainty about model performance.

(iv) The model has been designed with enough data. Models fitted with few data are not as effective as those designed with sufficient data.

We do not consider criterion (iii) to distinguish operational and non-operational evaluation because it is for ≥ M-class flare prediction. Our model is operationally evaluated. For a fair comparison, we denote whether flare prediction models for comparison are operationally evaluated or not.

Table 1 shows the results of the models. We note that it is hard to directly compare the models since the data and time periods are different. This comparison shows that our model is reliable for further analysis of visual explanation. The purpose of our study is not for comparison but the application of our model to attribution methods.

### 3.2. Attribution Methods

#### 3.2.1. Guided Backpropagation

Guided backpropagation is one of the popular methods that provides a visual explanation of CNN deep learning models. It is a guidance where negative gradients are set to zero by the ReLU during backpropagation. With this guidance, guided backpropagation denoises the saliency map, which makes it clearer than other visual explanation methods (Simonyan et al. 2014; Zeiler & Fergus 2014). We apply the guided backpropagation to
Table 1 Comparison of Models

|  Model | Event Definition | Operationally Evaluated | ACC | HSS | ApSS | TSS | FB | Event Rate  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  Our model | Full-disk | yes | 0.83 | 0.65 | 0.61 | 0.65 | 1 | 0.55  |
|  Park et al. (2018) | Full-disk | yes | 0.82 | 0.63 | 0.60 | 0.63 | 1 | 0.55  |
|  Event statistics ${ }^{\text {a }}$ | Full-disk | yes | 0.82 | 0.36 | 0.08 | 0.41 | $\cdots$ | 0.81  |
|  Nishizuka et al. (2018) | AR | yes | 0.82 | 0.53 | 0.09 | 0.63 | 1.53 | 0.2  |
|  Huang et al. (2018) | AR | no | 0.76 | 0.34 | -0.61 | 0.49 | 2.06 | 0.15  |

Note. ${ }^{a}$ Statistical method. Event statistics flare forecasting (Wheatland 2005) by Barnes et al. (2016). the input image. The result of guided backpropagation on the input image is a detailed gradient mask by pixel. Interpreting the gradient of the input image has to consider the context of the input pixel and many calculations ( 96,761 parameters) between the input and the output. Guided backpropagation is used for making a saliency map to distinguish an important area for the results of the model. To avoid ambiguity and focus on the magnitude of the gradient, we consider the absolute value of a guided backpropagation mask.

### 3.2.2. Grad-CAM

Grad-CAM is a strict generalization of class activation mapping (CAM; Zhou et al. 2016). CAM has achieved classspecific feature maps which identify the importance of image regions, but it can be applied to CNN models with a specific kind of architecture. Grad-CAM is a modified version of CAM to be applied to any kind of CNN using the gradient. Every element in the feature maps from CNN has a gradient. To produce a class-specific feature map, Grad-CAM multiplies every feature map and the sum of all gradients of elements in the feature map, sums all weighted feature maps, and applies ReLU to the feature map to set the negative gradation to zero. We apply Grad-CAM to the last convolutional layer of the model.

## 4. Results and Discussion

It is noted that in order to put the influence of the gradient on the same scale for each result of the model, the values of the gradient mask are scaled in the range $0-1$ for plotting and analyzing. Figure 2 shows examples of applying guided backpropagation and Grad-CAM to our model, for the flaring label in a hit case. Figure 2(a) is a line-of-sight magnetogram observed at 00:00 UT on 2013 October 23. During the day, AR 11875 produced seven C-class flares and three M-class flares and AR 11877 produced two C-class flares while other ARs did not produce flares. Figure 2(b) is an example of applying guided backpropagation to our model in the case where the magnetogram is input data. It shows that two ARs, 11875 and 11877, producing flares are more highlighted than the other ARs. Figure 2(c) is an example of applying Grad-CAM to our model in the same case. It is interesting to note that AR 11875 is paid greater attention by our model than AR 11877, while the other ARs are paid weak attention. Such a tendency is consistent with the flaring activities of ARs. These results indicate that our model focuses on ARs, especially flaring ones. Magnetogram data at high bytescales such as $\pm 1500$ Gauss could produce different results of attribution methods because they contain more information than the data we used.

Figure 2 shows the results for an interesting case (flaring label in a hit case). Attribution methods also can be used for the non-flare label. In addition, model results are expressed as hit, miss, false alarm, and null.

Figure 3 shows the results of guided backpropagation overlaid on full-disk magnetograms in hit, false alarm, miss, and null. Whether the predictions are right or wrong, the guided backpropagation masks concentrate on certain ARs for the flare label, while they are more spread out for the non-flare label. In false alarm and miss cases, for the flaring label, ARs close to the limb are activated (Figures 3(b) and (c), left). It shows that the projection effect may be a reason for the prediction failure.

In all cases, the parts of the limbs are activated. The guided backpropagation is applied to the earliest stage of the forecasting process, the input image. At this stage, our model considers many interesting features such as small ARs, a wide area filled with spots of negative or positive polarity, and limbs. The effect of such less important features such as limbs disappears as the computation goes on.

Figure 4 is the results of Grad-CAM overlaid on full-disk magnetograms in hit, false alarm, miss, and null cases on the same days as in Figure 3. We can see that Grad-CAM highlights the specific ARs that the guided backpropagation has paid attention to. For the non-flare label, a gradient of zero covers the data in all cases. We apply Grad-CAM without ReLU for the non-flare label and find that negative gradients are in the ARs that are highlighted in the analysis for the flaring label.

Figure 5 shows the AR patches of line-of-sight magnetograms, guided backpropagation masks, and overlaid images for a flaring label in a hit. It is interesting to note that the important pixels are in the vicinity of PILs, implying that our model looks at the PILs when it determines whether this magnetogram produces flares or not.

Table 2 shows the analysis of the guided backpropagation results. The results in Table 2 have uncertainties considering root mean square errors.

We want to note that the classifications of hit, miss, false alarm, and null are based on forecasting results using solar fulldisk input data containing flaring and non-flare ARs whether they are a hit or miss case. We analyze flaring and non-flare ARs separately because attribution methods highlight specific ARs (maybe flaring or flaring-like ARs), not all ARs (Figures 2-4). We calculate the ratio of average gradients of guided backpropagation in the PIL to that in the non-PIL area of AR. The ratios are higher than 3.0 for all cases, indicating that the guided backpropagation is usually more activated on the PIL than non-PIL area. The ratios are similar between nonflare ARs and flaring ARs because the guided backpropagation is applied to the earliest stage of forecasting (i.e., input image), which was already mentioned in the discussion of Figure 3.
![img-1.jpeg](img-1.jpeg)

Figure 2. Results of attribution methods for the flaring label in a hit case. (a) A line-of-sight magnetogram observed at 00:00 UT on 2013 October 23. The white area corresponds to positive polarity and the black one to negative polarity. (b) A guided backpropagation mask (left) and the mask overlaid on (a); right. (c) A Grad-CAM mask (left) and the mask overlaid on (a); right. The brightness of the masks corresponds to the magnitude of the gradient of pixels/areas for prediction. Values are scaled from 0 to 1 for each mask.

![img-2.jpeg](img-2.jpeg)

Figure 3. Results of guided backpropagation for a flaring label (left) and a non-flare label (right). (a) 2013 October 23 (hit). (b) 2015 August 10 (false alarm). (c) 2016 December 5 (miss). (d) 2016 May 22 (null). Values are scaled from 0 to 1 for each mask.

From these analysis results, we can interpret not only that the PIL is activated but also that the boundaries of strong polarities are activated. In order to find a better interpretation, we calculate the ratio of average gradients of guided backpropagation in the PIL to that in the boundaries of the positive and negative polarities of ARs.
![img-3.jpeg](img-3.jpeg)

Figure 4. Results of Grad-CAM for a flaring label (left) and a non-flare label (right). (a) 2013 October 23 (hit). (b) 2015 August 10 (false alarm). (c) 2016 December 5 (miss). (d) 2016 May 22 (null). Values are scaled from 0 to 1 for each mask.

We can find that the ratios of the PIL/boundary are higher than the ratios of PIL/non-PIL for the non-flare ARs in hit, miss, and null cases (we call them the PB-group). In the flaring ARs for hit and miss cases and the non-flare ARs for false alarm cases, it is the opposite (we call them the PN-group). This means that the guided backpropagation would be differently activated for the flaring ARs and the non-flare ARs. To find the reasons for the ratio differences, we examine several ARs by visual inspection. We find that the many ARs in the PB-group consist of the polarities of narrow and long branch-like structure. The guided backpropagation is mainly activated in the vicinity of PIL while not activated in the branch-like polarities. The ARs in the PN-group have usually simpler structures than the ARs in the PB-group.

The values of the ratios and their analyses suggest that the guided backpropagation mask is more focused in the PIL for the flaring ARs than for the non-flare ARs. In order to find the association between the guided backpropagation and the PIL of AR, we calculate the average IoU between the PIL and the guided backpropagation mask. IoU is an evaluation metric of segmentation and object detection, which is the ratio between the intersection area of the object and segmentation to the union area of them. For this, we use the subsets of the gradient masks with a normalized gradient of 0.1 or larger. The average IoU of the flaring AR is higher than that of a non-flare AR, but not beyond uncertainties corresponding to the root mean square errors. IoU values are quite small because they are the results of the line (PIL) versus the area (mask). Considering the analysis results, we can say that the PIL is an important feature for our model to recognize flaring ARs.

Many research groups have shown that a flare occurrence and its class are associated with PILs (Schrijver 2007; Kim et al. 2008; Mason & Hoeksema 2010; Falconer et al. 2011, 2012, 2014; Schrijver 2016; Sadykov & Kosovichev 2017; Sharykin et al. 2017; Toriumi & Takasao 2017; Vasantharaju et al. 2018). Schrijver (2007, 2016) and Vasantharaju et al. (2018) showed that large flares are associated with pronounced high-gradient PILs. Kim et al. (2008) pointed out that preflare activities such as sigmoidal UV structure appeared along the PIL. Mason & Hoeksema (2010) showed that GWILL, which combines the PIL length and gradient across it, is a better parameter than effective separation (Chumak & Chumak 1987; Chumak et al. 2004; Guo et al. 2006) and total unsigned magnetic flux for flare forecast. Falconer et al. (2011, 2012, 2014) investigated flare forecast using the transverse gradient of the line-of-sight magnetic field and potential transverse field on the PIL. Sharykin et al. (2017) showed that the flare energy release develops near the PIL. Sadykov & Kosovichev (2017) indicated that flare forecasts based on PIL characteristics are more effective than those using global characteristics of ARs. These studies show that flaring activities are very intimately associated with the characteristics of PILs, which are consistent with our results.

To find the relationship between Grad-CAM results and the flare occurrence rate, we define the Grad-CAM value for a given HARP as a maximum value of Grad-CAM in the area of HARP. Figure 6 shows the relationship between the average flare occurrence rate (within 24 hr) and the Grad-CAM values for all HARPs of hit, miss, and false alarm. As shown in the figure, all flares (C-, M-, and X-class) occur more frequently for ARs with higher Grad-CAM values.

In addition, we compare the Grad-CAM value with 16 SHARP parameters suggested by Bobra et al. (2014) in hit, miss, and false alarm cases. For this, we use the flare history of HARPs within ±60° longitudes of the central meridian to minimize the projection effect. To estimate the relationships between the Grad-CAM values and SHARP parameters, we consider Pearson correlation coefficients (r). In view of the absolute correlation coefficient, the top nine SHARP parameters whose correlation coefficients are larger than 0.5 are as follows: total unsigned vertical current (TOTUSJZ, r = 0.72), total
![img-4.jpeg](img-4.jpeg)

Figure 5. Patches of ARs at 00:00 UT (left), guided backpropagation masks (center), and masks overlaid on the patches (right) for the flaring label. (a) AR 11429 on 2012 March 9. (b) AR 11564 on 2012 September 4. (c) AR 11731 on 2013 May 1. (d) AR 11967 on 2014 February 4. (e) AR 12158 on 2014 September 10. (f) AR 12339 on 2015 May 12. The color bars in (a) are applied to (b), (c), (d), (e), and (f).
![img-5.jpeg](img-5.jpeg)

Figure 6. Average flare occurrence rate according to the Grad-CAM value in hit, miss, and false alarm cases.

Table 2 Analysis of the Guided Backpropagation with Root Mean Square Error

|  PIL/Non-PIL |  |  | PIL/Boundary |  | IoU |   |
| --- | --- | --- | --- | --- | --- | --- |
|   | Flaring AR $(\geqslant \mathrm{C})$ | Non-flare AR $(<\mathrm{C})$ | Flaring AR $(\geqslant \mathrm{C})$ | Non-flare AR $(<\mathrm{C})$ | Flaring AR $(\geqslant \mathrm{C})$ | Non-flare AR $(<\mathrm{C})$  |
|  Hit | $3.996 \pm 1.787$ | $3.815 \pm 2.320$ | $3.768 \pm 1.840$ | $3.820 \pm 2.363$ | $0.112 \pm 0.059$ | $0.069 \pm 0.077$  |
|  Miss | $3.691 \pm 1.755$ | $3.428 \pm 2.343$ | $3.626 \pm 1.720$ | $3.578 \pm 2.488$ | $0.077 \pm 0.051$ | $0.051 \pm 0.068$  |
|  False alarm | $\cdots$ | $3.584 \pm 2.085$ | $\cdots$ | $3.581 \pm 2.113$ | $\cdots$ | $0.056 \pm 0.058$  |
|  Null | $\cdots$ | $3.209 \pm 1.834$ | $\cdots$ | $3.232 \pm 1.874$ | $\cdots$ | $0.048 \pm 0.060$  |

Note. (left) The ratio of average gradients in the PIL to that in the non-PIL area of AR. (center) The ratio of average gradients in the PIL to that in the boundary AR. (right) The average IoU between PIL and guided backpropagation mask. unsigned current helicity (TOTUSJH, $r=0.71$ ), total unsigned flux (USFLUX, $r=0.70$ ), total photospheric magnetic free energy density (TOTPOT, $r=0.62$ ), mean photospheric excess magnetic energy density (MEANPOT, $r=0.57$ ), shear angle (MEANSHR, $r=0.57$ ), fractional area with shear $>45^{\circ}$ (SHRGT45, $r=0.56$ ), sum of the modulus of the net current per polarity (SAVNCPP, $r=0.55$ ), and absolute value of the net current helicity (ABSNJZH, $r=0.52$ ). Figure 7 shows box plots of the relationships between the Grad-CAM values and the nine SHARP parameters.

Our findings are consistent with previous studies. The top 8 SHARP parameters except for MEANSHR in our study are used for flare prediction by Bobra \& Couvidat (2015) who used 13 SHARP parameters having high Fisher ranking scores. Liu et al. (2017) suggested that TOTUSJZ and TOTUSJH are the most important parameters to classify flaring ARs considering Gini importance (Breiman 2001). Toriumi \& Takasao (2017) noted from numerical simulations that TOTUSJH and TOTPOT are highly correlated with the stored magnetic free energy which is the maximum flare energy that could be released. ALL
![img-6.jpeg](img-6.jpeg)

**Figure 7.** Box plots of the relationships between nine SHARP parameters and Grad-CAM values in Hit, Miss, and False alarm cases. Horizontal lines in the boxes are the median of the data. The upper/lower bounds of the boxes are the upper/lower quartiles of the data, while the upper/lower bounds of whiskers are the highest/lowest data below/above 1.5 times the box range from the upper/lower bounds of the boxes. Outliers, which are beyond the caps of the whisker, are not represented. Graphs are expressed in logarithm scale but ABSNJZH and SHRGT45 are in linear scale because their minimum values are zero.

six SHARP parameters (TOTUSJH, TOTUSJZ, TOTPOT, USFLUX, ABSNJZH, and SAVNCPP) used in Lim et al. (2019a), which have higher correlations with major flare occurrence larger than 0.86, also belong to our selection. Our results imply that interpretations of our flare forecast model using guided backpropagation and Grad-CAM is consistent with SHARP parameters associated with non-potentiality and PILs.

### 5. Summary and Conclusion

In this study we have presented visual explanation of our deep learning flare model and investigated its relationship with
the physical parameters of ARs. For this, we consider solar fulldisk line-of-sight magnetograms from SOHO/MDI and SDO/ HMI, SHARP parameters from the HMI data, and GOES X-ray flare data. We make a new flare forecast model, which is based on CNN, that has better performance than other deep learning models. For the first time, we apply guided backpropagation and Grad-CAM to our model for interpretation.

The major results of this study are as follows. First, we successfully apply our deep learning models to the forecast of daily solar flare occurrences. Second, the results of guided backpropagation show that the flare occurrence prediction score of the model is mainly determined by the vicinity of PILs. Third, the ARs with high Grad-CAM values produce more flares than those with low Grad-CAM values. Fourth, the top nine SHARP parameters such as TOTUSJZ ( $r=0.72$ ), TOTUSJH $(r=0.71)$, USFLUX $(r=0.70)$, TOTPOT $(r=0.62)$, MEANPOT $(r=$ 0.57 ), MEANSHR $(r=0.57)$, SHRGT45 $(r=0.56)$, SAVNCPP $(r=0.55)$, and ABSNJZH $(r=0.52)$ are well correlated with Grad-CAM values.

Much previous research suggests that flare occurrence is intimately related to PILs and SHARP parameters that denote the non-potentiality and magnetic complexity of ARs (Schrijver 2007; Kim et al. 2008; Mason \& Hoeksema 2010; Falconer et al. 2011, 2012, 2014; Bobra \& Couvidat 2015; Schrijver 2016; Liu et al. 2017; Sadykov \& Kosovichev 2017; Sharykin et al. 2017; Toriumi \& Takasao 2017; Vasantharaju et al. 2018; Lim et al. 2019a, 2019b). Very interestingly, guided backpropagation and Grad-CAM, which we adopt, show that our model pays attention to PILs and ARs with large values of the SHARP parameters under consideration.

In the SHARP parameter analysis using Grad-CAM, the best correlated parameters are insensitive to PILs while the guided backpropagation highlights PILs. The SHARP parameter analysis result could have a dependency on the characteristic of Grad-CAM that considers filtered and compressed spatial information. The interpretation results of the model could be different if other attribution methods are applied. To estimate the model dependence for interpretation by attribution methods, we have applied Grad-CAM++ (Chattopadhay et al. 2018) to the model. The differences between the analysis results of Grad-CAM and Grad-CAM++ are very small.

We really appreciate the constructive comments of the referee and the editor. We thank the numerous team members who have contributed to the success of the SDO mission as well as the SOHO mission. We thank contributors to Pytorch, Numpy, and the Matplotlib open-source package. This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131, NRF-2019R1A2C1002634, NRF2019R1C1C1004778, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and the Institute for Information \& Communications Technology Promotion (IITP) grant funded by the Korean government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares).

## ORCID iDs

Kangwoo Yi (1) https://orcid.org/0000-0003-4342-9483
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Daye Lim (1) https://orcid.org/0000-0001-9914-9080
Eunsu Park (1) https://orcid.org/0000-0003-0969-286X
Harim Lee (1) https://orcid.org/0000-0002-9300-8073

## References

Allouche, O., Tsoar, A., \& Kadmon, R. 2006, J. Appl. Ecol., 43, 1223
Appleman, H. S. 1960, BAMS, 41, 64
Barnes, G., Leka, K. D., Schrijver, C. J., et al. 2016, ApJ, 829, 89
Bobra, M. G., \& Couvidat, S. 2015, ApJ, 798, 135
Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SoPh, 289, 3549
Breiman, L. 2001, Math. Learn., 45, 5
Chattopadhay, A., Sarkar, A., Howlader, P., \& Balasubramanian, V. N. 2018, in 2018 IEEE Winter Conf. Applications of Computer Vision (WACV) (Piscataway, NJ: IEEE), 839
Chen, Y., Manchester, W. B., Hero, A. O., et al. 2019, SpWea, 17, 1404
Chumak, O., Zhang, H., \& Gou, J. 2004, A\&AT, 23, 525
Chumak, O. V., \& Chumak, Z. N. 1987, KFNT, 3, 7
Cinto, T., Gradwohl, A. L. S., Coelho, G. P., \& da Silva, A. E. A. 2020, MNRAS, 495, 3332
Colak, T., \& Qahwaji, R. 2009, SpWea, 7, S06001
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Falconer, D., Barghouty, A. F., Khazanov, I., \& Moore, R. 2011, SpWea, 9, S04003
Falconer, D. A., Moore, R. L., Barghouty, A. F., \& Khazanov, I. 2012, ApJ, 747, 32
Falconer, D. A., Moore, R. L., Barghouty, A. F., \& Khazanov, I. 2014, SpWea, 12, 306
Guo, J., Zhang, H., Chumak, O. V., \& Liu, Y. 2006, SoPh, 237, 25
Heidke, P. 1926, Geografiska Annaler, 8, 301
Hochreiter, S., \& Schmidhuber, J. 1997, Neural Computation, 9, 1735
Huang, G., Liu, Z., Van Der Maaten, L., \& Weinberger, K. Q. 2017, in 2017 IEEE Conf. Computer Vision and Pattern Recognition (CVPR) (Piscataway, NJ: IEEE), 2261
Huang, X., Wang, H., Xu, L., et al. 2018, ApJ, 856, 7
Ioffe, S., \& Szegedy, C. 2015, PMLR, 37, 448
Kim, S., Moon, Y.-J., Kim, Y.-H., et al. 2008, ApJ, 683, 510
Lecun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436
Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, IEEEP, 86, 2278
Leka, K. D., Park, S.-H., Kusano, K., et al. 2019, ApJS, 243, 36
Li, X., Zheng, Y., Wang, X., \& Wang, L. 2020, ApJ, 891, 10
Lim, D., Moon, Y.-J., Park, E., et al. 2019a, ApJ, 885, 35
Lim, D., Moon, Y.-J., Park, J., et al. 2019b, JKAS, 52, 133
Liu, C., Deng, N., Wang, J. T. L., \& Wang, H. 2017, ApJ, 843, 104
Liu, H., Liu, C., Wang, J. T. L., \& Wang, H. 2019, ApJ, 877, 121
Liu, Y., Hoeksema, J. T., Scherrer, P. H., et al. 2012, SoPh, 279, 295
Mason, J. P., \& Hoeksema, J. T. 2010, ApJ, 723, 634
Nair, V., \& Hinton, G. E. 2010, in Proc. 27th Int. Conf. Int. Conf. Machine Learning, ICML'10 (Madison, WI: Omnipress), 807
Nishizuka, N., Sugiura, K., Kubo, Y., et al. 2017, ApJ, 835, 156
Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., \& Ishii, M. 2018, ApJ, 858, 113
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Portilla, J., \& Simoncelli, E. P. 2000, Int. J. Comput. Phys, 40, 49
Priest, E. R., \& Forbes, T. G. 2002, A\&ARv, 10, 313
Ryan, D. F., Dominique, M., Seaton, D., Stegen, K., \& White, A. 2016, A\&A, 592, A133
Sadykov, V. M., \& Kosovichev, A. G. 2017, ApJ, 849, 148
Sampat, M. P., Wang, Z., Gupta, S., Bovik, A. C., \& Markey, M. K. 2009, ITIP, 18, 2385
Scherrer, P. H., Bogart, R. S., Bush, R. I., et al. 1995, SoPh, 162, 129
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Schrijver, C. J. 2007, ApJL, 655, L117
Schrijver, C. J. 2016, ApJ, 820, 103
Selvaraju, R. R., Cogswell, M., Das, A., et al. 2017, in 2017 IEEE Int. Conf. Computer Vision (ICCV) (Piscataway, NJ: IEEE), 618
Sharykin, I. N., Sadykov, V. M., Kosovichev, A. G., Vargas-Dominguez, S., \& Zimovets, I. V. 2017, ApJ, 840, 84
Shibata, K., \& Magara, T. 2011, LRSP, 8, 6
Simonyan, K., Vedaldi, A., \& Zisserman, A. 2014, arXiv:1312.6034
Springenberg, J., Dosovitskiy, A., Brox, T., \& Riedmiller, M. 2015, arXiv:1412.6806
Toriumi, S., \& Takasao, S. 2017, ApJ, 850, 39
Vasantharaju, N., Vemareddy, P., Ravindra, B., \& Doddamani, V. H. 2018, ApJ, 860, 58
Wang, Z., Bovik, A. C., Sheikh, H. R., \& Simoncelli, E. P. 2004, ITIP, 13, 600

Wheatland, M. S. 2005, SpWea, 3, S07003
Zeiler, M. D., \& Fergus, R. 2014, in Computer Vision-ECCV 2014, ed. D. Fleet et al. (Cham: Springer International Publishing), 818

Zhou, B., Khosla, A. A. L., Oliva, A., \& Torralba, A. 2016, in 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (Piscataway, NJ: IEEE), 2921"
Hyun-Jin Jeong et al 2022 - Improved AI-generated Solar Farside Magnetograms by STEREO and SDO Data Sets and Their Release.pdf,"# Improved AI-generated Solar Farside Magnetograms by STEREO and SDO Data Sets and Their Release 

Hyun-Jin Jeong ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), Eunsu Park ${ }^{3}$ (D), Harim Lee ${ }^{2}$ (D), and Ji-Hye Baek ${ }^{3,4}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{3}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{4}$ Technology Center for Astronomy and Space Science, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>Received 2022 April 24; revised 2022 August 24; accepted 2022 August 26; published 2022 October 7


#### Abstract

Here we greatly improve artificial intelligence (AI)-generated solar farside magnetograms using data sets from the Solar Terrestrial Relations Observatory (STEREO) and Solar Dynamics Observatory (SDO). We modify our previous deep-learning model and configuration of input data sets to generate more realistic magnetograms than before. First, our model, which is called Pix2PixCC, uses updated objective functions, which include correlation coefficients (CCs) between the real and generated data. Second, we construct input data sets of our model: solar farside STEREO extreme-ultraviolet (EUV) observations together with nearest frontside SDO data pairs of EUV observations and magnetograms. We expect that the frontside data pairs provide historic information on magnetic field polarity distributions. We demonstrate that magnetic field distributions generated by our model are more consistent with the real ones than previously, in consideration of several metrics. The averaged pixel-to-pixel CC for full disk, active regions, and quiet regions between real and AI-generated magnetograms with $8 \times 8$ binning are $0.88,0.91$, and 0.70 , respectively. Total unsigned magnetic flux and net magnetic flux of the AI-generated magnetograms are consistent with those of real ones for the test data sets. It is interesting to note that our farside magnetograms produce polar field strengths and magnetic field polarities consistent with those of nearby frontside magnetograms for solar cycles 24 and 25 . Now we can monitor the temporal evolution of active regions using solar farside magnetograms by the model together with the frontside ones. Our AI-generated solar farside magnetograms are now publicly available at the Korean Data Center for SDO (http://sdo.kasi.re.kr).


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); Convolutional neural networks (1938); The Sun (1693); Astronomy data analysis (1858)
Supporting material: animations

## 1. Introduction

Magnetic fields play a fundamental role in producing solar extreme events, i.e., solar flares and coronal mass ejections (Wiegelmann et al. 2014; Judge et al. 2021). A series of ground-based and space-borne magnetographs have provided solar magnetic field data to study the field's origin and evolution over the last few decades (Pietarila et al. 2013). The solar magnetograph is an instrument producing a map of magnetic field strength and/or direction on the Sun, which is called a magnetogram (Babcock 1953). The Helioseismic and Magnetic Imager (HMI; Scherrer et al. 2012) on board SDO, which is in geosynchronous orbit, has provided high-resolution magnetograms of the entire solar disk. Recently, the Polarimetric and Helioseismic Imager (Solanki et al. 2020) on board Solar Orbiter started obtaining data of photospheric fields from outside the Sun-Earth line (Müller et al. 2020).

Before the Solar Orbiter mission, twin STEREO spacecraft provided the first stereoscopic view of the Sun drifting ahead of and behind the Earth's orbit (Kaiser et al. 2008). The STEREO Ahead (A) and Behind (B) were launched in 2006 and offered a complete $360^{\circ}$ view of the entire Sun with frontside observations. The STEREO data, together with the SDO data,

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
have been widely used to study solar atmospheric phenomena in three dimensions (Sterling et al. 2012; Caplan et al. 2016; Zhou et al. 2021). However, because they did not include a magnetograph, studies of magnetic activities from the frontside to the farside of the Sun were limited during the STEREO era.

Kim et al. (2019; hereafter KPL19) generated solar farside magnetograms from the STEREO/Extreme UltraViolet Imager (EUVI; Howard et al. 2008) $304 \AA$ observations using deep learning. KPL19 applied the Pix2Pix model (Isola et al. 2017), which is a widely used deep-learning model in image translation tasks. They trained and evaluated the model with pairs of solar frontside SDO/Atmospheric Imaging Assembly (AIA; Lemen et al. 2011) $304 \AA$ observations and SDO/HMI line-of-sight (LOS) 720 s magnetograms. Results of KPL19 showed that the farside magnetograms could be used to monitor the temporal evolution of active regions (ARs). However they set the upper and lower saturation limits of the field strength at $\pm 100 \mathrm{G}$, because their model worked well with proper byte scaling (Park et al. 2021). Their model generated the distributions and shapes of the ARs well, but it was hard to produce original-scale magnetic fluxes (Liu et al. 2021). Jeong et al. (2020; hereafter J20) improved the AI-generated magnetograms using an upgraded deep-learning approach with $\pm 3000 \mathrm{G}$ dynamic range based on the Pix2PixHD model (Wang et al. 2018) and multichannel SDO/AIA images of 171, 195, and $304 \AA$ for the model input. J20 showed that their results could reproduce strong magnetic fluxes and the
![img-0.jpeg](img-0.jpeg)

Figure 1. Flowchart and structures of the Pix2PixCC model. *G*, *D*, and *I* are the generator, discriminator, and inspector, respectively. The generator produces target-like data from input data. When we train the model, input data are SDO/AIA EUV 304, 193, and 171 Å images and a reference data pair. The reference data pair is composed of the three SDO EUV images and the corresponding SDO/HMI magnetogram, which are the nearest available ones. The discriminator trains for distinguishing between the real pair and generated pair. The real pair consists of an input data and a target data, and the generated pair consists of an input data and a generated data. The inspector computes CCs between the target data and generated data. The generator and discriminator are updated from the losses calculated from the inspector and discriminator.

Distribution of polarities for not only ARs, but also quiet regions (QRs). They applied the AI-generated farside ones to a part of the boundary conditions for the extrapolation of coronal magnetic fields. Results of the application were much more consistent with coronal farside extreme-UV (EUV) observations than those of the conventional method.

In the present study, we generate more accurate solar farside magnetograms than those of KPL19 and J20. For this, we make an upgraded model including a correlation coefficient (CC)—based objective with additional input data: not only farside STEREO EUV images but also frontside data pairs of SDO/AIA EUV images and HMI magnetograms as reference information. In this paper, we call the farside data generated by KPL19 AISFM 1.0, the data generated by J20 AISFM 2.0, and our data AISFM 3.0, respectively. We describe the detailed structure of our model in Section 2, and describe our data configurations in Section 3. We show our evaluation results of the model trained with the frontside evaluation data sets in Section 4.1. Then we generate AISFMs by the model from the corresponding images of STEREO A (or B) and the frontside reference data pairs, and show the results in Section 4.2. We release the AISFMs 3.0 and describe the data in Section 5. We conclude our study in Section 6.

### 2. Deep-learning Model

We use a deep-learning model called the Pix2PixCC model to generate solar farside magnetograms from the farside EUV observations and reference data pairs. Figure 1 shows the main structure of our model. The model consists of three major components: a generator (*G*), a discriminator (*D*), and an inspector (*I*). The generator is a generative network, and tries to produce target-like data from inputs with the help of the discriminator and the inspector. The discriminator is a discriminative network, which attempts to distinguish between the more realistic pair between a real pair and a generated pair. The real pair consists of input data and target data. The generated pair consists of input data and output from the generator. The generator gets updated with objectives from the discriminator, and tries to generate the best outputs to fool the discriminator. The inspector computes CCs between the target data and output from the generator to produce realistic values for the generated ones.

The generator and discriminator are multilayer networks. The multiple layers of the generative network are composed of several convolutional and transposed-convolutional filters of which parameters are updated during the model training process. Briefly, the convolutional filters try to extract features automatically from the input data, and the transposed-convolutional filters attempt to reconstruct outputs from the extracted features. For the detailed function of the filters in the network, refer to Goodfellow et al. (2016) and Buduma & Locascio (2017). The discriminative network consists of several convolution layers. Each convolution layer generates a feature map based on input. Our model has a feature matching (FM) loss (Loss<sub>FM</sub>), which is an objective function to optimize the parameters of the generator. The FM loss is to minimize the absolute difference between the feature maps of the real and generated pair from multiple layers of the discriminator. It is more effective for a large dynamic range of data than the loss function derived from the absolute difference between the target and generated data directly (Rana et al. 2019; Marnerides et al. 2021). The objective function Loss<sub>FM</sub> is given by

$$
\text{Loss}_{\text{FM}}(G, D) = \sum_{i=1}^{T} \frac{1}{N_i} \|D^{(i)}(x, y) - D^{(i)}(x, G(x)) \|
$$
where $x, y, T, i$, and $N_{i}$ are input data, target data, the total number of layers, the serial number of the layers, and the number of elements in output feature maps of each layer, respectively. $G(x)$ means output data from the generator. $D^{(.)}$ denotes the $i$ th-layer feature extractor of the discriminator.

Our networks use least-squares generative adversarial network (LSGAN) losses (Mao et al. 2017). The LSGAN losses update the generator (Loss $\mathrm{LSGAN}^{G}$ ) and the discriminator (Loss $\mathrm{LSGAN}^{D}$ ), and are obtained by

$$
\begin{aligned}
& \operatorname{Loss}_{\mathrm{LSGAN}}^{G}(G, D)=\frac{1}{2}(D(x, G(x))-1)^{2} \\
& \operatorname{Loss}_{\mathrm{LSGAN}}^{D}(G, D)=\frac{1}{2}(D(x, y)-1)^{2}+\frac{1}{2}(D(x, G(x)))^{2}
\end{aligned}
$$

where $D(x, y)$ and $D(x, G(x))$ are probabilities in the range of 0 (generated) to 1 (real) at the end of the discriminator from the real pair and generated pair, respectively. The generator tries to minimize the $\operatorname{Loss}_{\mathrm{LSGAN}}^{G}$, and the discriminator tries to minimize $\operatorname{Loss}_{\mathrm{LSGAN}}^{D}$. The competition between the generator and the discriminator contributes to the generation of realistic data. The performance of the adversarial objectives has been well demonstrated in image-to-image translation tasks for solar data (Park et al. 2019; Shin et al. 2020; Lim et al. 2021; Son et al. 2021).

In order for stable training of the generator, we use an additional objective function called CC loss (Loss ${ }_{\mathrm{CC}}$ ). It is known that the CC-based loss function has performed better than error-based loss functions: mean squared error, mean absolute error, etc. (Vallejos et al. 2020; Atmaja \& Akagi 2021). We use Lin's concordance CC, which takes bios into Pearson's CC (Lawrence \& Lin 1989). The concordance CC is commonly used to assess the reproducibility evaluating the degree to which pairs of data fall on the $45^{\circ}$ line through the origin. The range of concordance CC is from -1 (perfect disagreement) to 1 (perfect agreement). The inspector computes the CC loss with multiscale target and generated data. The function of $\operatorname{Loss}_{\mathrm{CC}}$, which maximizes the agreement between the target and generated data, is defined as

$$
\operatorname{Loss}_{\mathrm{CC}}(G)=\sum_{i=0}^{T} \frac{1}{T+1}\left(1-C C_{i}(y, G(x))\right)
$$

where $T$ and $i$ are the total number of downsampling by a factor of two and the serial number of the downsampling, respectively. $\mathrm{CC}_{i}$ means the CC value between the $2^{i}$ times downsampled target and AI-generated data. The average of the CC values from the multiscale target and generated data helps the model to optimize the network parameters. In addition, with the help of $\operatorname{Loss}_{\mathrm{CC}}$, we do not impose artificial saturation limits on our model.

Our final objectives are as follows:

$$
\begin{aligned}
& \min _{G} \lambda_{1} \operatorname{Loss}_{\mathrm{LSGAN}}^{G}(G, D)+\lambda_{2} \operatorname{Loss}_{\mathrm{FM}}(G, D) \\
& +\lambda_{3} \operatorname{Loss}_{\mathrm{CC}}(G) \\
& \min _{D} \operatorname{Loss}_{\mathrm{LSGAN}}^{D}(G, D)
\end{aligned}
$$

where $\lambda_{1}, \lambda_{2}$, and $\lambda_{3}$ are hyperparameters that control the importance of $\operatorname{Loss}_{\mathrm{LSGAN}}^{G}, \operatorname{Loss}_{\mathrm{FM}}$, and $\operatorname{Loss}_{\mathrm{CC}}$, respectively. We use 2,10 , and 5 for $\lambda_{1}, \lambda_{2}$, and $\lambda_{3}$, respectively.

The purpose of the generative adversarial network (GAN) objectives is to generate an answer that is acceptable. It is designed to deal with the probability space of the output. Wang et al. (2018) improved the GAN objectives by incorporating the FM objective to produce stable outputs. They set the importance of the FM loss $\left(\lambda_{2}\right)$ to be higher than that of the GAN loss $\left(\lambda_{1}\right)$. In J20, we showed that realistic magnetograms can be produced by the Pix2PixHD model, which uses both FM and GAN losses. In the present study, we use not only the FM loss but also the CC loss. The CC objective guides our model to generate the fields balancing positive and negative polarities. As a result of multiple tests with different values of importance, we set the importance of CC loss $\left(\lambda_{3}\right)$ to be five, for which our model shows the best performance in terms of metrics and visual aspects. The importance of GAN loss in our final objectives is lower than that of Pix2PixHD. As the training of our model continues, the model gets a greater number of updates from the FM and CC loss than from the GAN loss. To minimize the objective functions, we use an adaptive moment estimation (Adam; Kingma \& Ba 2014) optimizer with a learning rate 0.0002 . We train the model for $1,000,000$ iterations, and save the model and AI-generated data from the evaluation inputs at every 10,000th iteration. We evaluate all of the saved models by the metrics and use the highest scoring model to generate farside magnetograms. Our codes of Pix2PixCC are available athttps://github.com/ JeongHyunJin/Pix2PixCC, and more details of our model are described in the readme file. The codes are archived on Zenodo (Jeong 2022).

## 3. Data Sets

### 3.1. Training Data Sets

Here we use SDO/AIA EUV 304, 193, and $171 \AA$ images and SDO/HMI LOS magnetograms to train our deep-learning model. The three EUV passbands correspond to the chromosphere, corona, and upper transition region of the Sun, respectively. We use multichannel inputs to generate target magnetograms. Channel dimensions of the inputs are composed of three EUV passband images and a reference data pair. The reference data pair is composed of three SDO/AIA images and an SDO/HMI magnetogram, which were observed one solar rotation ( 27.3 days) prior. We expect that the differences between the EUV images and the reference data pair give the model information on how the intensities or distributions of the magnetic fields have changed.

We use pairs of train data sets with 6 hr cadence (at 01:00, 07:00, 13:00, and 19:00 UT each day) from 2011 January 1 to 2021 June 30. We select 10 months of data per year, excluding the data sets for the evaluation of our model. The months are shifted by 4 months. Among the 2011 data sets, for example, we use data from March to December for the model training, and the remaining data from January to February for the model evaluation. We use data from 2012 January to April and from July to December to train the model, and the remaining data from May to June to evaluate the model. These data set configurations are given to consider various solar inclination conditions. The inclination of the solar rotation axis with respect to the ecliptic plane makes different distributions of southern/northern magnetic fields for each month (Pastor Yabar et al. 2015). We take 6437 pairs for the training data sets. We remove data with poor quality that are flagged by a nonzero
![img-1.jpeg](img-1.jpeg)

Figure 2. Automatically selected areas of AR (red boxes) and QR (blue boxes) in the SDO/HMI magnetogram acquired at 2013 October 13 07:00 UT. Polarities of the magnetic fields are displayed in white for positive and black for negative. The detected areas, each having 128 × 128 pixels (∼273° × 273°), are overlaid on AI-generated data corresponding to the real one. The green grid lines represent 64 pixel (∼136.5) intervals within 60° from the center of the solar disk. An animation of this figure is available and shows the results from 2013 October 1 to November 30. In the animation, the grid lines are erased for visual comparison of the results. The real-time duration of the animation is 77 s.

(An animation of this figure is available.)

Value of the QUALITY keyword for both AIA and HMI data sets. Then we align them to have the same rotational axis, pixel size of solar radius (R⊙), and location of the disk center. We downsample them from 4096 × 4096 to 1024 × 1024 for computational capability. The radius of the Sun is fixed at 450 pixels. A mask is applied to the area outside 0.998 R⊙ from the disk center for minimizing the uncertainty of limb data. All EUV data numbers are scaled by median values on the solar disk to calibrate the gradual in-orbit degradation of the AIA instrument (Ugarte-Urra et al. 2015; Liewer et al. 2017).

### 3.2. Evaluation Data Sets

We use the remaining SDO data pairs, which are 2 months of data per year, to evaluate our model. Among them, we use the pairs of data sets from 2011 to 2017 to compare with the results of KPL19 and J20. We take 1342 pairs for the evaluation data sets. The preprocessing steps of the evaluation data are the same as those of training data.

When we compute objective measures between the target magnetograms and AI-generated data to evaluate our model, we compare the results not only of data for the full disk, but also data for the ARs and QRs. We select areas of the ARs and QRs with a size of 128 × 128 pixels from the preprocessed target magnetograms with 1024 × 1024 pixel resolution. We do not consider the areas outside 60° from the disk center to exclude limb data with uncertainty. We compute the total unsigned magnetic flux (TUMF) for the area, moving at intervals of 64 pixels up, down, left, and right from the center of the solar disk. When the TUMF of the area is greater than 5 × 10<sup>21</sup> Mx, the area is classified as an AR (Waldmeier 1955; van Driel-Gesztelyi & Green 2015). Otherwise, the areas are candidates for QRs. The boundaries of all detected areas do not overlap with one another. In order to balance the number of AR and QR areas, we select up to three AR areas for each magnetogram in the order of the largest TUMF, and up to two QR areas for each magnetogram in the order of the lowest TUMF.

Figure 2 shows an example of the selection result on 2013 October 13. There are three solar ARs near the center of the solar disk. The green grid lines in Figure 2(a) represent the boundaries of the areas where the TUMF is calculated. Our method successfully detects the approximate positions of three AR areas (red boxes in Figure 2(a)), and two QR areas (blue boxes in Figure 2(a)). Each box is placed in the same position on the AI-generated data as shown in Figure 2(b). Figure 2(c) shows a difference ratio map between the SDO/HMI magnetogram and AI-generated data. To compare the differences of their significant magnetic features, we smooth them out using a method similar to Higgins et al. (2011), who used ±70 G as a minimum threshold and a 2D Gaussian smoothing for the magnetograms; here we take 1σ and a window size of 10 × 10 pixels.

### 3.3. STEREO Data Sets

We use solar farside STEREO/EUVI EUV observations and pairs of SDO/AIA EUV images and SDO/HMI magnetograms to generate the farside magnetograms. The EUV passbands of STEREO are 304, 195, and 171 Å, which have similar temperature responses to the passbands of the SDO. We use STEREO data sets with 6 hr cadence (at 00:00, 06:00, 12:00, and 18:00 UT each day) from 2011 January 1 to 2021 June 30. Since communications with STEREO B were lost on 2014 October 1, the data from STEREO B are available until that day. We align, downsample, mask, and scale the STEREO EUV images like the SDO EUV ones. We manually exclude a set of STEREO data with incorrect header information and noise or missing values because of solar flares. The SDO pairs are data from the frontside that are selected by considering the separation angle between STEREO A (or B) and SDO. The observation dates of the reference SDO pairs are obtained by

$$T_{\text{SDO}} = T_{\text{STEREO}} - \Phi_{\text{STEREO}} \times \frac{27.3 \text{ day}}{360^\circ},\tag{5}$$
![img-2.jpeg](img-2.jpeg)

Figure 3. Two objective measures of comparisons between the SDO/HMI magnetograms and AI-generated magnetograms for 1342 full disk, 2926 ARs, and 2684 QRs. Panels (a), (b), and (c) are scatter plots between the TUMFs of the SDO/HMI magnetograms and AI-generated magnetograms for the full disk, ARs, and QRs, respectively. Panels (d), (e), and (f) are scatter plots between the NMFs of the SDO/HMI magnetograms and AI-generated magnetograms for the same data sets, respectively.

where $T_{\text {SDO }}, T_{\text {STEREO }}$, and $\Phi_{\text {STEREO }}$ are the date of reference SDO pairs, the date of STEREO data sets, and heliographic longitude of the STEREO, respectively. Given these configurations, we expect that the frontside magnetograms give our model information about the overall magnetic field distribution. And the EUV image sets of the frontside SDO and the farside STEREO are used to give information about the changes of features on the Sun.

## 4. Results and Discussions

### 4.1. Evaluation of Our Deep-learning Model

We evaluate our deep-learning model using the frontside evaluation data sets that we did not use when training the model. To compare our results with KPL19 and J20, we use Pearson's CC as a measure for the evaluation. Table 1 shows the average pixel-to-pixel CCs between the SDO/HMI magnetograms and AI-generated ones with a full dynamic range. Our model shows that the average pixel-to-pixel CCs after $8 \times 8$ binning are $0.88,0.91$, and 0.70 for the 1342 full disk, 2926 ARs, and 2684 QRs, respectively. These imply that our model improves the generation of magnetograms when compared with the results of KPL19 and J20. In addition, the pixel-to-pixel CCs between the target and our AI-generated data after $4 \times 4$ binning show better results than the CCs between the target and those of KPL19 after $8 \times 8$ binning. The latitudinal or longitudinal heliographic resolution at the center of the solar disk is approximately $1^{\circ}$ per pixel after $8 \times 8$ binning.

Figure 3 shows scatter plots between two objective measures of SDO/HMI magnetograms and AI-generated ones for the same data sets when calculating the average pixel-to-pixel CCs. We compare the TUMF between the target and AI-generated data (Figures 3(a)-(c)). The TUMF CCs are $0.99,0.94$, and 0.94 , the R2 scores are $0.97,0.87$, and 0.85 , and the slopes are $1.02,0.82$, and 1.05 for the full disk, ARs, and QRs, respectively. We compare net magnetic flux (NMF) between

Table 1
Average Pixel-to-pixel CCs between SDO/HMI Magnetograms and AIgenerated Magnetograms for Full Disk, ARs, and QRs

|  | Pixel-to-pixel CC |  |
| :-- | :--: | :--: |
|  | $8 \times 8$ Binning |  |
|  | Full Disk <br> $(1342)$ | AR <br> $(2926)$ |
| AISFM 3.0 (Ours) | 0.88 | 0.91 |
| AISFM 2.0 (J20) | 0.81 | 0.79 |
| AISFM 1.0 (KPL19) | 0.77 | 0.66 |

Note. The results of J20 and KPL19 are shown for comparison.
the real and AI-generated data (Figures 3(d)-(f)). The NMF CCs are $0.90,0.94$, and 0.96 , the R2 scores are $0.79,0.87$, and 0.90 , and the slopes are $0.90,0.94$, and 0.96 for the full disk, ARs, and QRs, respectively. Most values of TUMF and NMF fall on the diagonal line (the black dotted line in Figure 3) through the origin. These values support that our model can generate consistent magnetic fluxes.

Figure 3(b) shows that our model slightly underestimates the TUMF of the strong ARs more than the real ones. We examine why a small portion of the ARs show underestimated TUMFs. We find that for some ARs, the AI-generated data do not produce strong magnetic fields. In these cases, the ARs do not have high intensities at 304,193 , and $171 \AA$ images but have high intensities at $94 \AA$ (Fe XVIII) and $131 \AA$ (Fe VIII, XXI) of SDO/AIA. These shorter-wavelength channels are characterized by high-temperature emissions (O'dwyer et al. 2010; Warren et al. 2011). We think that the 94 and $131 \AA$ observations are helpful in generating strong magnetic fluxes for the ARs. However, in this study, we do not use the 94 and $131 \AA$ images to train our model because the STEREO/EUVI only have filter bands of 171, 195, 284, and $304 \AA$.

For the ARs, we evaluate our model based on the similarity of the AI-generated magnetograms with the real ones. We use a
![img-3.jpeg](img-3.jpeg)

Figure 4. (a)-(f) Solar frontside SDO/HMI magnetogram, the farside STEREO composite EUV images (red: $304 \AA$; green: $193 \AA$; and blue: $171 \AA$ ), and AISFMs on 2011 March 5. The purple and brown boxes represent NOAA AR 11166 and 11165, respectively. (g)-(j) The two ARs are zoomed and converted from full disk data to heliographic coordinated maps. The color map of zoomed ARs shows the large dynamic range of values in gauss.

Structural SIMilarity (SSIM) method, which is widely used to measure the degree of similarity and consider measurements of luminance, contrast, and structure between two images. The SSIM produces a value between 0 and 1 . The maximum value of 1 indicates that they show perfectly similar structure, and vice versa. We use the dynamic range of the pixel values to compute the SSIM from -1500 G to +1500 G considering our test data sets. Our model shows that the average SSIM value for the ARs is 0.74 with a standard deviation of 0.09 . After $8 \times 8$ binning, the average SSIM value is 0.93 with a standard deviation of 0.09 .

### 4.2. Generation of Solar Farside Magnetograms

We generate farside magnetograms from the STEREO EUV images and frontside reference data pairs from the model. The dates of AISFMs for STEREO A (AISFMs A) are from 2011 January 1 to 2021 June 30, and those of the AISFMs for STEREO B (AISFMs B) are from 2011 January 10 to 2014 September 27. In 2011 January, the position of STEREO A was about $85^{\circ}$ longitude, and that of STEREO B was about $-90^{\circ}$ longitude in Stonyhurst heliographic coordinates. They drift away at a rate of about $22^{\circ}$ per year from Earth.

Figure 4 shows multiviewpoint data from SDO, and STEREO A and B on 2011 March 5. The position of STEREO A is about $88^{\circ}$ heliographic longitude near the west limb, and the position of STEREO B is about $265^{\circ}$ heliographic longitude near the east limb of the solar frontside (Figure 4(b)). We select an NOAA AR 11165 to the west, and an NOAA AR 11166 to the east of the solar disk from the frontside SDO/HMI magnetogram (Figure 4(e)). AR 11165 is observed by STEREO A (Figure 4(c)) and AISFM A (Figure 4(f)). The TUMF of AR 11165 from the SDO/HMI magnetogram in Figure 4(i) is about $1.85 \times 10^{22} \mathrm{Mx}$, and that from AISFM A in Figure 4(j) is about $1.74 \times 10^{22} \mathrm{Mx}$. AR 11166 is observed by STEREO B (Figure 4(a)) and AISFM B (Figure 4(d)). The TUMF of AR11166 from the SDO/HMI magnetogram in Figure 4(h) is $2.50 \times 10^{22} \mathrm{Mx}$, and that from AISFM B in Figure 4(g) is $2.71 \times 10^{22} \mathrm{Mx}$. The TUMFs of the ARs from our AISFMs are consistent with those of the real one, and the distributions of the magnetic fields look like the real one.

Figure 5 shows the temporal evolution of AR 11166, which is shown in Figures 4(g)-(h). We track the AR for three solar rotations at a Carrington rotation rate. We calculate the TUMF
![img-4.jpeg](img-4.jpeg)

Figure 5. (a) A temporal variation of the TUMF in NOAA AR 11166 from 2011 February 1 to May 7. The green, red, and blue dots are TUMFs from SDO/HMI, AISFM A, and AISFM B, respectively. (b)–(k) A series of magnetograms tracking the AR over three solar rotations. The magnetograms are converted from full disk data to heliographic coordinated maps. The color map of the magnetic fields is the same as shown in Figures 4(g)–(j).

For each area including the AR when the HMI or AISFMs are available, and the results are shown in Figure 5(a). We consider the ARs within 60° from the disk center of the HMI or AISFMs. One impressive thing is that the TUMFs from the SDO/HMI and those from our AISFMs between solar frontside and farside are smoothly overlapped, demonstrating that it is possible to monitor the change in magnetic flux quantitatively using our method. Figures 5(b)–(k) show magnetograms of the tracked AR. Combining SDO/HMI magnetograms and AISFMs makes it possible to continuously monitor the evolution of the magnetic field distribution over the solar surface.

Figure 6 shows the tracking of ARs over the solar surface from 2012 December 7 to 2013 January 20. The ARs from SDO/HMI and AISFM A and B are converted from full disk data to heliographic coordinated maps. When the data are not available, we replace them with the nearest available ones. The position of STEREO A is about 130° heliographic longitude, and that of STEREO B is about 230° heliographic longitude. AISFM A and B show their consistent growth and decay.

Figure 7 shows comparisons between an SDO/HMI magnetogram and two AISFMs of solar cycle 25. When the solar cycle changes, all of the solar magnetic field patterns are reversed (Hale & Nicholson 1925). On 2021 May 20, the position of STEREO A is about 309° heliographic longitude (Figure 7(b)). The magnetic field polarities of our AISFM A (Figure 7(d)) are consistent with the ones of an SDO/HMI magnetogram (Figure 7(e)), which is obtained at the frontside after 4 days. As shown in Figure 7(c), AISFM 2.0 cannot produce reasonable polarities that can be identified from HMI magnetograms. It is noted that the AISFM 2.0 is generated from the STEREO A EUV observations (Figure 7(a)) without reference information from the solar frontside. The polarity distributions of AISFM 2.0 are similar to those of solar cycle 24. We mark NOAA AR 12824 with yellow dotted circles in Figures 7(c)–(e). The AR from the HMI magnetogram shows leading positive and following negative polarities. Our AISFM 3.0 represents the polarity distributions of the AR well.

Figure 8 shows comparisons between mean polar field strengths from the SDO/HMI magnetograms and those from AISFMs A and B. The results from AISFM A and B are presented at 5 day intervals. The polar fields computed from our AISFMs 3.0 follow the trend of the polar field reversal process shown by the computed results from the SDO/HMI.
![img-5.jpeg](img-5.jpeg)

Figure 6. Temporal evolution of the ARs obtained by SDO/HMI, and AISFM A and B from 2012 December 7 to 2013 January 20. Three colored boxes denote the ARs on the solar surface. An animation of this figure is available. The real-time duration of the animation is 36 s .
(An animation of this figure is available.)

We use mean radial fields of the HMI polar field data series, hmi.meanpf_720s, which are provided from the JSOC. The LOS magnetic fields are converted to radial fields, under the assumption that the actual field vector is radial. The mean polar field strength is calculated from the values within $\pm 45^{\circ}$ longitude and above $60^{\circ}$ latitude (see Sun et al. 2015 for details). We calculate mean polar fields of the AISFMs according to their study, and our results are consistent with their results.

One may ask the question, ""How could one technique find the magnetic polarity distribution from EUV images?"" KPL19 showed that deep learning can generate solar farside magnetograms, with Hale-patterned ARs being well replicated from the EUV $304 \AA$ images. The pixel values, i.e., intensities, of the EUV images can give our model the distribution of magnetic fields. Ugarte-Urra et al. (2015) showed that integrated $304 \AA$ light curves can be used as a proxy for the TUMF of the AR. Based on these results, several studies tried to generate solar magnetograms from the EUV $304 \AA$ images using deep learning (Alshehhi 2020; Dani et al. 2022). J20 generated more realistic magnetograms using the EUV 304, 193, and $171 \AA$ images. The EUV 193 and $171 \AA$ passbands, which correspond to the corona and upper transition region, respectively, are widely used for detection of coronal holes (Garton et al. 2018; Linker et al. 2021). The distribution of coronal holes is related to that of open flux regions, i.e., unipolar regions (Lowder et al. 2014). The multichannel EUV images can give the model information about the distribution of not only the ARs, but also the unipolar regions related to the
coronal holes. Here we use reference solar frontside magnetograms and EUV images to generate the farside magnetograms. Hale et al. (1919) noted that most leading spots have opposite polarities in opposite hemispheres. Hale's law correctly predicts polarities of the ARs about $90 \%$ of the time (Li 2018). The reference data sets give our model overall magnetic field polarity distributions including the polarities of leading spots. Based on these arguments, our model successfully generates the farside magnetograms of solar cycles 24 and 25. However, it may not be exact when the magnetic fields of the ARs do not follow Hale's law. It is especially difficult to predict the polarity distributions of rapidly emerging ARs, which were not observed at the reference data sets. Our model generates magnetograms based on a large amount of iterative training to produce accurate magnetic field distributions from the input data sets. If additional training data sets with different distributions of magnetic field polarities, various shapes of ARs, and appearances and disappearances of ARs are provided, we expect our model to be able to predict more realistic magnetograms.

## 5. Data Release

Here we first release the AISFMs at the KDC for SDO. ${ }^{5}$ The names of AISFMs 3.0 A and B recorded in the AI-generated data base are aisfm_v3_stereo_a and aisfm_v3_stereo_b, respectively. There are 7913 AISFMs A from 2011 January 1 to 2021 June 30, and 2890 AISFMs B from 2011 January 10 to

[^0]
[^0]:    5 http://sdo.kasi.re.kr
![img-6.jpeg](img-6.jpeg)

**Figure 7.** A solar farside STEREO A EUV image and AISFMs on 2021 May 20, and the frontside SDO/HMI magnetogram on 2021 May 24. The yellow dotted circles represent NOAA AR 12824. A result of J20 (AISFM A 2.0) is shown together for comparison with our result (AISFM A 3.0).

![img-7.jpeg](img-7.jpeg)

**Figure 8.** Comparisons of solar polar field strengths from SDO/HMI and those from AISFM A and B. Yellow (purple) dots represent the mean radial field strength above 60° in the south (north) of SDO/HMI magnetograms; the lines represent the smoothed average. Brown (green) and red (blue) dots represent the mean radial field strength above 60° in the south (north) of AISFMs A and B, respectively.

2014 September 27 with 6 hr cadence. When the model inputs from the STEREO and the SDO have poor quality data, we do not produce AISFMs. The number of AISFMs A for 2015 is smaller than that for other years, because the contact with STEREO A was interrupted as it passed behind the Sun.

The AISFMs 3.0 are saved in the flexible image transport system (FITS) format (Pence et al. 2010). The data have 1024 × 1024 pixels, and the solar radius is fixed at 450 pixels. The data outside the solar radius are filled with not-a-number values like those of the SDO/HMI magnetograms. The data
Table A1
Ephemeris Keywords for the AISFMs

| Keyword | Description | Format or Unit |
| :-- | :-- | :-- |
| INPUTDAT | Observer of the model input data | STEREO_A (or _B) |
| DATE-OBS | Mean date and time of STEREO observations | universal Time |
| CTYPE1, CTYPE2 | Helioprojective (Cartesian) system | arcseconds |
| HGLN_OBS, HGLT_OBS | Stonyhurst heliographic longitude and latitude of STEREO | degrees |
| CRLN_OBS, CRLT_OBS | Carrington heliographic longitude and latitude of STEREO | degrees |
| RSUN_OBS, RSUN | Observed radius of the Sun in arcseconds | arcseconds |
| RSUN_REF | Reference radius of the Sun | meters |
| R_SUN | Pixel size of the solar radius | pixels |
| DSUN_OBS | Distance between the center of the Sun and STEREO | meters |
| DSUN_REF | Average distance from the Sun to Earth (1 au) | meters |

inside the solar radius represent magnetic fields along the LOS to STEREO A or B. The coordinate information of STEREO is stored in the FITS header keywords (see more details in the Appendix). We also provide example codes to understand the AISFMs at https://github.com/JeongHyunJin/AISFM3.0, and the codes are archived on Zenodo (Jeong 2022).

## 6. Summary and Conclusion

In this study we have generated improved solar farside magnetograms by the STEREO and SDO data sets using a deep-learning model. For this work, we have improved our model including the CC-based objectives and used model inputs on the farside STEREO EUV observations together with the frontside SDO data pairs. We selected 6437 pairs of input and target data sets from 2011 January 1 to 2021 June 30 for the model training. Targets for the training are SDO/HMI magnetograms. Inputs for the training consist of the EUV images, and the pairs of EUV images and magnetograms obtained 27.3 days prior. We have evaluated the model using test data sets not used for training.

The main results of this study are as follows. First, we improved the AI-generated magnetograms. The average pixel-to-pixel CCs between the SDO/HMI magnetograms and our AI-generated ones after $8 \times 8$ binning are $0.88,0.91$, and 0.70 for the full disk, ARs, and QRs, respectively, which are noticeably better than the previous results. We obtained good agreement between the TUMFs calculated from the SDO/HMI magnetograms and those calculated from the AI-generated ones, as well as the NMFs calculated from the HMI data and those calculated from the AI-generated ones. Second, we generate more realistic solar farside magnetograms using the STEREO EUV images and the frontside data pairs by the model. We compare the magnetic fields of ARs from the AISFMs and HMI when the positions of STEREO A and B are near the west and east limb of the solar frontside, respectively. Together with the AISFMs and HMI, we can continuously monitor the temporal evolution of the TUMF of an AR over three solar rotations. Third, our model can generate AISFMs of solar cycles 24 and 25, in which data have consistent magnetic field polarities with those of nearby frontside ones. We show that the temporal variation of the mean polar fields calculated from the AISFMs well represents the Sun's magnetic field reversal process.

Our method has several advantages over the conventional methods. First, our AISFMs can improve studies using solar magnetic flux distributions. We can track the ARs and study their flux evolution at the solar surface using the AISFMs together with the frontside magnetograms, as shown in Figure 5
(also see KPL19). Second, we can improve global coronal magnetic field extrapolation from the synchronic maps with our AISFMs. In J20, we showed that global extrapolations from the synchronic maps with AISFMs were more consistent with EUV observations than those from conventional data in view of the ARs and coronal holes. Third, we expect that our AISFMs provide better input data for heliospheric solar wind propagation models such as WSA-ENLIL (Arge \& Pizzo 2000) and EUHFORIA (Pomoell \& Poedts 2018). We also acknowledge that our method has a couple of limitations. First, physical quantities based on the pixel-to-pixel distribution of magnetic fields (e.g., neutral line) may not be exact. Second, small-scale magnetic field configurations, such as magnetic cancellation features, may not be well produced.

This study used a large amount of STEREO and SDO data. We appreciate numerous team members who have contributed to the success of the STEREO and SDO missions. We acknowledge the community efforts devoted to the development of the open-source packages that were used for this work. This work was supported by the Korea Astronomy and Space Science Institute (KASI) under the R\&D program (project Nos. 2022-1-850-05 and 2022-1-850-08) supervised by the Ministry of Science and ICT, and the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF2020R1C1C1003892, NRF-2021R1I1A1A01049615).

Software: PyTorch (Paszke et al. 2019), NumPy (Harris et al. 2020), Matplotlib (Hunter 2007), SciPy (Virtanen et al. 2020), Astropy (Robitaille et al. 2013; Price-Whelan et al. 2018), SunPy (The SunPy Community et al. 2020).

## Appendix

## FITS Header Keywords

AISFMs are stored in FITS files, each with a keyword header containing the information on the data. The keywords follow World Coordinate System conventions for describing the physical coordinate values of the data pixels (Greisen \& Calabretta 2002), and several ephemeris keywords are provided in Table A1. Since our data are generated by a deep-learning model, not an observational one, we store the keyword name not in OBSERVTRY (observatory) but in INPUTDAT (input data). The AISFMs are generated based on the features from three EUV images of STEREO A (or B). Thus, we record the mean date and time of the three EUV observations in the DATE-OBS keyword. More detailed information on the model inputs is stored in the HISTORY keyword.
## ORCID IDs

Hyun-Jin Jeong (1) https://orcid.org/0000-0003-4616-947X
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Eunsu Park (1) https://orcid.org/0000-0003-0969-286X
Harim Lee (1) https://orcid.org/0000-0002-9300-8073
Ji-Hye Baek (1) https://orcid.org/0000-0002-0230-4417

## References

Alshehhi, R. 2020, in Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition Workshops (Piscataway, NJ: IEEE), 204
Arge, C., \& Pizzo, V. 2000, JGR, 105, 10465
Atmaja, B. T., \& Akagi, M. 2021, JPhCS, 1896, 012004
Babcock, H. W. 1953, ApJ, 118, 387
Buduma, N., \& Locascio, N. 2017, Fundamentals of Deep Learning: Designing Next-generation Machine Intelligence Algorithms (Sebastopol, CA: O'Reilly Media)
Caplan, R., Downs, C., \& Linker, J. 2016, ApJ, 823, 53
Dani, T., Muhamad, J., Nurzaman, M., et al. 2022, JPhCS, 2214, 012016
Garton, T. M., Gallagher, P. T., \& Murray, S. A. 2018, JSWSC, 8, A02
Goodfellow, I., Bengio, Y., \& Courville, A. 2016, Deep Learning (Cambridge, MA: MIT Press)
Greisen, E. W., \& Calabretta, M. R. 2002, A\&A, 395, 1061
Hale, G. E., Ellerman, F., Nicholson, S. B., \& Joy, A. H. 1919, ApJ, 49, 153
Hale, G. E., \& Nicholson, S. B. 1925, ApJ, 62, 270
Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, Natur, 585, 357
Higgins, P. A., Gallagher, P. T., McAteer, R. J., \& Bloomfield, D. S. 2011, AdSpR, 47, 2105
Howard, R. A., Moses, J., Vourlidas, A., et al. 2008, SSRv, 136, 67
Hunter, J. D. 2007, CSE, 9, 90
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2017, in Proc. 2017 IEEE Conf. on Computer Vision and Pattern Recognition (Piscataway, NJ: IEEE), 1125
Jeong, H.-J. 2022, JeongHyunJin/Pix2PixCC: Pix2PixCC model: an improved image-to-image translation model to use scientific data sets, vSoftware, Zenodo, doi:10.5281/zenodo. 6668849
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Judge, P., Rempel, M., Ezzeddine, R., et al. 2021, ApJ, 917, 27
Kaiser, M. L., Kucera, T., Davila, J., et al. 2008, SSRv, 136, 5
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Lawrence, I., \& Lin, K. 1989, Biometrics, 45, 255
Lemen, J. R., Akin, D. J., Boerner, P. F., et al. 2011, The Solar Dynamics Observatory (Berlin: Springer), 17
Li, J. 2018, ApJ, 867, 89

Liewer, P., Qiu, J., \& Lindsey, C. 2017, SoPh, 292, 146
Lim, D., Moon, Y.-J., Park, E., \& Lee, J.-Y. 2021, ApJL, 915, L31
Linker, J. A., Heinemann, S. G., Temmer, M., et al. 2021, ApJ, 918, 21
Liu, J., Wang, Y., Huang, X., et al. 2021, NatAs, 5, 108
Lowder, C., Qiu, J., Leamon, R., \& Liu, Y. 2014, ApJ, 783, 142
Mao, X., Li, Q., Xie, H., et al. 2017, in Proc. 2017 IEEE Int. Conf. on Computer Vision (Piscataway, NJ: IEEE), 2794
Marnerides, D., Bashford-Rogers, T., \& Debattista, K. 2021, Senso, 21, 4032
Müller, D., Cyr, O. S., Zouganelis, I., et al. 2020, A\&A, 642, A1
O'dwyer, B., Del Zanna, G., Mason, H., Weber, M., \& Tripathi, D. 2010, A\&A, 521, A21
Park, E., Jeong, H.-J., Lee, H., Kim, T., \& Moon, Y.-J. 2021, NatAs, 5, 111
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Pastor Yabar, A., Martínez González, M., \& Collados, M. 2015, MNRAS, 453, L69
Paszke, A., Gross, S., Massa, F., et al. 2019, Advances in Neural Information Processing Systems 32, ed. H. Wallach et al., https://papers.nips.cc/paper/ 2019/hash/blBca288fee7f92f2bfa9f7012727740-Abstract.html
Pence, W. D., Chiappetti, L., Page, C. G., Shaw, R. A., \& Stobie, E. 2010, A\&A, 524, A42
Pietarila, A., Bertello, L., Harvey, J., \& Pevtsov, A. 2013, SoPh, 282, 91
Pomoell, J., \& Poedts, S. 2018, JSWSC, 8, A35
Price-Whelan, A. M., Sipőcz, B., Günther, H., et al. 2018, AJ, 156, 123
Rana, A., Singh, P., Valenzise, G., et al. 2019, ITIP, 29, 1285
Robitaille, T. P., Tollerud, E. J., Greenfield, P., et al. 2013, A\&A, 558, A33
Scherrer, P. H., Schou, J., Bush, R., et al. 2012, SoPh, 275, 207
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Solanki, S. K., del Toro Iniesta, J., Woch, J., et al. 2020, A\&A, 642, A11
Son, J., Cha, J., Moon, Y.-J., et al. 2021, ApJ, 920, 101
Sterling, A. C., Moore, R. L., \& Hara, H. 2012, ApJ, 761, 69
Sun, X., Hoeksema, J. T., Liu, Y., \& Zhao, J. 2015, ApJ, 798, 114
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Ugarte-Urra, I., Upton, L., Warren, H. P., \& Hathaway, D. H. 2015, ApJ, 815, 90
Vallejos, R., Pérez, J., Ellison, A. M., \& Richardson, A. D. 2020, Spat. Stat., 40, 100405
van Driel-Gesztelyi, L., \& Green, L. M. 2015, LRSP, 12, 1
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Waldmeier, M. 1955, Ergebnisse und Probleme der Sonnenforschung (Leipzig: Geest \& Portig)
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., et al. 2018, in Proc. 2018 IEEE Conf. on Computer Vision and Pattern Recognition (Piscataway, NJ: IEEE), 8798
Warren, H. P., Brooks, D. H., \& Winebarger, A. R. 2011, ApJ, 734, 90
Wiegelmann, T., Thalmann, J. K., \& Solanki, S. K. 2014, A\&ARv, 22, 78
Zhou, C., Xia, C., \& Shen, Y. 2021, A\&A, 647, A112"
Eun-Young Ji et al 2024 - Construction of global IGS-3D electron density (Ne) model by deep learning.pdf,"# Construction of global IGS-3D electron density $\left(N_{e}\right)$ model by deep learning 

Eun-Young $\mathrm{Ji}{ }^{\mathrm{a}}$, Yong-Jae Moon ${ }^{\mathrm{a}, \mathrm{b}, *}$, Young-Sil Kwak ${ }^{\mathrm{c}}$, Kangwoo $\mathrm{Yi}^{\mathrm{a}}$, Jeong-Heon Kim ${ }^{\mathrm{c}}$<br>${ }^{a}$ School of Space Research, Kyung Hee University, Yongin, South Korea<br>${ }^{\text {b }}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, South Korea<br>${ }^{\text {c }}$ Korea Astronomy and Space Science Institute (KASI), Daejeon, South Korea

## A R T I C L E I N F O

Handling editor: Dora Pancheva

## Keywords:

Deep learning
Ionospheric electron density profile
IGS TEC

## A B S T R A C T

In this study, we construct a global IGS-3D $N_{e}$ model that generates global 3-D electron density $\left(N_{e}\right)$ from International Global Navigation Satellite Systems (GNSS) Service (IGS) total electron content (TEC) data through deep learning. As a first step towards this, we make a model to generate a vertical electron density profile from a TEC value using Multi-Layer Perceptron (MLP). In this process, we use the vertical electron density profiles and the corresponding TEC values of the IRI-2016 model from 2001 to 2008 for training, 2009 and 2014 for validation, and 2010 to 2013 for a test. The next step is to generate global IGS electron density profiles using the global IGS TECs as input data for the model, which is called the global IGS-3D $N_{e}$ model. We evaluate the IGS-3D $N_{e}$ model by comparing the electron density profiles from the incoherent scatter radars (ISRs) at three stations with the IGS-3D $N_{e}$ model from 2010 to 2013. The evaluation shows that the electron density profiles from the IGS-3D $N_{e}$ model are closer to the ISR data than those of the IRI model, especially at high latitudes. The IGS-3D $N_{e}$ model shows that the averaged root mean square error (RMSE) values between IGS and ISR electron density profiles are $0.37 \log \left(\mathrm{~m}^{-3}\right), 0.22 \log \left(\mathrm{~m}^{-3}\right)$, and $0.34 \log \left(\mathrm{~m}^{-3}\right)$ for all test datasets at Jicamarca, Millstone Hill, and EISCAT stations, respectively. These results suggest that our method has sufficient potential to enhance the ability to predict global electron density profiles.

## 1. Introduction

The ionosphere is a region of about $60 \mathrm{~km}-1000 \mathrm{~km}$ above the Earth's atmosphere, containing most of the thermosphere and parts of the mesosphere and exosphere. It is very important in practical applications such as communication and satellite navigation. In particular, ionospheric electron density is a significant factor in radio wave propagation. Changes in the space environment such as solar activity cause disruptions in ionosphere electron density, affecting scientific analysis and practical applications.

To fully understand the state of the ionosphere, information on the global three-dimensional (3-D) ionospheric electron density distribution is essential. However, as it is difficult to continuously observe the ionospheric electron density in three dimensions globally, efforts have been made to obtain the information through modeling using various methods. The most representative global 3-D electron density model is the International Reference Ionosphere (IRI) model (Bilitza, 2001; Bilitza and Reinisch, 2008; Bilitza et al., 2017, 2022). The IRI model is an empirical model based on a global range of ground and space data. It
provides the electron density for a given location, time, and date in the altitude range from about 50 km to 2000 km . In addition, Hajj and Romans (1998) estimated the electron density profiles obtained from the Global Positioning System Meteorology (GPS/MET) using the Abel inversion technique. They statistically compared ionospheric peak density (NmF2) values obtained from GPS/MET electron density profiles with the observed NmF2 values, and showed an accuracy of about 20\%. Yue et al. (2012) generated a global 3-D ionospheric electron density based on the data assimilation technique using the IRI-2007 model and a Kalman filter technique. They statistically compared the electron density profiles and critical frequency of ionosphere (foF2) obtained from the model with the observed data. The results of this comparison showed that the model had smaller deviations and biases than those of the IRI-2007 model. Li et al. (2021a,b) built a high precision global empirical model, named $\alpha$-Chapman Based Electron Density Profile Model, using the Constellation Observing System for Meteorology, Ionosphere, and Climate (COSMIC)-1 electron density profiles. Compared with observed data, their model captured the ionospheric climatology well and described nearly $80 \%$ of the variability of the

[^0]
[^0]:    * Corresponding author. School of Space Research, Kyung Hee University, Yongin, South Korea. E-mail address: moonyj@khu.ac.kr (Y.-J. Moon).
    https://doi.org/10.1016/j.jastp.2024.106370
    Received 25 May 2023; Received in revised form 4 September 2024; Accepted 11 October 2024
    Available online 16 October 2024
    1364-6826/© 2024 Published by Elsevier Ltd.
electron density profile in the F region.
Recently, deep learning, one of the machine learning methods based on artificial neural networks (ANNs) that automatically performs feature extraction and learning through a combination of nonlinear transformation methods, has been applied to predict variations in ionosphere parameters such as ionospheric peak density and height (NmF2 and hmF2), total electron content (TEC), and critical frequency of ionosphere (foF2) (e.g., Sai Gowtam and Tulasi Ram, 2017; Liu et al., 2020; Ji et al., 2020; Lee et al., 2021). The ionosphere models developed based on deep learning showed fairly accurate predictions. For example, Ji et al. (2020) improved the predictive ability of IRI TEC from the International Global Navigation Satellite Systems (GNSS) Service (IGS) TEC based on deep learning. Moreover, global 3-D electron density models based on deep learning have also been developed. For example, Sai Gowtam et al. (2019) developed an ANN-based global 3-D ionospheric model (ANNIM-3D) by assimilating long-term ionospheric observations. They found that the electron densities derived from their ANNIM-3D model were in good agreement with the data observed on the ground and by satellite. Their model also successfully reproduces the large-scale ionospheric phenomena. Li et al. (2021a) developed a completely global ionospheric 3-D electron density model based on an artificial neural network (ANN-TDD) using the long-term observations of COSMIC, Fengyun-3C, and Digisonde. They found that the predicted accuracy of the ANN-TDD model is $30 \%-60 \%$ higher than the IRI-2016 at the Millstone Hill and Jicamarca incoherent scatter radars (ISRs) under quiet space weather. Their model also successfully reproduces the large-scale horizontal-vertical ionospheric electrodynamic features.

Although the preceding models based on theory, experience, and deep learning have contributed to realizing the global 3-D ionospheric electron density distribution, continuous efforts are needed to develop a model that can produce results closer to actual observations. Therefore, in this study, as part of this effort, based on deep learning, we propose a method to realize a global 3-D ionospheric electron density distribution from the global TEC distribution that is close to the actual value. In addition, while previous global 3-D electron density models based on deep learning generated the electron density profile using several input data such as solar wind, solar, and geomagnetic activity indices (e.g., Sai Gowtam et al., 2019; Li et al., 2021b), our method is that the electron density profile can only be generated from the TEC. As a first step for this purpose, based on the definition of TEC as the total number of electrons incorporated between two points, such as between the height of a terrestrial receiver and a transmitting satellite, we make a model that generates an electron density profile from a TEC value by MLP. That is, the purpose of model development is to generate improved electron density profiles from deep learning models using inversion method. To train the model generating the global electron density profile from the global TEC, we use the pairs of TEC and vertical electron density profiles from the IRI-2016 model. The IRI model is an empirical model that is most efficient at mid-latitudes during magnetically quite conditions, but it is not designed to account for individual dates with specific solar and geomagnetic conditions. However, since we need to obtain a complete global TEC and electron density profile for the same time period, we have no choice but to use the data of the IRI model. Using the data of the IRI-2016 model, we train the MLP-based model through the application of inversion to calculate TEC by integrating electron density as follows:
$\int N_{e, I R I}(z) d z=T E C_{I R I}$,
where $N_{e, I R I}$ and $T E C_{I R I}$ are the vertical electron density and vertical total electron content generated by IRI-2016 model, respectively.

The next step is to generate the global IGS electron density profiles using realistic global IGS TEC as input data for the trained model as follows:
$\int N_{e, I G S}(z) d z=T E C_{I G S}$,
where $N_{e, I G S}$ is IGS vertical electron density generated from IGS TEC and $T E C_{I G S}$ is IGS vertical total electron content.

In other word, we construct a global IGS-3D electron density $\left(N_{e}\right)$ model using the IGS TEC as input data instead of the IRI TEC in the trained model. To evaluate the IGS-3D $N_{e}$ model constructed through the application method of inversion, we compare the electron density profiles generated from the model with those from the ISRs at three stations.

Nowadays there are so many generation models such as chatGPT and Sora. These models did not experience the same questions but reasonably answer them since they have been trained using similar questions a lot. They assume that when some students have the experience (training in the deep learning model) to solve some integral equations, they can solve other similar equations. Recently, many inversion problems in astronomy and space science have been solved successfully [e.g. Jang et al., 2021; Lee et al., 2022]. Our deep learning model is based on the same assumption.

The main purpose of this study is to present a method to design a global 3-D electron density model by applying a deep learning-based inversion method. The inversion method can be applied to model development under the premise of calculating TEC by integrating the electron density. Our model is a deep learning model that applies this inversion method and shows the possibility of generating an electron density profile if the TEC value of a specific point is known.

## 2. Data and method

### 2.1. Data

The IGS working group was set up in 1998 (Hernández-Pajares et al., 2009). Since then, the four IGS Ionosphere Associate Analysis Centers, which are the Jet Propulsion Laboratory, the Center for Orbit Determination in Europe, the European Space Operations Center of the European Space Agency, and the Universitat Politècnica de Catalunya, have continuously been contributing to reliable global TEC maps. We use the IGS final vertical TEC maps obtained from the National Aeronautics and Space Administration Crustal Dynamics Data Information System (ft p://cddis.nasa.gov/gnss/products/ionex/). It has a resolution of 2 h in time, $2.5^{\circ}$ in geographic latitude, and $5^{\circ}$ in geographic longitude.

For the global IRI TEC and IRI electron density profiles, we utilize a version of the IRI-2016, whose source code is available at http://www. irimodel.org/. We adopt the URSI option foF2 and NeQuick topside model to calculate the TEC and electron density profiles. The global IRI TEC are calculated with the same temporal and spatial resolutions as the IGS TEC at 2 h in time, $2.5^{\circ}$ in geographic latitude, $5^{\circ}$ in geographic longitude, respectively. The global IRI electron density profiles are calculated at each place with resolution of 25 km in altitude from $100-700 \mathrm{~km}$.

We use the pairs of vertical electron density profiles and TEC values (about 4.2 million) of the IRI-2016 model from 2001 to 2013. For the deep learning model, we use the vertical density profiles and TEC values from 2001 to 2008 for training, 2009 and 2014 for validation, and 2010 to 2013 for the test. Using the model, we derive the IGS vertical density profiles from IGS TEC values from 2010 to 2013.

For evaluation of our model, we use the electron density profiles, which are called ISR electron density profile, observed at low, middle, and high latitudes as follows: Jicamarca ( $11.9^{\circ} \mathrm{S}, 76.8^{\circ} \mathrm{W}$, dip latitude $=$ $1^{\circ} \mathrm{N}$ ); Millstone Hill ( $42.6^{\circ} \mathrm{N}, 288.5^{\circ} \mathrm{E}$, invariant latitude $=55^{\circ}$ ); European Incoherent Scatter Tromsø UHF Radar (EISCAT: $69.6^{\circ} \mathrm{N}, 19.2^{\circ} \mathrm{E}$, invariant latitude $=66^{\circ}$ ). The Jicamarca and Millstone Hill ISR data are taken from the Madrigal Database at Jicamarca Radio Observatory (htt p://jro1.igp.gob.pe/madrigal/) and Millstone Hill (http://millstonehill. haystack.mit.edu/). The EISCAT ISR data are taken from the EISCAT database in the National Institute of Polar Research (NIPR, http://pc 115.seg20.nipr.ac.jp/www/eiscatdata/index.html). We only use the ISR electron density profiles with an elevation angle of $85^{\circ}$ or higher and
an observation time of 4 h or more per day. Table 1 shows the numbers of days available from three ISRs from 2010 to 2013. We selected this period because all ISR data at three stations have been unavailable since then. Since the ISR electron density profiles have an irregular altitude step, we reproduce each electron density profile with a regular altitude step of 25 km using a simple linear interpolation in a logarithmic scale, which is the simplest way to estimate the value of a function between two data point in the electron density profile from the formula as follows:
$\mathrm{y}=y_{1}+\left(\boldsymbol{x}-\boldsymbol{x}\right) \frac{\boldsymbol{y}_{2}-\boldsymbol{y}_{1}}{\boldsymbol{x}_{2}-\boldsymbol{x}_{1}}$
where, $x_{1} . y_{1}$ are first points, $x_{2} . y_{2}$ are second points, x is the point to perform the interpolation, and y is the interpolated value.

### 2.2. Method

Our model is based on multi-layer perceptron (MLP). MLP is a class of feedforward artificial neural networks (ANNs) that consists of one or more hidden layers between the input and output layers. The MLP has a structure similar to that of a single-layer perceptron. However, it improves the ability of the network by placing one or more hidden layers between the input layer and the output layer so that it can learn even non-linearly separated data. The input layer receives the input signal to the process. In the output layer, essential operations such as prediction and classification are performed. The hidden layer is where the MLP takes care of the computation, whose function weights the input and directs them through activation functions as outputs. The hidden layers perform nonlinear transformations of the inputs entered into the network.

In this study, we use pytorch software for model design and training. We use a feedforward network to make a model that generates a 1-D electron density profile from a TEC value. Fig. 1 shows the structure of our model. We use the following six parameters as input data of the model: day of the year (DOY), universal time (UT, $0-24 \mathrm{~h}$ ), local time (LT, $0-24 \mathrm{~h}$ ), geographic latitude, geographic longitude, and TEC. We normalize the input data to a range of $[-1,1]$, and further apply normalization using trigonometric functions for DOY, UT, LT, geographic latitude, and geographic longitude. We conduct various tests by changing the input parameters, and the results show that the best performance is achieved when using six parameters as input data. Through the hidden layer, an electron density profile is generated in the output layer. We train the model using the training dataset consisting of the pairs of IRI TECs and IRI electron density profiles. We try several different models with different layers and nodes. The optimal number of hidden layers and nodes are 2 and 195, respectively. For the activation function in the hidden layer, we use the Rectified Linear Unit (Nair V. \& Hinton G. E., 2020), a function that outputs the input as it is if the input is 0 or more, and outputs 0 if the input is 0 or less, as shown in the equation $\mathrm{y}=\max (0, \mathrm{x})$. We use Adam optimizer (Kingma, D. P., \& Ba, J., 2014) and Mean Absolute Error (MAE) loss function. We train the model with 3000 epochs and batch size $=1$. Here, 'Epoch' refers to the process of learning the entire data set once. The number of epochs determines how many times the model will train the entire dataset. We confirm from the validation data loss curve that the model's performance stabilizes quickly and does not lead to overfitting, even after extended training sessions. We select and analyze several models after a stable number of training sessions and use the best-performing model trained for 3000

Table 1
Numbers of days available from three ISRs from 2010 to 2013.

| ISR | Jicamarca <br> $(-11.95^{\circ} \mathrm{S}, 283^{\circ} \mathrm{E})$ | Millstone Hill <br> $(42.61^{\circ} \mathrm{N}, 288.5^{\circ} \mathrm{E})$ | EISCAT Tromsø <br> $(67.5^{\circ} \mathrm{N}, 20.24^{\circ} \mathrm{E})$ |
| :-- | :-- | :-- | :-- |
| Number of <br> day | 73 | 259 | 281 |

epochs. Because it is difficult to predict the relationship between the input data and model hyperparameters in advance, we create models using various internal parameters and analyze their performance to determine the final model parameters. Through this training process, the output layer of the IRI TEC-based model, 3-D global electron density profiles, is generated from a given TEC value at specific times and location according to global DOY, UT, LT, latitude, and longitude. That is, the trained model generates the electron density profiles in steps of 25 km from 100 km to 700 km ( 24 output values) for given time, location and TEC. The pair of global IRI TEC and global IRI electron density profiles is used only to generate a corresponding electron density profile from the TEC value. Fig. 2 shows the comparisons of the averaged electron density profiles from the IRI model and the IRI TEC-based model at 12 LT and 23 LT for the test datasets from 2010 to 2013 at three locations by latitude in the vicinity of where ISRs are located. As shown in Fig. 2, the trained model, which is IRI TEC-based model, successfully generates electron density profiles that are quite similar to the corresponding IRI ones, the target data. The averaged root-mean-square error (RMSE) value between IRI electron density profiles and electron density profiles from the model is $0.064 \times 10^{11} \mathrm{~m}^{-3}$ for all test data. We confirm that the trained model well generates the electron density profiles from TEC. Then, the electron density profiles are generated using IGS TEC as input data instead of the IRI TEC in the trained model. We define it the IGS-3D electron density $\left(N_{e}\right)$ model, and the data generated from the model call the IGS electron density profile. Fig. 3 shows an example of global electron density distribution maps generated from the IGS-3D $N_{e}$ model and the electron density profiles observed at three ISR stations (Jicamarca, Millstone Hill, and EISCAT) at 16 UT on August 02, 2011. As shown in Fig. 3, our model well generates the ionospheric peak structure at an altitude similar to the observed the electron density profiles as well as the global 3-D electron density distribution.

## 3. Results and discussions

We make the IGS-3D $N_{e}$ model by applying the method of inverting the IGS TEC to IGS electron density profile. To evaluate the IGS electron density profiles generated from the IGS-3D $N_{e}$ model, we compare them with the electron density profiles of the ISR and IRI model from 2010 to 2013. Fig. 4 presents the electron density profiles of the IRI model, the IGS-3D $N_{e}$ model, and the ISR at Jicamarca station $\left(11.9^{\circ} \mathrm{S}, 76.8^{\circ} \mathrm{W}\right)$ for each representative date: Equinox, December solstice, and June solstice. In Fig. 4, it can be seen that the IGS electron density profile and the ISR electron density profile are very similar at topside at 1 LT during equinox. On the other hand, the IRI electron density profile is quite different from the ISR electron density profile. The IGS electron density profile is also seen to follow the pattern of variation shown in the ISR electron density profile at 13 LT during June Solstice. At 7 LT for all seasons, both IGS and IRI electron density profiles are overall similar to the ISR electron density profiles. In the case of 13 LT for equinox, both IGS and IRI electron density profiles have noticeable deviations on the topside from ISR electron density profiles. This is probably related to the performance of the IRI model at low latitudes where the Jicamarca station is located. The low latitude ionosphere (especially during the daytime) is a region with very large latitudinal changes in the electron density due to equatorial ionization anomaly (EIA). Several studies evaluating the IRI model found a significant difference between the observed data and the IRI model results in the low-latitude ionosphere (e.g., Kumar et al., 2015). This difference may possibly be related to the NeQuick option. In addition, the IRI model tends to underestimate the electron density at low latitude during the solar maximum period (Jee et al., 2005). The difference between the IRI and the IGS electron density profiles is related to the difference between the IRI TEC and the IGS TEC. Consequently, the IGS-3D $N_{e}$ model produces an electron density profile closer to the ISR electron density profile observations than the IRI model, because it takes IGS TECs close to the actual values as an input.
![img-0.jpeg](img-0.jpeg)

**Fig. 1.** The structure of our model.

![img-1.jpeg](img-1.jpeg)

**Fig. 2.** Comparisons of the averaged electron density profiles from the IRI and the IRI TEC-based model for test datasets from 2010 to 2013 at three locations by latitude.

The averaged RMSE values between IGS and ISR electron density profiles are 0.35 log(m<sup>-3</sup>), 0.22 log(m<sup>-3</sup>), and 0.27 log(m<sup>-3</sup>) during equinox, December solstice, and June solstice, respectively. On the other hand, the averaged RMSE values between IRI and ISR electron density profiles are 0.37 log(m<sup>-3</sup>), 0.27 log(m<sup>-3</sup>), and 0.34 log(m<sup>-3</sup>) during equinox, December solstice, and June solstice, respectively. Significantly, there are noticeable improvements in the density profiles on the June solstice.

Fig. 5 shows the electron density profiles of the IRI model, the IGS-3D *N<sub>e</sub>* model, and the ISR at Millstone Hill station (42.6°N, 288.5°E) for each representative date: equinox, December solstice, and June solstice. As seen in Fig. 5, the IGS electron density profiles are similar to the ISR electron density profiles on the topside rather than those of the IRI, with the exception of the December solstice. Notably, the IGS-3D *N<sub>e</sub>* model generates electron density profiles consistent with the ISR electron density profiles at 13 LT and 19 LT for equinox and June solstice, respectively. During December solstice, our model as well as the IRI model generates electron density profiles similar to those of ISR on the
![img-2.jpeg](img-2.jpeg)

**Fig. 3.** Example of global electron density distribution maps generated from the IGS-3D *N*<sub>e</sub> model and the electron density profiles observed at three ISR stations (Jicamarca, Millstone Hill, and EISCAT) at 16 UT on August 02, 2011.

![img-3.jpeg](img-3.jpeg)

**Fig. 4.** The electron density profiles of the IRI model, the IGS-3D *N*<sub>e</sub> model, and the ISR at Jicamarca station for each representative date: equinox, December solstice, and June solstice.
![img-4.jpeg](img-4.jpeg)

Fig. 5. The electron density profiles of the IRI model, the IGS-3D *N*e model, and the ISR at Millstone Hill station for each representative date: equinox, December solstice, and June solstice.

bottom-side, but not on the topside. Fig. 5 reveals that not only the IGS electron density profiles but also the IRI electron density profiles are not significantly different from the ISR electron density profiles except on December solstice. This may be related to the fact that the middle latitude ionosphere has fewer fluctuations and disturbances. Moreover, since there are a lot of observational data in the middle latitudes, the IRI model is known to predict middle latitude regions better than low and high latitudes (Ogwala et al., 2021). Therefore, we guess that the IGS-3D *N*e model generates electron density profiles similar to the ISR electron density profiles. The averaged RMSE values between IGS and ISR electron density profiles are 0.15 log(m<sup>−3</sup>), 0.46 log(m<sup>−3</sup>), and 0.14 log(m<sup>−3</sup>) during equinox, December solstice, and June solstice, respectively. Similarly, the averaged RMSE values between IRI and ISR electron density profiles are 0.19 log(m<sup>−3</sup>), 0.32 log(m<sup>−3</sup>), and 0.13 log(m<sup>−3</sup>) during equinox, December solstice, and June solstice, respectively. This means that our model generates the electron density profile slightly better than the IRI model at Millstone Hill station.

Fig. 6 shows the electron density profiles of the IRI model, the IGS-3D *N*e model, and the ISR at EISCAT station (69.6°N, 19.2°E) for each representative date: equinox, December solstice, and June solstice. With regards to EISCAT, since the number of data satisfying the conditions used in this study is insufficient, the representative days for each season in the example shown in Fig. 6 are different from the dates in Figs. 4 and 5. In Fig. 6, it can be seen that the IGS electron density profiles during equinox and June solstice are very similar to the ISR electron density profiles except for 1 LT. Significantly, our model generates electron density profiles almost identical to the ISR electron density profiles from the bottom-side to the topside at 7 LT during equinox and June solstice. On the other hand, the IRI electron density profiles are slightly different from the ISR electron density profiles. Fig. 6 shows a significant difference between the electron density profiles of the bottom-side generated by the two models and the ISR electron density profiles during the nighttime (1 LT). This may be related to the uncertain thickness parameter (B0) of the IRI model on the bottom-side. Compared to the middle-latitude regions, it is known that B0 of the IRI model does not reflect the dynamics of the diurnal variations over high-latitude regions such as those related to auroral activity (Themens et al., 2014). However, our model generates electron density profiles of the topside which are almost similar to the ISR electron density profiles, except in a few cases. The averaged RMSE values between IGS and ISR electron density profiles are 0.22 log(m<sup>−3</sup>), 0.37 log(m<sup>−3</sup>), and 0.16 log(m<sup>−3</sup>) during equinox, December solstice, and June solstice, respectively. The averaged RMSE values between IRI and ISR electron density profiles are 0.26, 0.57, 0.2 log(m<sup>−3</sup>) during equinox, December solstice, and June solstice, respectively. This means that the IGS-3D *N*e model produces the electron density profile better than the IRI model at EISCAT station, with especially great improvement during the December solstice.

Fig. 7 shows the averaged RMSE values between IGS and ISR electron density profiles and between IRI and ISR ones as a function of local time at Jicamarca, Millstone Hill, and EISCAT stations for all test datasets from 2010 to 2013. At Jicamarca station, the averaged RMSE values of our model are smaller than those of the IRI model during the daytime to evening hours (11 LT ~ 19 LT). On the other hand, the opposite tendency is apparent during the nighttime (21 LT ~ 3 LT). At Millstone Hill station, the averaged RMSE values of both models are lower than those of the other two regions, implying that they have little difference from the ISR electron density profiles. The averaged RMSE values of the two models at all local times are almost the same, but the IGS-3D *N*e model is
![img-5.jpeg](img-5.jpeg)

**Fig. 6.** The electron density profiles of the IRI model, the IGS-3D *N*<sub>e</sub> model, and the ISR at EISCAT station for each representative date: equinox, December solstice, and June solstice.

![img-6.jpeg](img-6.jpeg)

**Fig. 7.** The averaged RMSE values between IGS and ISR electron density profiles and between IRI and ISR ones as a function of local time at Jicamarca, Millstone Hill, and EISCAT stations for all test datasets.

very slightly lower than the IRI one except at 17 LT. At the EISCAT station, the averaged RMSE values of the two models are almost identical to each other during the daytime (9 LT ~ 17 LT). However, the averaged RMSE values of our model are much smaller than those of the IRI model, and the averaged RMSE values of the IRI model are comparably quite large during the night to dawn hours (19 LT ~ 7 LT). The large averaged RMSE value during the nighttime at the high latitude is probably related to local electron density variations. Both particle precipitation and transport process events generally dominate ionospheric variability at high latitudes (Themens and Jayachandran, 2016). In particular, in the auroral oval region where the EISCAT radar is located, there is usually an influx of aurora particles during the night. As can be seen in Fig. 6 (1 LT), the electron density of the E layer increases as much as the F layer due to this inflow of aurora particles. The IRI model does not properly reflect these ionosphere characteristics in high latitudes. However, the IGS-3D *N*<sub>e</sub> model generates an electron density
|  |   |   |   |
| --- | --- | --- | --- |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  IRI (10^{12} m^{-3}) | 0.37 | 0.23 | 0.44  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.23 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  |   |   |   |
|  IGS-3D *N* (10^{12} m^{-3}) | 0.37 | 0.22 | 0.34  |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |
|  |   |   |   |

![img-7.jpeg](img-7.jpeg)

**Fig. 9.** Examples of the electron density profiles of IGS-3D *N*e model, IRI model, and ISR at three ISR stations during geomagnetic disturbances.

#### **Table 3**

The averaged RMSE values between IGS and ISR electron density profiles and between IRI and ISR ones at Jicamarca, Millstone Hill, and EISCAT stations during geomagnetic disturbance periods among all test datasets.

|   | Jicamarca | Millstone Hill | EISCAT Tromsø  |
| --- | --- | --- | --- |
|  IGS-3D *N*e (10^{11}m^{-3}) | 0.3 | 0.28 | 0.32  |
|  IRI (10^{11}m^{-3}) | 0.33 | 0.31 | 0.51  |

Means that the IGS-3D *N*e model generates electron density profiles closer to the ISR ones than the IRI model, especially during geomagnetic disturbance periods.

From the above results, as a whole, it can be seen that the electron density profiles generated from the IRI model are different from those of the ISR, while those from the IGS-3D *N*e model are in good agreement with those from the ISR. Although the trained model uses the pair of the IRI TEC and IRI electron density profiles with a difference from the actual value, it is only trained to generate the variation pattern of electron density profile according to a given TEC value under conditions such as DOY, UT, LT, latitude, and longitude. Also, since the IRI model is not designed to account for individual dates with specific solar and geomagnetic conditions, the training of model with the inverted method to generate electron density profile using only TEC during individual days is not perfect. However, we think that the IGS TEC is so close to the actual value that it well reflected the solar and geomagnetic conditions, and as the result, it is confirmed that the electron density profile generated from the IGS TEC is closer to the observed value than that of the IRI model.

Our proposed method is to generate the electron density profiles only with the TEC without other parameters such as Kp index, F10.7, NmF2. The IGS-3D *N*e model is trained on the relationship between the TEC and the corresponding variation in the electron density profile at a given location and time, and as a result, it is confirmed that the electron density profile is well generated with only TEC. In addition, we confirm that the trained model generates the improved electron density profile from IGS TEC, which is close to the actual value by comparing it with ISR data. In other words, the trained deep learning model can scale the electron density profile from the TEC.

The IRI TEC is calculated up to an altitude of 700 km, while the IGS TEC represents the electron density integral value at the altitude of GNSS. This means that the IGS TEC includes the contribution of the plasmasphere, whereas the IRI TEC does not. In this study, we train the model using IRI TEC and then input IGS TEC into the trained model to generate IGS electron density profiles. Since the IGS TEC includes the plasmaspheric contribution, the model may have potential errors. The plasmasphere is known to contribute up to 50% to the GPS TEC at night and up to 10% during the daytime [Yizengaw et al., 2008; Jee et al., 2010; Lee et al., 2013]. Therefore, we assume that there is little plasmaspheric contribution to the IGS TEC during the daytime (12–14 LT) and a high plasmaspheric contribution at night (0–4 LT). We evaluate the potential error of the model by calculating the RMSE between the ISR electron density profile data and the IGS electron density profile data, separated according to the plasmaspheric contribution. As a result, the potential error of the model is about 22%. This indicates that when the plasmaspheric contribution to the IGS TEC is significant, our model must account for a potential error of about 22%.

#### **4. Summary and conclusion**

In this study, we have proposed the method to generate the electron density profile from TEC by applying an inversion method. For this, we have made the global IGS-3D *N*e model for generating global electron density from the IGS TEC data using a deep learning method based on MLP. We have evaluated our model by comparing the electron density profiles obtained from ISRs and IRI model. The results from this study can be summarized as follows.
1) Our method is to generate a 1-D electron density profile from a TEC value. To do this, we train a model using pairs of the IRI TEC and IRI electron density profile.
2) The IGS-3D $N_{e}$ model is constructed to generate a global IGS electron density profile from the global IGS TEC that is close to the actual TEC value based on the trained model.
3) The electron density profiles generated from the IGS-3D $N_{e}$ model are more similar to those from the ISR than those of the IRI model during the day ( $11 \mathrm{LT} \sim 17 \mathrm{LT}$ ) at the Jicamarca station. At the Millstone Hill station, both the electron density profiles from the IGS-3D $N_{e}$ model and the IRI model show similar differences to those from the ISR one, with the exception of the December solstice. The electron density profiles generated from the IGS-3D $N_{e}$ model show very small differences from the ISR electron density profiles than those of the IRI model during the night to dawn hours ( $19 \mathrm{LT} \sim 7 \mathrm{LT}$ ) at the EISCAT station.
4) The IGS-3D $N_{e}$ model shows that the averaged RMSE values between IGS and ISR electron density profiles are $0.37 \log \left(\mathrm{~m}^{-3}\right), 0.22 \log$ $\left(\mathrm{m}^{-3}\right)$, and $0.34 \log \left(\mathrm{~m}^{-3}\right)$ for all test datasets at Jicamarca, Millstone Hill, and EISCAT stations, respectively, which is better than those of the IRI model.
5) The evaluation of the IGS-3D $N_{e}$ model during geomagnetic disturbance periods shows that the electron density profiles of the IGS-3D $N_{e}$ model are much more similar to the ISR electron density profiles than those of the IRI model, especially at the EISCAT station. The IGS-3D $N_{e}$ model shows that the averaged RMSE are $0.3 \log \left(\mathrm{~m}^{-3}\right)$, $0.28 \log \left(\mathrm{~m}^{-3}\right)$, and $0.32 \log \left(\mathrm{~m}^{-3}\right)$ during geomagnetic disturbance periods among all test datasets at the Jicamarca, Millstone Hill, and EISCAT stations, respectively.

These results suggest that our IGS-3D $N_{e}$ model can improve the prediction ability of the global electron density profile. The method applied to our model shows the possibility of predicting the global 3-D electron density distribution in near real-time.

## CRediT authorship contribution statement

Eun-Young Ji: Writing - review \& editing, Writing - original draft, Visualization, Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Formal analysis, Data curation, Conceptualization. Yong-Jae Moon: Writing - original draft, Supervision, Project administration, Funding acquisition. Young-Sil Kwak: Writing - review \& editing, Project administration, Funding acquisition, Conceptualization. Kangwoo Yi: Writing - review \& editing, Software. Jeong-Heon Kim: Writing - review \& editing.

## Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Acknowledgments

This work was supported by The Korea Astronomy and Space science Institute under the R\&D program(Project No. 2024-1-850-02) supervised by the Ministry of Science and ICT, Institute for Information and Communications Technology Promotion(IITP) grant funded by the Korea government(MSIP) (RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15\%), BK21 FOUR program through National Research Foundation of Korea (NRF) under Ministry of Education (MoE) (Kyung Hee University, Human Education Team for the Next Generation of Space Exploration). The IRI-2016 model source code in Fortran was obtained from the IRI homepage (http://www.irimodel.org/). The IGS TEC maps were obtained from NASA Crustal Dynamics Data Information System (ftp://cddis.nasa.
gov/gnss/products/ionex/). The Jicamarca and Millstone Hill ISR data were obtained from the Madrigal Database at Jicamarca Radio Observatory (http://jro1.igp.gob.pe/madrigal/) and Millstone Hill (http://millstonehill.haystack.mit.edu/). The EISCAT ISR data were obtained from the EISCAT database in NIPR (http://pc115.seg20.nipr.ac .jp/www/eiscatdata/index.html).

## Data availability

Data will be made available on request.

## References

Bilitza, D., 2001. International reference ionosphere 2000. Adv. Space Res. 36, 261-275.
Bilitza, D., Reinisch, B.W., 2008. International reference ionosphere 2007: improvements and new parameters. Radio Sci. 42, 599-609.
Bilitza, D., Altadill, D., Truhlik, V., Shubin, V., Galkin, I., Reinisch, B.W., Huang, X., 2017. International reference ionosphere 2016: from ionospheric climate to realtime weather predictions. Space Weather 15, 418-429.
Bilitza, D., Pezzopane, M., Truhlik, V., Shubin, V., Altadill, D., Reinisch, B.W., Pignatheri, A., 2022. The international reference ionosphere model: a review and description of an ionospheric benchmark. Rev. Geophys. 60, e2022RG000792.
Hajj, G.A., Romans, L.J., 1998. Ionospheric electron density profiles obtained with the global positioning system: results from the GPS/MET experiment. Radio Sci. 33, $175-190$.
Hernández-Pajares, M., Juan, J.M., Sanz, J., Oroz, R., García-Rigo, A., Feltens, J., Konijsthy, A., Schaer, S.C., Kraskowski, A., 2009. The IGS VTEC maps: a reliable source of ionospheric information since 1998. J. Geodesy 83, 263-275.
Jang, X., Kwon, R.-Y., Linker, J.A., Riley, P., Shin, G., Dewea, C., Kim, Y.-H., 2021. Development of a deep learning model for inversion of rotational coronagraphic images into 3D electron density. Astrophys. J. Lett. 920 (2), 6.
Jee, G., Schunk, R.W., Scherllem, L., 2005. Comparison of IRI-2001 with TOPEX TEC measurements. J. Atmos. Sol. Terr. Phys. 67, 365-380.
Jee, G., Lee, H.-B., Kim, Y.H., Chung, J.-K., Cho, J., 2010. Assessment of GPS global ionosphere maps (GIM) by comparison between CODE GIM and TOPEX/Jason TEC data: ionospheric perspective. J. Geophys. Res. 115, A10319.
Ji, E.-Y., Moon, Y.-J., Park, E., 2020. Improvement of IRI global TEC maps by deep learning based on conditional generative adversarial networks. Space Weather 18, e2019SW002411.
Kumar, S., Tan, E.L., Murti, D.S., 2015. Impacts of solar activity on performance of the IRI-2012 model predictions from low to mid latitudes. Earth Planets Space 67, 42. https://doi.org/10.1186/s40623-015-0205-3.
Kingma, D.P., Ba, J.L., 2014. Adam: A Method for Stochastic Optimization. arXiv: 1412.6980.
Lee, H.-B., Jee, G., Kim, Y.H., Shim, J.S., 2013. Characteristics of global plasmaspheric TEC in comparison with the ionosphere simultaneously observed by Jason-1 satellite. J. Geophys. Res.: Space Phys. 118, 935-946.
Lee, K.S., Chae, J., Park, E., Moon, Y.-J., Kwak, H., Cho, K., 2022. Deep learning-based fast spectral inversion of He and Ca II 8542 line spectra. Astrophys. J. 940 (2), 16.
Lee, S., Ji, E.-Y., Moon, Y.-J., Park, E., 2021. One-day forecasting of global TEC using a novel deep learning model. Space Weather 19, e2020SW002600.
Li, Q., Liu, L., He, M., Huang, H., Zhang, J., Yang, N., Zhang, M.-L., Jiang, J., Chen, Y., Le, H., Cui, J., 2021a. A global empirical model of electron density profile in the F region ionosphere basing on COSMIC measurements. Space Weather 19, e2020SW002642.
Li, W., Zhao, D., He, C., Shen, Y., Hu, A., Zhang, K., 2021b. Application of a multi-layer artificial neural network in a 3-D global electron density model using the long-term observations of COSMIC, Fengyun-3C, and Digizonde. Space Weather 19, e2020SW002605.
Liu, L., Zou, S., Yao, Y., Wang, Z., 2020. Forecasting global ionospheric TEC using deep learning approach. Space Weather 18, e2020SW002501.
Nair, V., Hinton, G.E., 2020. Rectified linear units improve restricted Boltzmann machines. ICML'10: Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 807-814.
Ogwala, A., Somoye, E.O., Panda, S.K., Ogunmodimu, O., Onori, E., Sharma, S.K., Okoh, D., Oyedokun, O., 2021. Total electron content at equatorial and low-, middleand high-latitudes in African longitude sector and its comparison with IRI-2016 and IRI-PLAS 2017 models. Adv. Space Res. 68, 2160-2176.
Sai Gowtam, V., Tulasi Ram, S., 2017. An artificial neural network based ionospheric model to predict NmF2 and hmF2 using long-term data set of FORMOSAT-3/ COSMIC radio-occultation observations: preliminary results. J. Geophys. Res.: Space Phys. 122 (11), 755, 743-11.
Sai Gowtam, V., Tulasi Ram, S., Reinisch, B., Prajapati, A., 2019. An new artificial neural network-based global three-dimensional ionospheric model (ANNIM-3D) using longterm ionospheric observations: preliminary results. J. Geophys. Res.: Space Phys. 124, 4639-4657.
Themens, D.R., Jayachandran, P.T., Nicolls, M.J., MacDougall, J.W., 2014. A top to bottom evaluation of IRI 2007 within the polar cap. J. Geophys. Res.: Space Phys. 119, 6689-6703.
Themens, D.R., Jayachandran, P.T., 2016. Solar activity variability in the IRI at high latitudes: comparisons with GPS total electron content. J. Geophys. Res.: Space Phys. 121, 3793-3807.
Yizengaw, E., Moldwin, M.B., Galvan, D., Iijima, B.A., Komjathy, A., Mannucci, A.J., 2008. Global plasmaspheric TEC and its relative contribution to GPS TEC. J. Atmos. Sol. Terr. Phys. 70, 1541-1548.

Yue, X., Schreiner, W.S., Kuo, Y.-H., Hunt, D.C., Wang, W., Solomon, S.C., Burns, A.G., Bilitza, D., Liu, J.-Y., Wan, W., Wickert, J., 2012. Global 3-D ionospheric electron density reanalysis based on multisource data assimilation. J. Geophys. Res. 117, A09325."
Jihyeon Son et al 2021 - Generation of He i 1083 nm Images from SDO AIA Images by Deep Learning.pdf,"# Generation of He I 1083 nm Images from SDO AIA Images by Deep Learning 

Jihyeon Son ${ }^{1}$ (D), Junghun Cha ${ }^{2}$, Yong-Jae Moon ${ }^{1,2}$ (D), Harim Lee ${ }^{2}$ (D), Eunsu Park ${ }^{2}$ (D), Gyungin Shin ${ }^{3}$, and Hyun-Jin Jeong ${ }^{1}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic Of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic Of Korea<br>${ }^{3}$ Department of Engineering Science, University of Oxford, Oxford, United Kingdom<br>Received 2021 April 7; revised 2021 June 23; accepted 2021 June 28; published 2021 October 20


#### Abstract

In this study, we generate He I 1083 nm images from Solar Dynamic Observatory (SDO)/Atmospheric Imaging Assembly (AIA) images using a novel deep learning method (pix2pixHD) based on conditional Generative Adversarial Networks (cGAN). He I 1083 nm images from National Solar Observatory (NSO)/Synoptic Optical Long-term Investigations of the Sun (SOLIS) are used as target data. We make three models: single-input SDO/ AIA 19.3 nm image for Model I, single-input 30.4 nm image for Model II, and double-input (19.3 and 30.4 nm ) images for Model III. We use data from 2010 October to 2015 July except for June and December for training and the remaining one for test. Major results of our study are as follows. First, the models successfully generate He I 1083 nm images with high correlations. Second, Model III shows better results than those with one input image in terms of metrics such as correlation coefficient (CC) and root mean square error (RMSE). CC and RMSE between real and synthetic ones for model III with 4 by 4 binnings are 0.88 and 9.49 , respectively. Third, synthetic images show well observational features such as active regions, filaments, and coronal holes. This work is meaningful in that our model can produce He I 1083 nm images with higher cadence without data gaps, which would be useful for studying the time evolution of the chromosphere and transition region.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar chromosphere (1479); Convolutional neural networks (1938); Solar filaments (1495); Solar coronal holes (1484)
Supporting material: animation

## 1. Introduction

The He I 1083 nm line is a spectral line formed at the upper chromosphere and lower transition region in the Sun. He I 1083 nm spectroheliograms show both chromospheric and coronal structures such as coronal holes, filaments, and filament channels. Especially, they have been mainly used to observe coronal holes at ground. Coronal holes indicate regions where the solar corona is colder and has lower plasma densities compared to surroundings. They are well known as the source of high-speed solar wind owing to their open magnetic fields. Therefore, they have been regarded as an important component for forecasting space weather, and they have been observed in several wavelengths such as extreme-ultraviolet (EUV), X-ray, and infrared lines. Unlike EUV and X-ray observations, they appear bright in He I 1083 nm images. This is because coronal radiation overpopulates the He I 1083 multiplet transition levels, resulting in the increase of the absorption at places where the coronal radiation is high (Harvey \& Recely 2002). In other words, the absorption of the He I 1083 nm line decreases in coronal holes where coronal radiation is low. Such characteristics in He I 1083 nm make it possible to indicate a more precise shape of coronal holes without disturbance of bright structures at the line of sight than in X-rays. Because of this advantage, He I 1083 nm images have been used for the study of coronal holes by several authors (Fox et al. 1998; Harvey \& Recely 2002; Malanushenko \& Jones 2005; Wang 2017; Wallace et al. 2019). Henney \& Harvey (2005) developed a method for automated coronal hole detection with He I 1083 nm images and magnetograms. De Toma et al. (2005) observed transient coronal holes that form during the onset of coronal mass ejections (CMEs) using He I 1083 nm data. In addition, they are used for not only coronal holes but
also active regions and filaments (Harvey \& Recely 1984; MacQueen et al. 2000; Yang et al. 2008; Kuckein et al. 2012; Wang et al. 2016; Baranovskii et al. 2017).

Deep learning is a branch of machine learning in which a computer forms an artificial neural network similar to a human's brain to solve complex nonlinear relationships. Deep learning has widely been used to handle multiple problems in various fields. In particular, image-to-image translation methods based on Generative Adversarial Networks (GANs; Goodfellow et al. 2014) have been successfully used. One of the most well-known methods among them is ""pix2pix"" (Isola et al. 2017) based on conditional GAN (cGAN; Mirza \& Osindero 2014). There have been several researches in the astronomy field using it, and they show competent results (Kim et al. 2019; Park et al. 2019, 2020; Ji et al. 2020; Lee et al. 2021, 2021). However, the pix2pix has a drawback of poor results with artifacts and lack of details when performing highresolution image generation. To solve this problem, Wang et al. (2018) proposed the ""pix2pixHD"" model with a novel adversarial loss and multiscale network architectures. Shin et al. (2020) successfully generated solar magnetograms with $1024 \times 1024$ pixel resolution from Ca II K images using the pix2pixHD. Jeong et al. (2020) developed a model based on the pix2pixHD to generate solar farside magnetograms from EUV images and then extrapolated the global magnetic fields using frontside and synthetic farside data. Their results show that the pix2pixHD method is appropriate for translating between highresolution solar images having different wavelengths.

In this paper, we present three models to generate synthetic He I 1083 nm images from Solar Dynamic Observatory (SDO; Pesnell et al. 2011)/Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) images using pix2pixHD. For this work, we use 19.3 and 30.4 nm images as input data and He I 1083 nm
images from Synoptic Optical Long-term Investigations of the Sun (SOLIS; Keller et al. 2003) as target data. This study has several advantages. First, we can generate He I 1083 nm images with high spatial resolution and uniform quality regardless of seeing conditions. Second, we can produce synthetic He I 1083 nm images with a high time cadence of 12 s . Third, we can fill up observational gaps caused by instrumental problems and/or nighttime. To the best of our knowledge, our study is the first trial to generate ground-based data by deep learning.

This paper is organized as follows. We introduce detailed explanations of our data and models in Sections 2 and 3, respectively. We evaluate our models with metrics and discuss the results in Section 4. Finally, we summarize and conclude our study in Section 5.

## 2. Data

AIA is one of the instruments of SDO that provides simultaneous high-resolution solar full-disk images of the corona and transition region in multiple wavelengths with 12 s temporal resolution (Lemen et al. 2012). Among the provided passband data, we choose 19.3 and 30.4 nm as input data of our model. The 19.3 nm data represent solar corona, and 30.4 nm data represent solar chromosphere, so that we can expect that the model can generate both coronal and chromospheric structures seen in He I 1083 nm observations. The SDO/AIA data are available from the Joint Science Operation Center (JSOC) database (http://jsoc.stanford.edu/ ajax/lookdata.html).

SOLIS has three instruments, including Vector Spectromagnetograph, which is a 50 cm effective aperture reflective telescope (Keller et al. 2003). VSM provides photospheric and chromospheric full-disk vector-magnetograms and He I 1083 nm line observations. We use daily full-disk images of equivalent width in He I 1083 nm for target data of our model. The observation of He I 1083 nm by this instrument ended in 2015 July.

For SDO/AIA data, we use better-quality images with the quality index of zero in the header information. We make level 1.5 images using the aiaprep library from the Sunpy (The SunPy Community et al. 2020) software package, which processes rotating, centering, and aligning axis. To make images have the same exposure condition, we divide the data number by exposure time. We use the data that are scaled from 0 to $2^{12} \mathrm{DN} \mathrm{s}^{-1}$. We choose this range by referring to count rates in Boerner et al. (2012). Then, we divide them by median values on the solar disk to calibrate the data (UgarteUrra et al. 2015; Jeong et al. 2020) and take the logarithm of them. In the case of He I 1083 nm data, we use geometrycorrected level 2 data. The data are available from the SOLIS homepage (https://solis.nso.edu/0/vsm/VSMDataSumm. php). We adopt the data that are scaled from -150 to 100, which is a typical approach (https://solis.nso.edu). Note that limb darkening was not compensated very well on the He I 1083 nm data we used, thus hiding polar coronal holes. This is due to equivalent width processing for observations by SOLIS. A detailed data description is given at the SOLIS website. ${ }^{4}$ For He I 1083 nm images, we check all images by visual inspection and remove improper data having noisy and

[^0]missing information. Original pixel sizes of SDO/AIA and He I 1083 nm data are $4096 \times 4096$ and $2048 \times 2048$, respectively. For training, they are resized to $1024 \times 1024$ pixels.

The total period of data used in this study is from 2010 October to 2015 July. After the preprocessing described above, we get a total of 1123 pairs of images. The data except June and December ( 915 pairs) are used for training, and the remaining ones (208 pairs) for test. This is because we want to take into account the solar cycle effect in training and test.

## 3. Method

We use the ""pix2pixHD"" model, which is well known as an effective deep learning method for high-resolution image-toimage translation (Wang et al. 2018). The pix2pixHD consists of two main networks named ""generator"" and ""discriminator."" The training process of the pix2pixHD is as follows. At first, the generator produces target-like images from input images. The discriminator gets two pairs, a synthetic pair with synthetic and input images, and a real pair with target and input images. Then, the discriminator trains for distinguishing between the real pair and synthetic pair. The generator refers to this result and tries to generate more real-like images to deceive the discriminator.

In the generator, several convolution layers and transposed convolution layers are stacked. A convolution layer has major parameters named kernels (or filters) that extract features of input images. A transposed layer is an inverse of the convolution layer and generates an output image from extracted features. The discriminator consists of a number of convolution layers to classify real pairs and synthetic pairs. The kernels of convolution layers and transposed convolution layers are updated as training progresses. Our model has one generator and two discriminators (discriminator A and B) as shown in Figure 1. The two discriminators have the same network structure but work at different image scales. Discriminator A trains with original size images, and discriminator B trains with those downsampled by half. The multiscale discriminators help the generator to produce globally consistent images and finer details.

The pix2pixHD has two loss functions, Least Squares GAN (LSGAN; Mao et al. 2017) loss ( $\mathcal{L}_{\text {LSGAN }}$ ) and a feature matching loss ( $\mathcal{L}_{\mathrm{FM}}$ ), to optimize the model. The LSGAN losses to update the generator and discriminator are given by

$$
\begin{aligned}
& \mathcal{L}_{\mathrm{LSGAN}}^{G}(G, D)=(D(x, G(x))-1)^{2} \\
& \mathcal{L}_{\mathrm{LSGAN}}^{D}(G, D)=\frac{1}{2}(D(x, y)-1)^{2}+\frac{1}{2}(D(x, G(x)))^{2}
\end{aligned}
$$

where $x, y$, and $G(x)$ are an input, a real output, and an output from the generator, respectively. The objective of the $\mathcal{L}_{\mathrm{FM}}$ is to match the distribution of the generated pair and real pair from multiple layers of the discriminator. The $\mathcal{L}_{\mathrm{FM}}$ is obtained by

$$
\mathcal{L}_{\mathrm{FM}}(G, D)=\sum_{i=1}^{T} \frac{1}{N_{i}}\left[\left\|D^{(i)}(x, y)-D^{(i)}(x, G(x))\right\|_{L}\right]
$$

where $T, i$, and $N_{i}$ denote the total number of layers, the serial number of the layers, and the number of elements in each layer, respectively. We use a relative weight $(\lambda)$ that controls the importance of $\mathcal{L}_{\mathrm{LSGAN}}^{G}$ and $\mathcal{L}_{\mathrm{FM}}$ (Wang et al. 2018). For our


[^0]:    4 https://nispdata.nso.edu/webProdDesc2/presenter.php?file=solis_vsm_ 10830_sky_images.html\&echoExact=0\&name=SOLIS\%20VSM\%2010830\% 20full-disk\%20images
![img-0.jpeg](img-0.jpeg)

Figure 1. Architecture of the model with two input images (model III).

![img-1.jpeg](img-1.jpeg)

Figure 2. Two input images, one target image and three synthetic from our three models at 17:45 UT on 2013 December 29. (a) SDO/AIA 19.3 nm image as input. (b) SDO/AIA 30.4 nm image as input. (c) He I 1083 nm image as target. (d) synthetic He I 1083 nm image from Model I. (e) Synthetic He I 1083 nm image from Model II. (f) Synthetic He I 1083 nm image from Model III.
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison between the target He I 1083 nm image and synthetic one from Model III at 18:17 UT on 2013 December 2. (a) Target He I 1083 nm image; (b) synthetic He I 1083 nm image; (c) the difference map between panels (a) and (b).

model, we use 10 for λ as Wang et al. (2018) did. When the model is trained, the generator and discriminators try to minimize the losses given by

$$
\begin{aligned}
& \min\_{G} \left( \sum\_{k=1,2} \mathcal{L}\_{\text{LSGAN}}^{G}(G, D_k) + \lambda \sum\_{k=1,2} \mathcal{L}\_{\text{FM}}(G, D_k) \right) \\
& \min\_{D_k, D_2} \sum\_{k=1,2} \mathcal{L}\_{\text{LSGAN}}^{D}(G, D_k).
\end{aligned}
$$

To optimize the loss functions, we use the adaptive moment estimation (Adam; Kingma & Ba 2014) optimizer with learning late 0.0002.

In this study we make three models: a model with 19.3 nm images as input data (model I), a model with 30.4 nm images as input data (model II), and a model with both 19.3 and 30.4 nm images as input data (model III). These three models have the same architecture as shown in Figure 1. We train these models for 180,000 iterations (about 200 epochs). For evaluation, we save each model and synthetic image at every 5000th iterations. We evaluate all the saved models by metrics, and the results shown in the next section are the highest scores among them.

## 4. Results and Discussion

Figure 2 shows two input images, a target image and three synthetic images from our models (Model I, II, and III) at 17:45 UT on 2013 December 29. The overall distributions of the target image are well reproduced in the synthetic images, but there are a few differences in details between them. For Model I, it generates coronal holes clearly but cannot catch detailed structures such as filaments. On the other hand, for Model II, coronal holes in the generated images seem to be less precise than those from Model I. However, this model can capture the features that Model I misses. Because both 30.4 nm and He I 1083 nm are emitted from similar regions (chromosphere and transition region), it would be easier for Model II to find the relation between the two lines. Also, 30.4 nm images could have a small contribution from nearby coronal emission. Complementing each other's shortcomings, Model III with two input images generates He I 1083 nm images most plausibly.

Figure 3 shows a comparison between the target He I 1083 nm image and synthetic one from Model III at 18:17 UT on 2013 December 2. We make the difference map after masking outside 0.95 R⊙. We find that most values of the difference map are near zero as shown in Figure 3(c). Especially, there is little difference in coronal hole regions. On the other hand, there is a slight difference in the filament. This may be because the absorption process in filaments differs depending on wavelength. Dark features of extreme-ultraviolet (input wavelengths) can be explained by hydrogen Lyman continuum absorption (Schmahl & Orrall 1979). It is known that a high recombination rate from He<sup>+</sup> produces the absorption features at He I 1083 nm (Andretta & Jones 1997). The above absorption mechanisms make filaments appear differently in the corresponding images. To reduce the difference, more data sets for training and test may be needed. Nevertheless, the shape, location, and size of the filament in the two images are almost the same. Overall, it can be said that the model successfully generates the target-like images.

To evaluate our models quantitatively, we calculate the pixel-to-pixel Pearson correlation coefficient (CC) and root mean square error (RMSE) of each model for the test set. The closer CC is to 1, the higher linear relationship between two data. RMSE shows the differences between pixel values of real images and synthetic ones. CC and RMSE are given by

$$
\begin{gathered}
\text{CC} = \frac{\sum\_{i=1}^{N} (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum\_{i=1}^{N} (X_i - \bar{X})^2} \sqrt{\sum\_{i=1}^{N} (Y_i - \bar{Y})^2}} \\
\text{RMSE} = \sqrt{\frac{\sum\_{i=1}^{N} (X_i - Y_i)^2}{N}}
\end{gathered}
$$

where X, Y, *X*, *Y*, and *N* denote pixel values of generated images and real images, their average values, and total number of pixels, respectively. We obtain these metrics after performing 4 × 4 binning of images yielding 256 × 256 pixel size images, and we consider pixel values only inside 0.95 R⊙ because of limb uncertainty. Table 1 shows the average of metrics of all our test data. As seen in Table 1, Model III shows the best performance in terms of both CC and RMSE values. This result coincides with the discussion of Figure 2. All results shown hereafter are generated by Model III.

To examine how well each of the specific structures is reproduced, we present pixel-by-pixel scatter plots between real and synthetic He I 1083 nm images for 182 filaments, 511 active
![img-3.jpeg](img-3.jpeg)

Figure 4. Pixel-by-pixel scatter plots between real and synthetic He I 1083 nm images for 182 filaments, 511 active regions, and 74 coronal holes (left to right).

Table 1
The Average Pixel-to-pixel Correlation Coefficient (CC) and Root Mean Square Error (RMSE) after $4 \times 4$ Binning between Real He I 1083 Images and Synthetic Ones from Our Models

|  | Model I <br> (input: <br> 19.3 nm ) | Model II <br> (input: <br> 30.4 nm ) | Model III <br> (input: 19.3 and <br> 30.4 nm ) |
| :-- | :--: | :--: | :--: |
| Mean pixel-to-pixel CC <br> $(4 \times 4$ binning) | 0.83 | 0.86 | $\mathbf{0 . 8 8}$ |
| Mean RMSE $(4 \times 4$ <br> binning) | 11.28 | 10.15 | $\mathbf{9 . 4 9}$ |

regions, and 74 coronal holes in Figure 4. The scatter plots are made after $4 \times 4$ binning. These events are all significantly large in view of space weather forecasting. For this, we crop each feature using coordinate information from Heliophysics Events Knowledgebase (HEK) provided by Lockheed Martin Solar and Astrophysics Laboratory (LMSAL). For evaluation we obtain CC and RMSE for each structure as we did for the full-disk data, and they are shown in each scatter plot. Three features have good CC and RMSE values, which are similar to those of the full-disk data. Especially, active regions and coronal holes show better results than filaments. This is in the same line with the discussion of the above difference map (Figure 3).

To demonstrate that our model can generate coronal holes well, we try to detect coronal holes from the target and synthetic images and then compare them. For this, we refer to the recipe of automatic coronal hole detection in Henney \& Harvey (2005). A key point of their process is using pixel values to find the boundaries of coronal holes. This takes advantage that coronal holes seem brighter than the surroundings in He I 1083 nm images. A more detailed procedure is described in Henney \& Harvey (2005). We apply the same method to the target image and synthetic image at 17:45 UT on 2013 December 29, and the results are shown in Figure 5. As shown in Figures 5(a) and (c), the locations of coronal holes in the synthetic image are almost identical to those of the real one. Also, there are no noticeable differences in the overall forms as seen in Figures 5(b) and (d). These results show that our model can well reproduce coronal holes. It is more reliable to combine various wavelengths than when using only one passband for determining boundaries of coronal holes because their forms
depend on observation wavelength (Toma \& Arge 2005). We expect that using synthetic images from our model with the observation data in different wavelengths is helpful for studying coronal holes in this respect. In addition, to check the coronal hole identification near the limb, we divide the coronal holes used in Figure 4 into two groups: (A) within $60^{\circ}$, and (B) the others. The A group has 48 coronal holes, and the B group has 26 . We calculate CC and RMSE of each group. The A group shows 0.83 and 10.1 for CC and RMSE, respectively, and the B group shows 0.85 and 11.0 , respectively. This result shows that our model generates coronal holes without significant differences depending on location.

Figure 6(c) shows an abnormal He I 1083 nm observation because of the malfunction of the instrument on 2012 August 23. In the image, active regions and coronal holes cannot be identified. Using SDO/AIA data at the same time, we can generate the He I 1083 nm image using Model III in Figure 6(d). Comparing SDO/AIA images (Figures 6(a) and (b)), the coronal holes and active regions that are invisible in Figure 6(c) are well reproduced in Figure 6(d). In addition, the seeing condition is not constant because He I 1083 nm data are observed at the ground. Therefore, the quality of the observations is nonuniform. However, our model can produce He I 1083 nm images with homogeneous data quality regardless of weather conditions. Also, our model can produce He I 1083 nm images at nighttime only if SDO/AIA data are available. These facts indicate that our model can be a method to overcome the limitations of ground-based observations.

We can fill in the data gap of He I 1083 nm observations by our model. For this we consider 2011 June, during when there are only 13 days of observations. Figure 7 shows daily He I 1083 nm images in 2011 June combining real observations and synthetic images. We generate daily He I 1083 nm data at 18:00 UT to complement the absences of observations. As a result, long-term synoptic evolution seen in the He I 1083 nm line can be shown without data gap. By synthetic images, we can identify the evolution of structures such as filaments, coronal holes, and active regions that are missed in real observations. Especially, we mark the evolution of a filament and coronal holes by yellow arrows and red arrows, respectively. Combining synthetic images and real observations makes it possible to continuously
![img-4.jpeg](img-4.jpeg)

Figure 5. Coronal hole detection using the He I 1083 nm image at 16:41 UT on 2013 June 26. Panels (a) and (c) show real and synthetic He I 1083 nm images, respectively. Panels (b) and (d) are zoomed-in images of the coronal hole region in panels (a) and (c), respectively.
monitor temporal evolution of structures in the chromosphere and transition region.

Filaments are one of the important chromospheric features because their eruptions are known to be associated with CMEs. However, the entire filament eruption process cannot be monitored in He I 1083 nm observations owing to their low cadence and/or seeing conditions and/or instrumental problems. Because filaments clearly appear in He I 1083 nm data, it is very useful to study the evolution of filaments if we can observe them continuously. Figure 8 shows synthetic He I 1083 nm images during filament
eruption from 21:10 UT on 2016 February 18 to 00:20 UT on February 19. The model generates the images every 10 minutes. The process of filament eruption is well shown in Figure 8. This result suggests a sufficient possibility to study the eruption of filaments with a high time cadence.

## 5. Conclusion

In this study, we have developed three deep learning models to generate synthetic He I 1083 nm images from SDO/AIA
![img-5.jpeg](img-5.jpeg)

Figure 6. Restoration of damaged He I 1083 nm data at 22:08 UT on 2012 August 23 by our model. (a) SDO/AIA 19.3 nm image as input; (b) SDO/AIA 30.4 nm image as input; (c) real He I 1083 nm image; (d) synthetic image from Model III.

Images. To treat high-resolution (1024 × 1024 pixels) images, we apply the pix2pixHD model to these data. We train the following models: single-input SDO/AIA 19.3 nm image for Model I, single-input 30.4 nm image for Model II, and double-input (19.3 and 30.4 nm) images for Model III. Target data of all the models are He I 1083 nm images from NSO/SOLIS. The summaries of this study are as follows. First, the models successfully generate He I 1083 nm images from SDO/AIA images. Especially, Model III shows the best performance in terms of metrics and visual aspects. Second, pixel-to-pixel CC and RMSE values after 4 × 4 binning of Model III are 0.88 and 9.49, respectively. Third, the boundaries of coronal holes from the synthetic images are well consistent with those from real ones. Fourth, the synthetic He I 1083 nm images are generated with uniform quality and high time resolution regardless of instrumental problems and atmospheric conditions. This implies that our model should be a reliable tool for studying the time evolution of chromospheric phenomena. Our study may be extended to modernize He I 1083 nm data whose observations began 45 yr ago. Recently, Pineci et al. (2021)
![img-6.jpeg](img-6.jpeg)

Figure 7. Filling in the absence of He I 1083 nm observations by our model in 2011 June. The blue boxes are real He I 1083 nm observations, and the yellow boxes are synthetic images when the observations are absent. The yellow arrows follow the evolution of a filament, and the red arrows indicate the evolution of coronal holes.
generated 30.4 nm images from He I 1083 nm images by deep learning. If historical 30.4 nm images are generated by their model, our model together with their results may be used for modernizing
the He I 1083 nm images. We leave this to a future work. Our results show a possibility that the deep learning method could help overcome shortcomings of various ground-based observations.
![img-7.jpeg](img-7.jpeg)

Figure 8. Synthetic He I 1083 nm images during filament eruption from 21:10 UT on 2016 February 18 to 00:20 UT on February 19. They are generated every 10 minutes. An animation of this figure with 1-minute cadence is available. The animation starts at 2016 February 18 at 21:10 UT and ends the next day at 00:29 UT. (An animation of this figure is available.)

This work utilizes SOLIS data obtained by the NSO Integrated Synoptic Program (NISP), managed by the National Solar Observatory, which is operated by the Association of Universities for Research in Astronomy (AURA), Inc., under a cooperative agreement with the National Science Foundation. We thank the numerous team members who have contributed to the success of the SDO and AIA mission. We acknowledge the community efforts devoted to developing the open-source packages that were used in this work. This work was supported by the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2019R1A2C1002634, NRF-2019R1C1C1004778, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R&D program (project No. 2021-1-850-05) supervised by the Ministry of Science and ICT, and the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares).

## ORCID IDs

- Jihyeon Son https://orcid.org/0000-0003-2678-5718
- Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
- Harim Lee https://orcid.org/0000-0002-9300-8073
- Eunsu Park https://orcid.org/0000-0003-0969-286X
- Hyun-Jin Jeong https://orcid.org/0000-0003-4616-947X

## References

- Andretta, V., & Jones, H. P. 1997, ApJ, 489, 375
- Baranovskii, E., Stepanyan, N., Tarashchuk, V., & Shtertser, N. 2017, ARep, 61, 74
- Boerner, P., Edwards, C., Lemen, J., et al. 2012, SoPh, 275, 41
- De Toma, G., Holzer, T., Burkepile, J., & Gilbert, H. 2005, ApJ, 621, 1109
- Fox, P., McIntosh, P., & Wilson, P. 1998, SoPh, 177, 375
- Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. 2014, in Advances in Neural Information Processing Systems 27, ed. Z. Ghahramani et al. (Red Hook, NY: Curran Associates, Inc.), 2672
- Harvey, K. L., & Recely, F. 1984, SoPh, 91, 127
- Harvey, K. L., & Recely, F. 2002, SoPh, 211, 31
- Henney, C. J., & Harvey, J. W. 2005, in ASP Conf. Ser., 346, Large-scale Structures and their Role in Solar Activity, ed. K. Sankarasubramanian, M. Penn, & A. Pevtsov (San Francisco, CA: ASP), 261
- Isola, P., Zhu, J.-Y., Zhou, T., & Efros, A. A. 2017, in Proc. IEEE Conf. on Computer Vision and Pattern Recognition (Los Alamitos, CA: IEEE Computer Society), 5967
- Jeong, H.-J., Moon, Y.-J., Park, E., & Lee, H. 2020, ApJL, 903, L25
- Ji, E.-Y., Moon, Y.-J., & Park, E. 2020, SpWea, 18, e2019SW002411
- Keller, C. U., Harvey, J. W., & Giampapa, M. S. 2003, Proc. SPIE, 4853, 194
- Kim, T., Park, E., Lee, H., et al. 2019, Suicks, 3, 397
- Kingma, D. P., & Ba, J. 2014, arXiv:1412.6980
- Kuckein, C., Martínez Pillet, V., & Centeno, R. 2012, A&A, 539, A131
- Lee, H., Park, E., & Moon, Y.-J. 2021, ApJ, 907, 118
- Lee, S., Ji, E.-Y., Moon, Y.-J., & Park, E. 2021, SpWea, 19, e2600
- Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
- MacQueen, R., Hendrickson, M., Woods, J., et al. 2000, SoPh, 191, 85
- Malanushenko, O. V., & Jones, H. P. 2005, SoPh, 226, 3
Mao, X., Li, Q., Xie, H., et al. 2017, in Proc. IEEE Int. Conf. on Computer Vision (Los Alamitos, CA: IEEE Computer Society), 2813
Mirza, M., \& Osindero, S. 2014, CoRR, arXiv:1411.1784
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, E., Moon, Y.-J., Lim, D., \& Lee, H. 2020, ApJL, 891, L4
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. 2011, The Solar Dynamics Observatory (Berlin: Springer), 3
Pineci, A., Sadowski, P., Gaidos, E., \& Sun, X. 2021, ApJL, 910, L25
Schmahl, E. J., \& Orrall, F. Q. 1979, ApJL, 231, L41
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890,68

Toma, G. D., \& Arge, C. N. 2005, in ASP Conf. Ser., 346, Large-scale Structures and their Role in Solar Activity, ed. K. Sankarasubramanian, M. Penn, \& A. Pevtsov (San Francisco, CA: ASP), 251

Ugarte-Urra, I., Upton, L., Warren, H. P., \& Hathaway, D. H. 2015, ApJ, 815, 90
Wallace, S., Arge, C., Pattichis, M., Hock-Mysliwiec, R., \& Henney, C. 2019, SoPh, 294, 19
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., et al. 2018, in Proc. IEEE Conf. on Computer Vision and Pattern Recognition (Los Alamitos, CA: IEEE Computer Society), 8798
Wang, Y., Su, Y., Hong, Z., et al. 2016, ApJ, 833, 250
Wang, Y. M. 2017, ApJ, 841, 94
Yang, L.-H., Jiang, Y.-C., \& Ren, D.-B. 2008, ChJAA, 8, 329"
Junmu Youn et al 2025 - Can we properly determine differential emission measures from Solar OrbiterEUIFSI with deep learning.pdf,"# Can we properly determine differential emission measures from Solar Orbiter/EUI/FSI with deep learning? 

Junmu Youn ${ }^{1}$ (D), Harim Lee ${ }^{2}$ (D), Hyun-Jin Jeong ${ }^{3,1}$ (D), Jin-Yi Lee ${ }^{2}$, Eunsu Park ${ }^{4}$ (D), and Yong-Jae Moon ${ }^{1,2, *}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin 17104, Republic of Korea<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin 17104, Republic of Korea<br>${ }^{3}$ Centre for mathematical Plasma Astrophysics, Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium<br>${ }^{4}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon 34055, Republic of Korea

Received 19 September 2024 / Accepted 22 January 2025


#### Abstract

In this study, we address the question of whether we can properly determine differential emission measures (DEMs) using Solar Orbiter/Extreme Ultraviolet Imager (EUI)/Full Sun Imager (FSI) and AI-generated extreme UV (EUV) data. The FSI observes only two full-disk EUV channels ( 174 and $304 \AA$ ), which is insufficient for accurately determining DEMs and can lead to significant uncertainties. To solve this problem, we trained and tested deep learning models based on Pix2PixCC using the Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) dataset. The models successfully generated five-channel (94, 131, 193, 211, and $335 \AA$ ) EUV data from 171 and $304 \AA$ EUV observations with high correlation coefficients. Then we applied the trained models to the Solar Orbiter/EUI/FSI dataset and generated the five-channel data that the FSI cannot observe. We used the regularized inversion method to compare the DEMs from the SDO/AIA dataset with those from the Solar Orbiter/EUI/FSI dataset, which includes AI-generated data. We demonstrate that, when SDO and Solar Orbiter are at the inferior conjunction, the main peaks and widths of both DEMs are consistent with each other at the same coronal structures. Our study suggests that deep learning can make it possible to properly determine DEMs using Solar Orbiter/EUI/FSI and AI-generated EUV data.


Key words. Sun: atmosphere - Sun: corona - Sun: UV radiation

## 1. Introduction

Solar Orbiter (Müller et al. 2020) is a spacecraft orbiting the Sun that can capture images of the Sun in closer proximity than any previous spacecraft. Additionally, it enables the exploration of the Sun's uncharted polar regions for the first time. The Solar Orbiter/Extreme Ultraviolet Imagers (EUI; Rochus et al. 2020) include the Full Sun Imager (FSI), which observes the entire solar disk. Since the FSI only captures two channels (174 and $304 \AA$ ), this poses challenges for performing differential emission measure (DEM) analysis. Another instrument for observing extreme UV (EUV), the Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Atmospheric Imaging Assembly (AIA; Lemen et al. 2012), which has been in operation since May 2010, captures seven EUV (94, 131, 171, 193, 211, 304, and $335 \AA$ ) images from four telescopes. This instrument has two channels ( 171 and $304 \AA$ ) similar to those of the Solar Orbiter/EUI/FSI.

Differential emission measure is a technique used to quantify the density and temperature of emitting multithermal plasma in the solar atmosphere. The main peaks of the DEM profile represent the dominant plasma temperatures, and the width of the peak indicates whether the plasma is close to isothermal or multithermal at those temperatures (Schmelz et al. 2011). DEM analysis is a complex and ill-posed mathematical problem that

[^0]requires data from multiple channels to accurately determine the plasma characteristics. The DEM equation is defined as
$D N_{i}=\int_{0}^{\infty} K_{i}\left(T_{i j}\right) \operatorname{DEM}\left(T_{j}\right) d T$
where $D N_{i}(i=1, \ldots, N)$ is the observed digital number of the $i$-th channel. In this study, we used six channels to calculate the DEMs $(N=6) ; K_{i j}$ is the response function of a specific instrument, which indicates its sensitivity to plasma that is emitting at different temperature bins, $j$. More information about the response function can be found in Boerner et al. (2012). To effectively calculate the DEMs of the solar atmosphere, a combination of various EUV observations is essential. Therefore, numerous studies have utilized data from instruments like SDO/AIA for DEM analyses (e.g. Lee et al. 2017; Mandal et al. 2024). There are various methods for calculating DEMs (Hannah \& Kontar 2012; Cheung et al. 2015; Morgan \& Pickering 2019). Our analysis employs the method developed by Hannah \& Kontar (2012), which uses a regularization technique to address uncertainties inherent to DEM analysis and thus ensures more reliable results.

Recently, significant progress has been made in applying deep learning to image-to-image translation for solar observations. For example, solar (E)UV images have been successfully translated into magnetograms (e.g., Kim et al. 2019; Jeong et al. 2020, 2022). In contrast, Park et al. (2019) generated (E)UV images from magnetograms. Lee et al. (2021)


[^0]:    * Corresponding author; moonyj@khu.ac.kr
![img-0.jpeg](img-0.jpeg)

Fig. 1. Overview of our study. (a) Training process. This image is reproduced from Jeong et al. (2022). The generator (G) takes input images (AIA 171 and AIA 304 Å) and translates them into new images. The red box at the boundary of the images indicates the generator images. The discriminator (D) distinguishes between real pairs and fake pairs. The inspector (I) guides the generator by computing concordance CC values, ensuring the generated images are not only realistic but also scientifically accurate according to the data relationships. To utilize this model, we first trained five models for each channel (94, 131, 193, 211, and 335 Å) using only the AIA observed dataset. (b) Applying process. After we successfully trained the models, we applied the generative models to the FSI observed dataset. The dashed blue line shows the results when a deep learning model was applied to the FSI dataset. Finally, the observed FSI 174 Å with AI-generated sets could determine the DEMs.

transformed Galileo's historical sunspot sketches into modern EUV images, and Son et al. (2021) generated He I images from EUV images, effectively mitigating the limitations of ground-based observations through the use of satellite observations. Additionally, Salvatelli et al. (2019) demonstrated the capability to translate between different EUV image channels, and Lim et al. (2021) suggested how to select specific EUV channels to improve the translation performance among EUV channels. On the other hand, Salvatelli et al. (2022) explored the limitations of synthetic EUV images generated via deep learning, that is, the translation performance declines during extreme events such as solar flares. More recently, Park et al. (2023) made a pixel-to-pixel translation to generate EUV images and demonstrated that AI-generated data can be successfully used for DEM calculations.

In this study, we aim to address whether we can properly determine DEMs from Solar Orbiter/EUI/FSI using AI-generated data. To do this we generated five EUV channel datasets from FSI 174 and 304 Å using deep learning models. The paper is organized as follows. In Sect. 2, we describe the structure of our deep learning model and explain how we tested, trained, and applied it. We also explain how we applied the DEM method to the data. In Sect. 3, we describe the data and calibrate the Solar Orbiter/EUI/FSI and the SDO/AIA datasets. In Sect. 4, we present the results of the deep learning models and DEMs. A brief summary and conclusion are given in Sect. 5.

# 2. Methods

Here we explain the overall process shown in Figure 1. To perform this study, we first trained our deep learning models for SDO/AIA 171 and 304 Å to generate each channel (94, 131, 193, 211, and 335 Å). Next, we applied the trained models to EUI/FSI 174 and 304 Å data. Lastly, we determined the DEMs from FSI 174 Å data and AI-generated five-channel data.

### 2.1. Deep learning model

We used the Pix2PixCC model created by Jeong et al. (2022), the structure of which is shown in Figure 1a. This model is an improved version of Pix2PixHD (Wang et al. 2018). It combines the original loss functions with a concordance correlation coefficient (CC) (Lin 1989) loss to enhance performance. We first trained the models to generate data for five channels (94, 131, 193, 211, and 335 Å) from AIA 171 and AIA 304 Å. Then, we
applied the trained models to the FSI 174 and $304 \AA$ channels to generate the five-channel EUV data, which correspond to synthetic EUI/FSI data to be used as the input for DEM analysis.

### 2.2. Differential emission measure

For the present work, we used the DEM code that was initially developed by Hannah \& Kontar (2012) with Python codes ${ }^{1}$. This method provides error bars for both the temperature and the DEM value at that temperature. We computed the DEMs within a temperature range of 0.1 MK to 10 MK . First, we created AIgenerated five-channel FSI data from FSI 174 and $304 \AA$ pairs using deep learning models. Then we calculated the DEMs from FSI $174 \AA$ and the five-channel datasets (Figure 1b). Here we did not use FSI $304 \AA$ to calculate the DEMs because its He II $304 \AA$ line is optically thick (Dolliou et al. 2023).

We used the temperature response functions calculated for both FSI and AIA using the CHIANTI database version 10.1 (Dere et al. 2023), and these were calculated for a density of $n=10^{9} \mathrm{~cm}^{-3}$, a value typical for coronal loops. The calculated temperature response functions of AIA $171 \AA$ and FSI $174 \AA$ have similar profiles but different peak values; the response function of AIA $171 \AA$ tends to be slightly lower than that of FSI $174 \AA$. Based on Boerner et al. (2012), the response function is given by a product of the effective area and gain of the ChargeCoupled Device (CCD) camera system. The effective area, $A_{\text {eff }}$, provides information about the efficiency of the telescope optics, and the effective area is given by combinations of the geometrical collecting area, reflectance, transmission efficiency, quantum efficiency, and an additional correction. Due to different optical structures, the temperature response functions of SDO/AIA and Solar Orbiter/EUI/FSI are different. To compensate for this difference, we multiplied the FSI $174 \AA$ temperature response function by 0.7 , which corresponds to the ratio of the maximum values of both functions across the two instruments. Wright et al. (2017) employed a similar method across two different instruments (Hinode/XRT and SDO/AIA).

## 3. Data

### 3.1. SDO/AIA datasets

We used Level-1 SDO/AIA data from JSOC Data Export ${ }^{2}$. The data we used were taken at 00:00 UT from each day from January 2011 to December 2021, covering approximately one solar cycle. We discarded datasets with a QUALITY header value that was not zero to ensure the quality of the data. Then, we rotated the images to align the solar axis to the north, located the solar disk at the center of the image, and divided it by the exposure time. We also calibrated the degradation of the AIA imagers to set all the median values to 2011 January 01, 00:00 UT (Ugarte-Urra et al. 2015; Jeong et al. 2022). We took the logarithm base 2 and normalized it to a range of $[-1,1]$ to provide input for the deep learning models. The original image resolution was $4096 \times 4096$ pixels, which we resized to $1024 \times 1024$ pixels. We set the solar disk radius to 400 pixels, which is discussed in the next Sect. 3.2. We divided the datasets into training and testing sets, and considered the solar inclination and the elliptic orbit while maintaining a two-week gap between the two sets to prevent overlap. We selected 11 months for the training datasets and 1 month for the testing datasets, shifting the test month forward

[^0]each time. For example, in the 2011 datasets, January was used for the testing dataset and the remaining months for the training datasets. In 2012, February was used for the testing and the rest for the training.

### 3.2. Solar Orbiter/EUI/FSI datasets

We used Level-1 Solar Orbiter/EUI/FSI images after January 2022 from EUI data Release $6.0^{3}$. The FSI was tasked with capturing EUV images of the entire solar disk at $174 \AA$ and $304 \AA$. The original image resolution of FSI was $3072 \times 3072$. We removed the last 32 pixels since they were bad pixels. Due to the Solar Orbiter's elliptical orbit around the Sun, the apparent size of the solar disk in FSI images varied significantly throughout the mission. For example, at a distance of 1.01 AU , the solar disk had a radius of 213 pixels; at 0.29 AU , it had a radius of 739 pixels. To standardize input data for our deep learning models, which require consistency in radius pixels, we preprocessed all the images to have a reference radius of 400 pixels. For example, images with radii pixels larger than 400 pixels were resized, while those with smaller radii were cropped and then upscaled to maintain a consistent size. In addition, to calibrate the FSI degradation and the effect of its orbit, we set all the median values of the data to those at 00:00 UT on 2011 January 01, as used for the AIA data.

### 3.3. Intercalibration between AIA and FSI for deep learning

Since we trained our model using the AIA dataset, the application dataset (Solar Orbiter observations) should have been similar to the AIA dataset. However, there was a relative difference in brightness due to the optical differences between the two instruments: AIA has two mirrors while EUI/FSI has one. This led to slightly different temperature response functions and resulted in different contrasts in the images (Müller et al. 2020). Jarolim et al. (2024) also mentioned the same problem, noting that the two instruments need an intercalibration process. To correct the small discrepancies between AIA and FSI, we first calibrated the data to have the same median and then used thirdorder fitting to match the FSI $174 \AA$ data with the AIA $171 \AA$ data and the FSI $304 \AA$ data with the AIA $304 \AA$ data. To intercalibrate between the images from the two instruments, we drew a scatter plot of digital number (DN) within the range of $\pm 45^{\circ}$ longitude and latitude on the heliographic map, when the two instruments were at the inferior conjunction (at 08:30 UT on 2022 March 07 and at 21:30 UT on 2023 March 28, considering the time delay of the light). Further details can be found in Appendix A.

## 4. Result and discussion

### 4.1. Generation of EUV images

We generated the SDO/AIA 94, 131, 193, 211, and $335 \AA$ images from the SDO/AIA $171 \AA$ and AIA $304 \AA$ test datasets. Figure 2 shows a comparison of AIA observations and AI-generated ones on 2012 February 26. The comparison indicates that deep learning models successfully generate various coronal features on the solar disk. For instance, the shapes of coronal holes are similar. Additionally, the structures of active regions in the solar atmosphere are quite similar in both the observed and AI-generated

[^1]
[^0]:    1 github.com/ianan/demreg/
    2 jsoc.stanford.edu/

[^1]:    3 sidc.be/EUI/data/releases/202301_release_6.0/
![img-1.jpeg](img-1.jpeg)

**Fig. 2.** Comparison of observed and AI-generated AIA images at 00:00 UT on 2012 February 26. The top row shows observed AIA images in the 94, 131, 193, 211, and 335 Å channels. The bottom row shows AI-generated images from AIA 171 and 304 Å for the same channels. We note that the same conditions for each channel are used when plotting the images.

**Table 1.** Averaged metric values of SDO/AIA observed data and AI-generated data for the test dataset.

|  Channel (Å) | CC | RMSE [DN/s] | NRMSE  |
| --- | --- | --- | --- |
|  94 | 0.87 ± 0.07 | 0.50 ± 0.66 | 0.012 ± 0.006  |
|  131 | 0.96 ± 0.03 | 1.26 ± 0.88 | 0.008 ± 0.003  |
|  193 | 0.95 ± 0.02 | 38.96 ± 5.59 | 0.012 ± 0.003  |
|  211 | 0.95 ± 0.02 | 13.55 ± 3.08 | 0.010 ± 0.003  |
|  335 | 0.93 ± 0.05 | 0.84 ± 0.46 | 0.011 ± 0.005  |

images, suggesting that the AI models effectively generate the characteristics of active solar regions.

Table 1 shows the average metrics for the test datasets. We calculated the Pearson CC, the root mean square error (RMSE), and the normalized RMSE (NRMSE) between the observed data and the AI-generated data within one solar radius. In calculating these metrics, we restored the normalization and logarithm processes performed during the preprocessing of the dataset and calculated the comparison without any binning. The Pearson CC values between the AIA data and our deep learning models for the 94, 131, 193, 211, and 335 Å channels are 0.87, 0.96, 0.95, 0.95, and 0.93, respectively. The corresponding RMSE values are 0.50, 1.26, 38.96, 13.55, and 0.84. The NRMSE values are 0.012, 0.008, 0.012, 0.010, and 0.011. According to the metrics, the model that generates 131 Å yields the best performance, while 94 Å performs the worst. The 94 Å channel was intended to observe primarily the Fe XVIII at ~7 MK, while the input channels, 171 and 304 Å, observe relatively lower temperatures of ~7×10<sup>5</sup> K and ~5×10<sup>4</sup> K, respectively (O'Dwyer et al. 2010; Viall et al. 2020). In addition, the 94 Å images are relatively noisy (Park et al. 2023).

Figure 3 shows the comparison between observed AIA images and AI-generated ones from FSI 174 and 304 Å when the two instruments, SDO and Solar Orbiter, were in inferior conjunction (at 08:38 UT on 2022 March 07). At this moment, the Solar Orbiter was closer to the Sun than the SDO, at a distance of 0.5 AU. The overall structures are quite similar to each other. However, we found small discrepancies in the contrast, particularly in extremely dark or bright regions (e.g., coronal holes and active regions). To our knowledge, the response functions between AIA channels and EUI channels are slightly different (F. Auchère, private communication). These differences arise because the structures of the instruments are quite different, as mentioned in Sect. 3.3.

### 4.2. Comparison of differential emission measures

Now we can compare the DEMs from SDO and those from Solar Orbiter/EUI/FSI 174 Å together with AI-generated ones, when the Solar Orbiter and the SDO were in inferior conjunction (at 08:38 UT on 2022 March 07). This comparison could validate the hypothesis that our models successfully generate five channels from FSI 174 and 304 Å. To compare the DEMs between these two datasets, we selected the same coronal loop shown in Figure 4a. Before comparing the two datasets, we restored the dynamic scale, which ranges from [0, 2<sup>14</sup>−1]. We calculated the DEMs for each pixel within the contour of the region of interest (ROI) and then averaged them. Figure 4b illustrates the positions of the two spacecraft, with the plot showing the Solar Orbiter at 0.5 AU and in conjunction with SDO. Figure 4c shows the comparison between the DEMs determined from FSI 174 Å with the AI-generated dataset and the DEM determined from the AIA dataset. In determining the DEM for the Solar Orbiter, we used FSI 174 Å data that had been calibrated for degradation by only the median value (not third-order-fitted) and used the temperature response function of FSI 174 Å (see Sect. 3.3). The results of the two DEMs in Figure 4c are mostly consistent with each other. The temperature of the main peak in both DEMs is approximately log *T* = 6.0, and the DEM values at the main peak are within the error bar range. This similarity shows that the AI
![img-2.jpeg](img-2.jpeg)

**Fig. 3.** Comparison of observed images at 08:38 UT on 2022 March 07 and AI-Generated AIA images at 08:33 UT. The top row shows observed AIA images in the 94, 131, 193, 211, and 335 Å channels. The bottom row shows AI-generated images for the same channels, using input from third-order fitted FSI 174 and 304 Å data. We note that the same conditions are used in Figure 2.

![img-3.jpeg](img-3.jpeg)

**Fig. 4.** Results of the DEM calculation at 08:33 UT on 2022 March 07. (a) ROI for the DEM calculations. The red box in the first column of images marks the region shown in the remaining column of images. The white contour in the remaining column highlights the specific ROI. (b) Positions of the Solar Orbiter (purple dot) and SDO (green dot). (c) Comparison of DEM results from FSI 174 Å + AI-generated (purple line) images and AIA images. The vertical error bars represent the uncertainties of the DEM values, while the horizontal error bars indicate the temperature resolution.

generated images using our deep learning model can reproduce the DEMs well.

In a similar way, we determined the DEMs from two datasets when the Solar Orbiter was at different vantage points (at 00:00 UT on 2023 April 04), and the results are shown in Figure 5. Figure 5b illustrates the positions of the Solar Orbiter and the SDO when the Solar Orbiter is positioned at 0.33 AU and located 25 degrees ahead of the SDO. We selected the ROI from AIA and calculated it based on the solar coordinate system; then we used the calculated ROI in the FSI dataset. These different viewing angles might cause uncertainty in higher coronal structures, so we directed the ROI toward a low-height coronal
![img-4.jpeg](img-4.jpeg)

**Fig. 5.** Results of the DEM calculation at 00:00 UT on 2023 October 16. The layout of this figure is the same as in Figure 4.

structure. Figure 5a shows one of these coronal structures, specifically an active region that was growing. Figure 5c presents the results of the DEMs. The main peak temperature in both DEMs is almost consistent at around log *T* = 6.2, and the widths of the DEMs are quite similar. These facts suggest that our method can successfully determine DEMs even if the two observations are from different vantage points as well as from different datasets.

### 5. Summary and conclusion

In this study, we have demonstrated that we can properly determine the DEM from Solar Orbiter/EUI/FSI using deep learning, which can be a great extension of the coronal structures of the FSI. Using the Pix2PixCC model, we successfully generated five-EUV-channel data (94, 131, 193, 211, and 335 Å) from 174 and 304 Å observations of the Solar Orbiter/EUI/FSI datasets. We then determined the DEMs from FSI 174 Å together with AI-generated data for two cases. These DEMs are very consistent with those from the observed AIA dataset. This suggests that deep learning models successfully decompose the information of FSI 174 and 304 Å into each of the five channels.

In order to achieve more accurate results, further research on intercalibration techniques between the two instruments (SDO/AIA and Solar Orbiter/EUI/FSI) is essential. As the Solar Orbiter mission progresses, these intercalibration methods can be refined and improved, thereby improving the determination of DEMs.

While selecting the ROIs, calculations based on solar coordinates can introduce errors into the analysis because the morphology of a coronal loop is three-dimensional. Therefore, a more accurate method is needed to compare the DEMs using the data from the two instruments. For instance, Aschwanden et al. (2008b) performed triangulation and aligned the same loop using data from two spacecraft (STEREO A and B).

We plan to examine the DEM results observed from the same coronal loop when viewed stereoscopically. Analyzing the variations in the line-of-sight density and temperature distribution when observed from different angles will contribute to a more comprehensive understanding of the solar corona's three-dimensional structure (Aschwanden et al. 2008a). Additionally, this research could produce a global temperature map of the Sun.

We think our study can provide benefits for future missions at Lagrangian points L4 (Cho et al. 2023; Lee et al. 2024; Moon et al. 2024) and L5 (Vourlidas 2015). Since these missions cannot carry various channel imagers due to weight constraints, the ability to determine DEMs using fewer channels would be advantageous. Additionally, such missions would allow for long-term stereoscopic studies of the coronal structure from wide angles and enable researchers to determine coronal temperature changes from L5, via Earth, to L4.

Acknowledgements. We deeply appreciate the anonymous reviewers and editors for their thoughtful comments and detailed feedback, which have substantially contributed to improving the overall quality of this manuscript. Their perceptive suggestions have greatly strengthened both the clarity and the scientific depth of our work. This work was supported by the BK21 FOUR program through National Research Foundation of Korea (NRF) under Ministry of Education (MoE) (Kyung Hee University, Human Education Team for the Next Generation of Space Exploration), the Korea Astronomy and Space Science Institute under the R&D program (Project No. 2024-1-850-12) supervised by the Ministry of Science and ICT (MSIT), Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government MSIT (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15%), and Basic Science Research Program through the NRF funded by the MoE (NRF-2021R1I1A1A01049615, RS-2023-00248916, and RS-2024-00337363). Also, this research is partially funded by the BK21 FOUR program of Graduate School, Kyung Hee University (GS-1-JO-NON-20242364). CHIANTI is a collaborative project involving George
Mason University, the University of Michigan (USA), University of Cambridge (UK) and NASA Goddard Space Flight Center (USA). This study uses SDO and Solar Orbiter data. We acknowledge the Joint Science Operations Center (JSOC) for providing SDO/AIA data and Solar Influences Data Analysis Center (SIDC) for providing Solar Orbiter/EUI data. This work was supported by the use of PyTorch (Paszke et al. 2019), Astropy (Astropy Collaboration 2013), aiapy (Barnes et al. 2020), SunPy (Barnes et al. 2023), and NumPy (Harris et al. 2020), which provided essential tools for deep learning development and solar data preprocessing.

## References

Aschwanden, M. J., Nitta, N. V., Wuelser, J.-P., \& Lemen, J. R. 2008a, ApJ, 680, 1477
Aschwanden, M. J., Wülser, J.-P., Nitta, N. V., \& Lemen, J. R. 2008b, ApJ, 679, 827
Astropy Collaboration (Robitaille, T. P., et al.) 2013, A\&A, 558, A33
Barnes, W., Cheung, M., Bobra, M., et al. 2020, J. Open Source Software, 5, 2801
Barnes, W. T., Christe, S., Freij, N., et al. 2023, Front. Astron. Space Sci., 10, 1076726
Boerner, P., Edwards, C., Lemen, J., et al. 2012, Sol. Phys., 275, 41
Cheung, M. C. M., Boerner, P., Schrijver, C. J., et al. 2015, ApJ, 807, 143
Cho, K.-S., Hwang, J., Han, J.-Y., et al. 2023, J. Korean Astron. Soc., 56, 263
Dere, K. P., Del Zanna, G., Young, P. R., \& Landi, E. 2023, ApJS, 268, 52
Dolliou, A., Parenti, S., Auchère, F., et al. 2023, A\&A, 671, A64 (SO Nominal Mission Phase SI)
Hannah, I. G., \& Kontar, E. P. 2012, A\&A, 539, A146
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Nature, 585, 357
Jarolim, R., Tremblay, B., Muñoz-Jaramillo, A., et al. 2024, ApJ, 961, L31
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJ, 903, L25

Jeong, H.-J., Moon, Y.-J., Park, E., Lee, H., \& Baek, J.-H. 2022, ApJS, 262, 50
Kim, T., Park, E., Lee, H., et al. 2019, Nat. Astron., 3, 397
Lee, J.-Y., Raymond, J. C., Reeves, K. K., Moon, Y.-J., \& Kim, K.-S. 2017, ApJ, 844, 3
Lee, H., Park, E., \& Moon, Y.-J. 2021, ApJ, 907, 118
Lee, D.-Y., Kim, R.-S., Choi, K.-E., et al. 2024, J. Astron. Space Sci., 41, 1
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, Sol. Phys., 275, 17
Lin, L. I-K. 1989, Biometrics, 45, 255
Lim, D., Moon, Y.-J., Park, E., \& Lee, J.-Y. 2021, ApJ, 915, L31
Mandal, S., Peter, H., Klimchuk, J. A., et al. 2024, A\&A, 682, L9 (SO Nominal Mission Phase SI)
Moon, Y.-J., Cho, K.-S., Park, S.-H., et al. 2024, J. Korean Astron. Soc., 57, 35
Morgan, H., \& Pickering, J. 2019, Sol. Phys., 294, 135
Müller, D., St. Cyr, O. C., Zouganelis, I., et al. 2020, A\&A, 642, A1
O’Dwyer, B., Del Zanna, G., Mason, H. E., Weber, M. A., \& Tripathi, D. 2010, A\&A, 521, A21
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJ, 884, L23
Park, E., Lee, H., Moon, Y.-J., et al. 2023, ApJS, 264, 33
Paszke, A., Gross, S., Massa, F., et al. 2019, arXiv e-prints [arXiv:1912.01703]
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, Sol. Phys., 275, 3
Rochus, P., Auchère, F., Berghmans, D., et al. 2020, A\&A, 642, A8
Salvatelli, V., Bose, S., Neuberg, B., et al. 2019, arXiv e-prints [arXiv:1911.04006]
Salvatelli, V., dos Santos, L. F. G., Bose, S., et al. 2022, ApJ, 937, 100
Schmelz, J. T., Worley, B. T., Anderson, D. J., et al. 2011, ApJ, 739, 33
Son, J., Cha, J., Moon, Y.-J., et al. 2021, ApJ, 920, 101
Ugarte-Urra, I., Upton, L., Warren, H. P., \& Hathaway, D. H. 2015, ApJ, 815, 90
Viall, N. M., Kucera, T. A., \& Karpen, J. T. 2020, ApJ, 905, 15
Vourlidas, A. 2015, Space Weather, 13, 197
Wang, T. C., Liu, M. Y., Zhu, J. Y., et al. 2018, in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8798
Wright, P. J., Hannah, I. G., Grefenstette, B. W., et al. 2017, ApJ, 844, 132
## Appendix A: Intercalibration between AIA and FSI

Here, we reveal more information about results of our intercalibrations mentioned in Section 3.3. For the intercalibration between two different datasets, at the top row in Figure A. 1 we present scatter plots of $\mathrm{DN} / \mathrm{s}$ within the range of $\pm 45^{\circ}$ longitude and latitude of the heliographic map, when the two instruments are at the inferior conjunction (2022-03-07 08:30 UT and 202303-28 21:30 UT). As a result of the intercalibration, the standard deviation differences between the original data of AIA $304 \AA$ and EUI $304 \AA$ decreases from 19.32 to 16.04 when we apply the third-order fitting calibration. Similarly, the standard deviation differences between AIA $171 \AA$ and the FSI $174 \AA$ decreases from 105.21 to 85.23 . Moreover, these intercalibrations could gradually improve as the mission progresses. We present these results in the middle and bottom row in Figure A.1.

## Appendix B: Additional Figures

Here, we present additional cases of DEM results. Figure B1 highlights strands of coronal features, while Figure B2 highlights the outer coronal features. Both figures reveal the usefulness of our method in accurately determining DEMs for various solar structures, including off-disk features. The temperature of the main peak in Figure B1 is consistent at approximately $\log T=$ 6.2. Similarly, the temperature of the main peak in Figure B2 is also consistent at approximately $\log T=6.2$.
![img-5.jpeg](img-5.jpeg)

Fig. A.1. Scatter plots and histograms between two datasets. Each column shows the intercalibration between different channel of AIA and EUI. The solid lines in the scatter plots correspond to the third-order fitting, passing through zero, representing the relationship between FSI and AIA. Here, the x -axis corresponds to the $\mathrm{DN} / \mathrm{s}$ of FSI observations and the y -axis corresponds to the $\mathrm{DN} / \mathrm{s}$ of AIA observations. The histograms represent the distribution of differences between the original data and differences after third-order fitting. For both wavelengths, the standard deviations improve after intercalibration.
![img-6.jpeg](img-6.jpeg)

**Fig. B1.** The DEM determination of the strands of coronal features at 08:40 UT on 2024-03-20, when the two instruments were observing the same side of the disk. (a), (b), (c) layout of this figure is the same as in Figure 4.

![img-7.jpeg](img-7.jpeg)

**Fig. B2.** The DEM determination of outer coronal features at 21:30 UT on 2023-03-28, when the two instruments were observing the same side of the disk. (a), (b), (c) layout of this figure is the same as in Figure 4."
Sumiaya Rahman et al 2023 - Fast Reconstruction of 3D Density Distribution around the Sun Based on the MAS by Deep Learning.pdf,"# Fast Reconstruction of 3D Density Distribution around the Sun Based on the MAS by Deep Learning 

Sumiaya Rahman ${ }^{1}$, Seungheon Shin ${ }^{1,2}$ (D), Hyun-Jin Jeong ${ }^{3}$ (D), Ashraf Siddique ${ }^{4}$ (D), Yong-Jae Moon ${ }^{1,3}$ (D), Eunsu Park ${ }^{5}$ (D), Jihye Kang ${ }^{3}$ (D), and Sung-Ho Bae ${ }^{6}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Earth Intelligence Division, SI Analytics, Daejeon, 34047, Republic of Korea<br>${ }^{3}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{4}$ Department of Computer Science Engineering, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{5}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{6}$ Department of Computer Science Engineering, College of Software, Kyung Hee University, Yongin, 17104, Republic of Korea; sumiaya@khu.ac.kr<br>Received 2022 August 29; revised 2023 January 30; accepted 2023 February 16; published 2023 April 28


#### Abstract

This study is the first attempt to generate a three-dimensional (3D) coronal electron density distribution based on the pix2pixHD model, whose computing time is much shorter than that of the magnetohydrodynamic (MHD) simulation. For this, we consider photospheric solar magnetic fields as input, and electron density distribution simulated with the MHD Algorithm outside a Sphere (MAS) at a given solar radius is taken as output. We consider 155 pairs of Carrington rotations as inputs and outputs from 2010 June to 2022 April for training and testing. We train 152 deep-learning models for 152 solar radii, which are taken up to 30 solar radii. The artificial intelligence (AI) generated 3D electron densities from this study are quite consistent with the simulated ones from lower radii to higher radii, with an average correlation coefficient 0.97 . The computing time of testing data sets up to 30 solar radii of 152 deep-learning models is about 45.2 s using the NVIDIA TITAN XP graphics-processing unit, which is much less than the typical simulation time of MAS. We find that the synthetic coronagraphic images estimated from the deep-learning models are similar to the Solar Heliospheric Observatory (SOHO)/Large Angle and Spectroscopic Coronagraph C3 coronagraph data, especially during the solar minimum period. The AI-generated coronal density distribution from this study can be used for space weather models on a near-real-time basis.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar photosphere (1518); Astronomy data analysis (1858); Astronomy image processing (2306)

## 1. Introduction

Knowledge of the coronal electron density distribution can provide a better understanding of the physical dynamics of the solar corona (Gómez 2018) and inner heliosphere. To determine the compression ratio of shocks driven by coronal mass ejection (CME), determine the Alfvén Mach number, and predict the CME total mass and propagation, the ambient coronal electron density should be accurately estimated (Vourlidas et al. 2000; de Patoul et al. 2015; Feng et al. 2015). MHD modeling is the initial approach that can bridge self-consistently thoroughly from the heliocentric distances near the Sun to well beyond Earth's orbit (Usmanov 1998; Usmanov \& Goldstein 2003; Hayashi 2005; Detman et al. 2006; Riley et al. 2012; Feng et al. 2021). Furthermore, MHD models obtain a 3D distribution that can provide the distribution of the plasma parameters (i.e., density, temperature, and velocity) and the configuration of the magnetic field of the solar corona, inner heliosphere, and outer heliosphere (de Patoul et al. 2015). The MHD Algorithm outside a Sphere (MAS; Linker 1999; Mikić \& Linker 1999) model is such a 3D MHD model based on the photospheric magnetic field observations that have been developed to study the large-scale structure and dynamics of the solar corona (from 1 to 30 solar radii) and inner heliosphere (from 30 solar radii to 5 au ) (Riley et al. 2009). The MAS model considers the coronal heating, anisotropic

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
thermal conduction, and radiative losses to develop a more realistic model of the global coronal plasma density distribution (Lionello \& Linker 2009). However, this complex simulation technique demands significant computing time, resources, and effort. The entire simulation process, including the determination of the Sun's position, the gathering of the necessary data for one Carrington rotation (CR) process, the collection of data on the underlying magnetic field, and the computation of the simulation for space weather forecasting, takes at most a few days (Caplan \& Linker 2019).

Image-to-image translation is a well-known application in deep-learning-based image processing. It is the process of transforming an input image into a corresponding output image, where the main objective is to know the relationship between input and output data. Recently, several forms of network structure, loss functions, learning principles, and techniques have been employed in image translation. The generative adversarial network (GAN; Goodfellow et al. 2014) models like pix2pix (Isola et al. 2017) and pix2pixHD (Wang et al. 2018) prove to be a well-known deep-learning approach that provides state-of-theart performance in several applications in the field of astronomy and astrophysics, such as reconstruction and synthesis of data (Kim et al. 2019; Park et al. 2019; Jeong et al. 2020; Shin et al. 2020) and denoising (Park \& Moon 2019). For high-resolution image translation applications, the pix2pix model (Isola et al. 2017) fails to capture fine details and produces artifacts (Wang et al. 2018). On the other hand, the pix2pixHD model (Wang et al. 2018) can solve the limitations and generate fine features almost without any such artifacts for high-resolution data sets. It
can also increase computational efficiency significantly and minimize computational costs.

Jang et al. (2021) generated the 3D coronal electron density from coronagraphic images for one solar rotation period (27 days). They synthesized the input images from the target 3D electron density in their work by applying the Thomson scattering theory and developed their models only for six selected solar radii. According to our knowledge, this study is the first attempt to generate 3D solar coronal density distribution from the photospheric magnetic field using the deep-learning models. The most impressive thing is that the computation time is much faster than those of an MAS simulation. This study is organized as follows. The data are described in Section 2. The methods used for 3D coronal density distribution generation are discussed in Section 3. The results are given in Section 4. A brief summary and discussion are given in Section 5.

## 2. Data

The MAS model solves the time-dependent resistive thermodynamic MHD equations in 3D spherical coordinates to study the large-scale structure and dynamics of the solar corona (1 to 30 solar radii) and inner heliosphere ( 30 Rs to 5 au ) (Riley et al. 2009). The MAS model considers a synoptic map of the photospheric magnetic field constructed from a series of observations over 27 days and centered at the central meridian region. The synoptic map is formed by the fundamental boundary condition at the surface of the Sun, which has been described previously in Mikić \& Linker (1994), Lionello et al. (1999), Mikić \& Linker (1999), Linker et al. (2001), and Lionello \& Linker (2001). For this work, both the input and target data sets are collected from the MAS thermodynamic model. We consider the synoptic maps of the photospheric magnetic field data as input and the 3D distributions of coronal electron density up to 30 solar radii as the target. We collect the MAS simulation data from the MAS database (http://www.predsci.com/data).

The size of the input (photospheric magnetic field) data is $361 \times 182$ (longitude and latitude) from 2010 June to 2022 April (CR 2097 to CR 2256). The coronal electron density data of the MAS model have two different sizes for two different time periods: from 2010 June to 2020 May (CR 2097 to CR 2240), the size is $181 \times 101 \times 151$ (longitude, latitude, and altitude from the solar surface, respectively), and from 2020 June to 2022 April (CR 2241 to CR 2256), it is $300 \times 143 \times 255$. Since the size of the coronal electron density data of the MAS model for the two distinct periods is not the same, we align the coronal electron density data of these two different time periods and fix the size of our target data set as $182 \times 96 \times 151$ from 2010 June to 2022 April (CR 2097 to CR 2256). According to the size of the target, the input data (photospheric magnetic field data from MAS) are also aligned at the nearest latitude and longitude position of the target. After the alignment, the photospheric magnetic field data size is $182 \times 96$. We consider the same alignment process for the photospheric magnetic field data for testing our deep-learning models.

In this study, we consider 155 CRs for training and testing. The deep-learning models are tested using 33 CRs after being trained on 122 CRs. To include the solar maximum and the solar minimum period in training and testing data sets, we consider three consecutive CRs for training and the fourth CR for testing from 2010 June to 2022 April (CR 2097 to CR 2256). Here $26 \%$ of the data are used to evaluate the deep-
learning models. As the number of our simulation data is not many, we extend the training data set approximately 30 times by flipping (longitude and latitude) and rotating by $10^{\circ}$ in the longitudinal direction. Since the photospheric magnetic field data and coronal density map are asymmetric, we can obtain new magnetic field and density structures by flipping and rotating in order to provide a more diverse data set during training (Shorten \& Khoshgoftaar 2019).

We normalize input data sets with mean and standard deviation values according to Equation (1). The main goal of the normalization is recentering and rescaling the data. In addition, it increases the efficiency and stability of deeplearning models by allowing them to train the deep-learning models for various data values at the same scale. Since the density data are not limited in their ranges and they decrease rapidly as the altitude increases, we normalize the density data at each solar radius with their mean values and standard deviation according to Equation (2). The formulae of the normalization are as follows:

$$
\begin{gathered}
\bar{X}=\frac{X-\operatorname{mean}(X)}{\operatorname{Std}(X)} \\
\bar{Y}_{i}=\frac{Y_{i}-\operatorname{mean}\left(Y_{i}\right)}{\operatorname{Std}\left(Y_{i}\right)}
\end{gathered}
$$

where $X, \bar{X}$ are the original and normalized input data, respectively. $Y_{i}, \bar{Y}_{i}$ are the normalized and original electron density data of the $i$ th solar radii, respectively.

Here we apply random noise and Gaussian blur only to the input data of the training data set. When we consider any ground-based photospheric magnetic field observation with artifacts like noise and blurriness as input, our deep-learning models may overcome the artifacts.

## 3. Method

The pix2pixHD (Wang et al. 2018) is an improved version of the pix2pix (Isola et al. 2017) algorithm for high-resolution data. It employs conditional GANs (cGANs) with multiscale discriminator and generator architectures, as well as novel adversarial loss function. As the resolution of our data is low, we consider a generator $(G)$ that has U-Net (Ronneberger et al. 2015) architectures and a single discriminator $(D)$, shown in Figure 1. It simultaneously trains $D$ and $G$ during the learning process (Mao et al. 2017). The primary objective of the generator $G$ is to convert the inputs into outputs, and $D$ triggers the generator to create realistic outputs (Wang et al. 2018).

During the training process, the generator extracts features from the input data and reconstructs the outputs from the extracted features. The discriminator works like a classifier to identify whether the output is fake or real. In our training, we apply the Least Squares Generative Adversarial Networks (LSGAN; Mao et al. 2017) instead of GAN to stabilize the training, as followed in Wang et al. (2018). LSGAN penalizes the fake samples and forces the generator to generate outputs closer to the decision boundary. Thus, LSGAN overcomes the vanishing gradients' problem and performs more stably (Mao et al. 2017).
![img-0.jpeg](img-0.jpeg)

Figure 1. Architecture of our deep-learning model based on pix2pixHD.

![img-1.jpeg](img-1.jpeg)

Figure 2. Qualitative comparisons between electron density distribution from MAS simulation and AI-generated electron density distribution with deep-learning models of CR 2194, CR 2100, CR 2200, and CR 2174 at 2.004, 3.18, 10.48, and 20.28 solar radii, respectively. Left column: synoptic map of the photospheric magnetic field; middle column: electron density distribution from MAS simulation model; right column: AI-generated electron density distribution.

The objective function of LSGAN is as follows:

$$
\mathcal{L}_{\text{LSGAN}}^G(G, D) = \frac{1}{2} \left[ (D(x, y) - y)^2 \right] + \frac{1}{2} \left[ (D(x, G(x)))^2 \right],
$$

(3)

$$
\mathcal{L}_{\text{LSGAN}}^G(G, D) = \frac{1}{2} \left[ (D(x, G(x)) - y)^2 \right],
$$

(4)

where *x* and *y* denote input (photospheric magnetic field) and target (coronal electron density) data, respectively. *G(x)* means
![img-2.jpeg](img-2.jpeg)

Figure 3. Two descriptive comparisons of electron density map, density graph in 180° longitude, and 2D histogram plots between the target and the AI-generated one at 4.19, 12.63, 25.40, and 30.51 radii solar radii of CR 2178, CR 2220, CR 2205, and CR 2228, respectively. Left to right: MAS density map, AI-generated density map, density graph in 0° longitude, and 2D histograms.

an output from the generator for a given input *x*. *D*(*x*, *y*) and *D*(*x*, *G*(*x*)) are probabilities of the target pairs and AI-generated pairs, respectively.

To enable better learning, an additional loss function FM, known as ""feature matching loss,"" is added to the generator (Wang et al. 2018). FM loss is more effective for large dynamic range data because it reduces the absolute difference between the feature maps of the target data and AI-generated data (Rana et al. 2019; Marnerides et al. 2021). The FM loss function is expressed as follows:

$$
\mathcal{L}_{\text{FM}}(G, D) = \sum_{i=1}^{T} \frac{1}{N_i} \left\| [D^{(i)}(x, y) - D^{(i)}(x, G(x))] \right\|
$$

where *T* denotes the total number of layers in the discriminator and *N<sub>i</sub>* is the number of pixels in output feature maps of each layer.

The final objective of the deep-learning model is as follows:

$$
\min_{G} (\mathcal{L}_{\text{LSGAN}}^{G}(G, D) + \lambda \mathcal{L}_{\text{FM}}(G, D)),
$$

$$
\min_{D} \mathcal{L}_{\text{LSGAN}}^{D}(G, D),
$$

where *G* and *D* denote a generator and a discriminator, respectively, and $\mathcal{L}_{\text{GAN}}(G, D)$ and $\mathcal{L}_{\text{FM}}(G, D)$ are GAN loss and FM loss, respectively. Here $\lambda$ is a relative weight that controls $\mathcal{L}_{\text{LSGAN}}^{G}$ and $\mathcal{L}_{\text{FM}}$ following Wang et al. (2018).

During the learning process, the generator tries to minimize the FM loss, while the discriminator tries to maximize the LSGAN loss. The goal of the loss function is to optimize the deep-learning model parameters. For training and testing, we consider the photospheric magnetic field (*x*) as input and 3D coronal density distribution (*y*) as output from the MAS simulation as shown in Figure 1. To cover whole coronal regions from 2.004 to 30 solar radii, we consider 152 deep-learning models separately. We compare the AI-generated ones with the MAS-simulated data, as well as SOHO (Domingo et al. 1995)/LASCO (Brueckner et al. 1995) C3 coronagraph data. We employ the Adam (Kingma & Ba 2014) optimizer with momentum $\beta_1 = 0.5$ and $\beta_2 = 0.999$. The initial learning rate is $2 \times 10^{-4}$. In every $2 \times 10^5$ iterations of back-propagation, the learning rate is decreased to half from the previous label.

## 4. Results and Discussion

### 4.1. Qualitative and Quantitative Evaluation

A comparison between the results of our deep-learning models and the MAS simulation at 2.004, 3.18, 10.48, and 20.28 solar radii is shown in Figure 2. It seems from the results that our deep-learning models can reliably generate the electron density map of the solar minimum cases (CR 2100 and
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparisons of electron density distribution from the MAS simulation and AI-generated electron density map of CR 2186 at 2.07, 4.65, 25.40, and 30.51 solar radii. The left column represents the MAS density map, and the right column represents the AI-generated one.

CR 2174), as well as the solar maximum cases (CR 2194 and CR 2200).

Figure 3 shows a comparison of electron density maps, density graph in 180° longitude, and 2D histogram plots between the target and the AI-generated ones at 4.19, 12.63, 25.40, and 30.51 radii, respectively. As shown in Figure 3, both the target and AI-generated density are quite consistent with each other. Even at higher solar radii, the histograms accurately depict pixel distributions around the diagonal. We notice that just a small fraction of points are deviated from the line, while the rest of the points are near the diagonal line. Next, we compare the MAS density maps with the AI-generated density maps at 2.07, 4.65, 25.40, and 30.51 radii of CR 2186 in Figure 4. From the color bar of both the columns, we can see that the electron density varies with radius. It is impressive that our deep-learning models are able to reproduce the coronal electron density variation over radius. Although the data have a fairly complex structure, the deep-learning models can
![img-4.jpeg](img-4.jpeg)

Figure 5. Comparison of (a) SOHO/LASCO C3 coronagraph image observed on 2017 May 13, (b) the synthetic image based on the MAS simulation model, and (c) the AI-generated one.

#### Table 1

The Averaged Values of Pearson's Correlation Coefficient (CC) and Normalized rms Error (NRMSE) of the Solar Minimum and the Solar Maximum Period at 2.004, 3.12, 10.48, and 20.28 Solar Radii for the Testing Data Set

|  Radius | Solar Minimum | Solar Maximum  |
| --- | --- | --- |
|   | Pixel-to-pixel CC | NRMSE  |
|  2.004 | 0.99 | 0.080  |
|  3.12 | 0.98 | 0.085  |
|  10.48 | 0.99 | 0.078  |
|  20.28 | 0.99 | 0.076  |

qualitatively reproduce the overall 3D density structure. Additionally, our AI-generated results can be employed to infer the global coronal magnetic field topology since the density structures, like the heliospheric current sheet, manifest the magnetic structures.

In order to evaluate the testing data sets quantitatively, we took two types of objective metrics for solar maximum and solar minimum period separately. The first metric is a pixel-to-pixel Pearson's correlation coefficient between the target and the generated electron density map. The second metric is the normalized rms error (NRMSE). It is given by

$$RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y - y')^2},$$

$$NRMSE = \frac{RMSE}{y_{\text{max}} - y_{\text{min}}},$$

where *N*, *y*, and *y'* are the number of pixels in a single testing data, target, and AI-generated coronal electron density distribution, respectively. Here *y*max and *y*min are the maximum and minimum electron density value from the MAS model of the corresponding solar radius, respectively.

Table 1 represents the averaged pixel-to-pixel Pearson's correlation coefficient and NRMSE values between the density distribution obtained from the MAS model and the AI-generated ones at 2.004, 3.18, 10.48, and 20.28 solar radii for the testing data set. As shown in Table 1, the pixel-to-pixel correlation of our deep-learning model is higher during the solar minimum than the solar maximum. However, due to the more complex structure during the solar maximum period, the solar minimum period has better metrics values.

#### 4.2. Computational Time

To predict adverse space weather events and their impact on the geospace environment in near real time, high-performance computational and shorter-running time models are needed (Feng et al. 2013). In our study, we use a single NVIDIA TITAN XP graphics-processing unit (GPU), CUDA 11.4, and 12 GB. For the computing time evaluation, an AMD Ryzen 5 3600 6-Core Processor with 16.0 GB RAM CPU is used for loading and preprocessing of inputs, while a single GPU is used for forwarding inference. The most impressive result is that it takes approximately 45.2 s to compute 152 deep-learning models for the test data set with a resolution of 182 × 96 × 152 up to 30 solar radii, which is much faster than a usual computational time on the MAS model (Caplan & Linker 2019). On the other hand, the computing time of the MAS model with a resolution of 101 × 101 × 128 is about several days on a single-CPU machine (Mikić et al. 2018).

#### 4.3. Comparison with SOHO/LASCO C3 Data

Figure 5 shows the comparison among the SOHO/LASCO C3 coronagraph image on 2015 May 15 (solar minimum case), the corresponding synthetic image based on the MAS model, and the AI-generated result of our deep-learning models. We construct the synthetic images using the densities near the limb of the solar frontside from 2.004 to 30 solar radii as seen by SOHO/LASCO. The LASCO C3 and synthetic images are processed using the normalizing radial graded filter, as described by Morgan et al. (2006), in order to reveal faint coronal structures. It seems that the AI-generated ones are quite similar to the MAS simulation and the observation in view of the position of polar regions and other structures.

Figure 6 shows the comparison among the SOHO/LASCO C3 coronagraph image on 2012 March 17, the synthetic image based on the MAS model, and the one AI-generated by our deep-learning models. The figure shows that the global
![img-5.jpeg](img-5.jpeg)

Figure 6. Comparison result with (a) SOHO/LASCO C3 coronagraph observed on 2012 March 17, (b) the synthetic image based on the MAS simulation model, and (c) the one AI generated by our deep-learning models during the solar maximum period.

Configurations are similar but not exactly the same. We found few differences between the observed SOHO/LASCO C3 coronagraph image and AI-generated one. Such differences may be caused by synoptic magnetic fields used as input data of the MAS model. It is noted that these fields had been taken for one solar rotation period.

## 5. Conclusion and Summary

Our study is the first attempt to apply 152 deep-learning models to generate the 3D solar coronal electron density distribution up to 30 solar radii, whose computing times are much shorter than the MAS simulation one. For this work, we obtained the input photospheric magnetic field map and target solar coronal density distribution from the MAS simulation model collected from Predictive Science Inc. We used 155 pairs of data from CR 2097 to CR 2256 for training and testing our deep-learning models, which cover the time period from 2010 June to 2022 April.

Based on the outcomes of this study, we conclude that advanced deep-learning models can successfully generate the 3D solar coronal electron density distribution. Those results can be potentially used for specifying the condition on global magnetohydrodynamic models to reproduce large-scale coronal, heliospheric structures and solar wind modeling, such as the MAS heliospheric models (Lionello & Linker 2009) and the Wang-Sheeley-Arge model (Arge et al. 2004). Furthermore, deep-learning models can generate even small-scale density structures for both solar minima and maxima periods. We demonstrate that both solar maxima and minima AI-generated results are strongly correlated (the average pixel to pixel CC 0.97) with the target MAS-simulated electron density distribution map. In addition, the NRMSE values show that the deep-learning models are quite effective at reconstructing the electron density distribution map. Moreover, the intensity map and the histograms seem to be consistent not only in lower solar radii but also in higher solar radii. The impressive advantage of this work is that the computational time of the deep-learning models is much faster than the MAS simulation one. Finally, the synthetic coronagraphic data estimated from the models matched with the real coronagraph image of SOHO/LASCO data during the solar minimum period.

The deep-learning algorithm can generate complex and higher-radius electron density structures. The obtained AI-generated results can provide valuable insight for investigating and comparing physical aspects of the data from spacecraft in the inner heliosphere like Solar Orbiter (Müller et al. 2020) and Parker Solar Probe (Vourlidas et al. 2016). Furthermore, on a near-real-time basis, the AI-generated coronal density distribution can be used for reliable input for future space weather simulations. We intend to use more improved synchronic magnetic field data from the Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) and AI-generated farside magnetograms as input data in the future to overcome the limitation of the MAS for the solar maximum period Jeong et al. (2022). In the future, we have plan to generate other parameters of the MAS simulation model, like magnetic fields, temperature, and velocities which can be reproduced using the deep learning algorithm.

This research was supported by the Korea Astronomy and Space Science Institute under the R&D program (Project No. 2023-1-850-07) supervised by the Ministry of Science and ICT and NRF-2020R1C1C1003892 project. We thank the numerous team members who have contributed to the MAS simulation. We acknowledge the community effort devoted to developing the following open-source packages used in this work. We thank the numerous team members who have contributed to the success of the SDO mission. The SDO data were (partly) provided by the Korea Data Center (KDC) for SDO in cooperation with NASA, Stanford University (JSOC), and KISTI (KREONET), which is supported by the ""Next Generation Space Weather Observation Network"" project of the Korea Astronomy and Space Science Institute (KASI).

Software: SunPy (The SunPy Community et al. 2020), PyTorch (Paszke et al. 2019), NumPy (Harris et al. 2020).

## ORCID iDs

Seungheon Shin https://orcid.org/0000-0002-5144-8230
Hyun-Jin Jeong https://orcid.org/0000-0003-4616-947X
Ashraf Siddique https://orcid.org/0000-0003-2186-5735
Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
Eunsu Park (1) https://orcid.org/0000-0003-0969-286X Jihye Kang (1) https://orcid.org/0000-0001-6213-4088 Sung-Ho Bae (1) https://orcid.org/0000-0003-2677-3186

## References

Arge, C., Luhmann, J., Odstrcil, D., et al. 2004, JASTP, 66, 1295
Brueckner, G., Howard, R., Koomen, M., et al. 1995, SoPh, 162, 357
Caplan, R. M., Linker, J. A., Mikić, Z., et al. 2019, JPhCS, 1225, 012012
de Patoul, J., Foullon, C., \& Riley, P. 2015, ApJ, 814, 68
Detman, T., Smith, Z., Dryer, M., et al. 2006, JGRA, 111, A07102
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Feng, L., Inhester, B., \& Gan, W. 2015, ApJ, 805, 113
Feng, X., Wang, H., Xiang, C., et al. 2021, ApJS, 257, 34
Feng, X., Zhong, D., Xiang, C., \& Zhang, Y. 2013, ScChD, 56, 1864
Gómez, J. R., Vieira, L., \& Dal, L. 2018, ApJ, 852, 137
Goodfellow, I. , Pouget-Abadie, J., Mirza, M., et al. 2014, Commun. ACM, 63,139
Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, Natur, 585, 357
Hayashi, K. 2005, ApJS, 161, 480
Isola, P., Zhu, J.-Y., Tinghui, E., \& Efros, A. A. 2017, in Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, CVPR 2017 (Piscataway, NJ: IEEE), 1125
Jang, S., Kwon, R.-Y., Linker, J. A., et al. 2021, ApJL, 920, L30
Jeong, H.-J., Moon, Y.-J., Park, E., et al. 2020, ApJL, 903, L25
Jeong, H.-J., Moon, Y.-J., Park, E., et al. 2022, ApJS, 262, 50
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Linker, J. A., Lionello, R., Mikić, Z., et al. 2001, JGR, 106, 25165
Linker, J. A., Mikić, Z., \& Biesecker, D. A. 1999, JGR, 104, 9809
Lionello, R., Linker, J. A., \& Mikić, Z. 2001, ApJ, 546, 542
Lionello, R., Linker, J. A., \& Mikić, Z. 2009, ApJ, 690, 902
Lionello, R., Mikić, Z., \& Linker, J. A. 1999, JCoPh, 152, 346

Mao, X., Li, Q., Xie, H., et al. 2017, in Proc. of the IEEE Int. Conf. on Computer Vision (Piscataway, NJ: IEEE), 2794
Mamerides, D., Bashford-Rogers, T., \& Debattista, K. 2021, Senso, 21, 4032
Mikić, Z., Downs, C., Linker, J., et al. 2018, NatAs, 2, 913
Mikić, Z., \& Linker, J. A. 1994, ApJ, 430, 898
Mikić, Z., Linker, J. A., \& Schnack, D. D 1999, PhPl, 6, 2217
Morgan, H., Habbal, S. R., \& Woo, R. 2006, SoPh, 236, 263
Müller, D., Cyr, O. S., Zouganelis, I., et al. 2020, A\&A, 642, A1
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, E., Moon, Y.-J., Lim, D., et al. 2019, ApJL, 891, L4
Paszke, A., Gross, S., Massa, F., et al. 2019, Advances in Neural Information Processing Systems 32 (Red Hook, NY: Curran Associates), https://papers. nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740Abstract.html
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Rana, A., Singh, P., Valenzise, G., et al. 2019, ITIP, 29, 1285
Riley, P., Linker, J., Lionello, R., et al. 2009, Proc. Solar Heliospheric and Interplanetary Environment Conf. (SHINE 2009), 38
Riley, P., Linker, J., Lionello, R., et al. 2012, JASTP, 83, 1
Ronneberger, O., Fischer, P., \& Brox, T. 2015, in Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th Int. Conf. (Cham: Springer), 234
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Shorten, C., \& Khoshgoftaar, T. M. 2019, J. Big Data, 6, 1
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Usmanov, A. 1998, SoPh, 146, 377
Usmanov, A., \& Goldstein, M. 2003, in AIP Conf. Proc. 679, Solar Wind Ten (Melville, NY: AIP), 393
Vourlidas, A., Howard, R., \& Plunkett, A. 2016, SSRv, 204, 83
Vourlidas, A., Subramanian, P., Dere, K. P., et al. 2000, ApJ, 534, 456
Wang, T.-c., Liu, M.-Y., Jun-Yan, T., et al. 2018, in Proc. IEEE Conf. on Computer Vision and Pattern Recognition (Piscataway, NJ: IEEE), 8798"
Taeyoung Kim et al 2019 - Solar farside magnetograms from deep learning analysis of STEREO EUVI data.pdf,"# Solar farside magnetograms from deep learning analysis of STEREO/EUVI data 

Taeyoung Kim ${ }^{1,7}$, Eunsu Park ${ }^{1,7}$, Harim Lee ${ }^{1,2,7}$, Yong-Jae Moon ${ }^{1,2 *}$, Sung-Ho Bae ${ }^{3}$, Daye Lim ${ }^{1}$, Soojeong Jang ${ }^{4}$, Lokwon Kim ${ }^{3}$, II-Hyun Cho ${ }^{2}$, Myungjin Choi ${ }^{5}$ and Kyung-Suk Cho ${ }^{4,6}$

Solar magnetograms are important for studying solar activity and predicting space weather disturbances'. Farside magnetograms can be constructed from local helioseismology without any farside data ${ }^{2,3}$, but their quality is lower than that of typical frontside magnetograms. Here we generate farside solar magnetograms from STEREO/Extreme UltraViolet Imager (EUVI) 304-Å images using a deep learning model based on conditional generative adversarial networks (cGANs). We train the model using pairs of Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) 304-Å images and SDO/Helioseismic and Magnetic Imager (HMI) magnetograms taken from 2011 to 2017 except for September and October each year. We evaluate the model by comparing pairs of SDO/HMI magnetograms and cGAN-generated magnetograms in September and October. Our method successfully generates frontside solar magnetograms from SDO/AIA 304-Å images and these are similar to those of the SDO/HMI, with Hale-patterned active regions being well replicated. Thus we can monitor the temporal evolution of magnetic fields from the farside to the frontside of the Sun using SDO/HMI and farside magnetograms generated by our model when farside extreme-ultraviolet data are available. This study presents an application of image-to-image translation based on cGANs to scientific data.

Here we apply a deep learning model based on cGANs ${ }^{5,6}$ to solar magnetograms and extreme-ultraviolet images. For training and evaluation datasets, we consider pairs of $\mathrm{SDO}^{+} / \mathrm{AIA}^{6} 304-\AA$ images and $\mathrm{SDO} / \mathrm{HMI}^{5}$ line-of-sight magnetograms with 12 -hour cadence from 2011 to 2017. As a result, we produce 4,972 pairs of SDO/AIA images and SDO/HMI magnetograms. We select 4,147 pairs from 2011 to 2017 (except for September and October) for the training dataset and 825 pairs from September and October for the evaluation dataset.

Next we evaluate how well our model generates magnetograms. Figure 1 shows AIA images as the input data, artificial intelligence (AI)-generated magnetograms, and HMI magnetograms. A comparison shows that the bipolar structures of the HMI magnetograms were well restored. The quality of the AI-generated magnetograms is highly comparable with that of the HMI ones. Even though we do not have any a priori conditions, bipolar structures in AI-generated magnetograms mostly follow Hale's law, which is an observational rule: one polarity should precede the other polarity in the northern hemisphere and vice versa in the southern hemisphere. In the
training step, the generator is trained to learn the polarity patterns of active regions. In the evaluation and generation step, the generator reproduces the pattern. Since all data are from the 24th solar cycle, there is no difficulty in producing the Hale's law pattern in this cycle. We note that the polarity of the solar magnetic field is reversed cycle by cycle. Hence, since our model has been trained on the 24th solar cycle, it would be effective for even solar cycles, but should be tested for odd cycles. A careful comparison between two magnetograms shows that the tilt angle between a preceding sunspot and the one that follows it is not always properly generated, which is a limitation of our method. The extreme-ultraviolet $304-\AA$ emission is from the chromosphere-transition region, whereas the HMI measures the magnetic field in the photosphere. The discrepancy in the detailed structures between the real magnetogram and the AI-generated magnetogram reflects the difficulty of the present model in precisely reconstructing the photospheric magnetic fields from chromosphere emission signatures.

Here we also quantitatively evaluate our model for full disk, active regions and quiet regions. Table 1 shows the results of four types of objective measure between SDO/HMI magnetograms and AI-generated ones for three regions. The first objective measurement is the correlation coefficient (CC) between the total unsigned magnetic flux (TUMF) of HMI magnetograms and that of AI-generated ones. Our model shows that the correlation coefficients are $0.97,0.95$ and 0.74 for the full disk, active regions and quiet regions, respectively. These values demonstrate that the generation of the TUMF through deep learning is consistent. The second objective measurement is the pixel-to-pixel correlation between SDO/HMI magnetograms (signed) and AI-generated ones after $8 \times 8$ binning. The average correlation coefficients are 0.77 for 825 full-disk images, 0.66 for 1,033 active regions, and 0.21 for 825 quiet regions. These imply that AI-generated magnetograms are well generated for active regions but not for quiet regions. The third objective measurement is the relative error $(R 1)$ of the TUMF. The mean $R 1$ values are $0.067,0.072$ and 0.091 , and the standard deviation values are $0.036,0.019$ and 0.075 for the full disk, active regions and quiet regions, respectively. This means that our model tends to slightly overestimate the TUMF. The last objective measurement is the normalized mean square error $(R 2)$ of the magnetic field. The mean $R 2$ values are $0.053,0.125$ and 0.041 , and the standard deviation values are $0.008,0.049$ and 0.005 for the full disk, active regions and quiet regions, respectively, implying that overall there are differences of about $23 \%$, about $35 \%$ and about $20 \%$ between them.

[^0]
[^0]:    ${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, South Korea. ${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, South Korea. ${ }^{3}$ Department of Computer Science and Engineering, College of Electronics and Information, Kyung Hee University, Yongin, South Korea. ${ }^{4}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon, South Korea. ${ }^{5}$ InSpace Co., Ltd., Daejeon, South Korea. ${ }^{6}$ Department of Astronomy and Space Science, University of Science and Technology, Daejeon, South Korea. ${ }^{7}$ These authors contributed equally: Taeyoung Kim, Eunsu Park, Harim Lee. *e-mail: moonyj@khu.ac.kr
![img-0.jpeg](img-0.jpeg)

Fig. 1 | Comparison between HMI magnetograms and AI-generated ones from SDO/AIA 304-Å images. a, The AIA images, which are the input data for the generator, taken from 1 September to 7 September 2017 with 2-day cadence. b, The AI-generated magnetograms from AIA images using the model. c, The HMI images are the ground truth. We use $\pm 100 \mathrm{G}$ as the saturation value for both SDO/HMI magnetograms and AI-generated magnetograms to show the structure of the active regions.

Table 1 | Four objective measures of comparison between SDO/HMI magnetograms and AI-generated ones for full disk, active regions and quiet regions

|  |  | Full disk | Active region | Quiet region |
| :--: | :--: | :--: | :--: | :--: |
|  |  | 825 images ( $1,024 \times 1,024$ pixels) | 1,033 patches ( $128 \times 128$ <br> pixels) | 825 patches <br> ( $128 \times 128$ <br> pixels) |
| Total unsigned magnetic flux CC |  | 0.97 | 0.95 | 0.74 |
| Pixel-to-pixel CC ( $8 \times 8$ binning) |  | 0.77 | 0.66 | 0.21 |
| Relative error, R1 | Mean | 0.067 | 0.072 | 0.091 |
|  | Standard deviation | 0.036 | 0.019 | 0.075 |
| Normalized mean square error, R2 | Mean | 0.053 | 0.125 | 0.041 |
|  | Standard deviation | 0.008 | 0.049 | 0.005 |

Now we apply our model to the EUVI ${ }^{32} 304-\AA$ images on board the Solar TErrestrial RElationship Observatory (STEREO ${ }^{33}$ ), whose filter response function is consistent with that of AIA images ${ }^{32-34}$, in order to generate farside magnetograms. In the case of STEREO-B, the data are only available before 1 October 2014, owing to multiple hardware anomalies affecting control of the spacecraft orientation. Figure 2 shows a series of extreme-ultraviolet $304-\AA$ images from STEREO-B/ EUVI and AIA, and magnetograms from 4 June to 13 June 2014: two farside magnetograms generated by our model and two HMI ones. On 4 June, the STEREO-B is located -164 heliographic longitudinal degrees from the central meridian, which makes the STEREO-B images mostly farside ones. It is evident that the parallel bipolar structures in the NOAA active region 12087, as indicated by the yellow box in Fig. 2, are well generated and conserved during the time of observations. Thus, we can successfully monitor the temporal evolution of this active region from the farside to the frontside of the Sun when farside extreme-ultraviolet data are available.

It is interesting to monitor the continuous evolution of active regions from the farside to the frontside. Figure 3 shows the temporal evolution of the TUMF of the NOAA active region 12087 from 3 June to 19 June 2014. As seen in the figure, the magnetic fluxes of this active region taken from the farside are about two times larger than those from the frontside. We note that the measurements of magnetic fluxes near the eastern limb are substantially underestimated because of the projection effects. The temporal evolution is valuable for space weather forecasts because there were three consecutive strong flares (X2.2, X1.5 and X1.0) near the limb on 10-11 June 2014. We estimate the uncertainty of magnetic flux from farside magnetograms. It is not easy to conduct an inter-calibration between AIA and EUVI given that they could not observe the same field of view. To estimate the uncertainty, we compare one monthly average intensity from two instruments in June 2014, taking into account the solar rotation rate of $13.2^{\circ}$ per day. The comparison shows that the EUVI intensity is about $25.5 \%$ higher than that of AIA. As seen in Table 1, the magnetic
![img-1.jpeg](img-1.jpeg)

Fig. 2 | A series of 304-Å images and magnetograms. a, The first two 304-Å images are taken from STEREO-B/EUVI and the last two from SDO/AIA. b, The first two magnetograms are AI-generated farside ones from the model and the last two are taken from SDO/HMI. The yellow boxes show the tracking of the NOAA active region 12087 from the farside to the frontside. We use $\pm 100 \mathrm{G}$ as the saturation value for both SDO/HMI and AI-generated magnetograms.
![img-2.jpeg](img-2.jpeg)

Fig. 3 | A temporal evolution of total unsigned magnetic flux of the NOAA active region 12087 from 3 to 19 June 2014. The filled diamonds represent the total unsigned magnetic flux from AI-generated magnetograms using STEREO-B/EUVI 304-Å images. The error bars indicate their relative error described in Table 1. The downward arrows indicate that the NOAA active region 12087 is located at eastward $60^{\circ}$ and $100^{\circ}$ heliographic longitudes from the central meridian, respectively. The filled circle represents the total unsigned magnetic flux from SDO/HMI magnetograms. The solid line represents 5-min-averaged GOES-15 X-ray flux (0.1-0.8 nm). GOES, Geostationary Operational Environmental Satellite.
flux is slightly overestimated by about $6.7 \%$. By assuming that the two types of errors are independent, we conjecture that the total uncertainty is an overestimation of $26.3 \%$. As shown in Fig. 3, the magnetic fluxes of the farside magnetograms are about two times larger than those of HMI, which are about four times higher than $26.3 \%$. Thus we conclude that the magnetic flux of this active region greatly decreased from the farside to the frontside of the Sun. Under conditions where extreme-ultraviolet farside data are available, farside magnetograms can be well reproduced, greatly improving current knowledge of the magnetic structure of farside active regions. Our method is reliable if the farside active regions conform to Hale's law, as long as the slight overestimation of their total flux and a possible slight difference in their tilt angle are considered.

In this Letter, the proposed model shows sufficient potential for image-to-image translation between two different scientific sensor images. In the fields of astronomy and geophysics, many multiwavelength observations are available, so the model can be used to extend these kinds of data. This method can also be applied to a variety of scientific fields that use different kinds of sensor image.

## Methods

The use of generative adversarial networks, a popular deep learning method, has been widely examined for image-generation tasks. Isola et al. ${ }^{5}$ suggested a general-purpose way to resolve image-to-image translation problems using cGANs. A comparison between output images from the model and the ground truth shows that this model works for various types of image-to-image translations. In this Letter we adopt their model for the image translation of solar magnetograms from SDO/AIA images.

Datasets. We use SDO datasets from the Joint Science Operations Center and STEREO datasets from the STEREO Science Center using SunPy and Solar Software. We first made Level 1.5 images by calibrating, rotating and centring the images, and then converted them to 8 -bit scale images of 1,024 by 1,024 pixels. Then we excluded a set of image pairs with poor quality; for example, images too noisy because of solar flares, those with incorrect header information, and images untypical for reasons such as the eclipse of a planet.

Training. The model trains the generator network and the discriminator network with three objectives. The first objective is to train the generator network to minimize the mean error between an HMI magnetogram $\left(I_{H}\right)$ and an AI-generated magnetogram $\left(I_{I J}\right)$, which is generated using the corresponding AIA image $\left(I_{A}\right)$. The second objective is to train the generator network to reproduce the true data distribution of HMI magnetograms $\left(I_{H}\right)$ from the corresponding AIA images $\left(I_{A}\right)$. The third objective is to train the discriminator network to distinguish the real pair $\left(I_{R}, I_{I J}\right)$ from the AI-generated pair $\left(I_{A}, I_{I J}\right)$. In every epoch, one generator network is constructed. Here, one epoch is complete when an entire training dataset of 4,147 pairs has passed through the model for training. As a result, we acquire 120 generator networks while the generator network and the discriminator network are alternately trained for 500,000 iterations (about 120 epochs). For specific hyperparameters, 120 epochs correspond to three days, which is the maximum period that we can compute with our computing capability.

Evaluation. In the evaluation step, we compare the HMI magnetograms with the ones generated using AIA images. For comparison, we estimate the correlation between TUMFs from both magnetograms. Then we compute the relative error, the normalized root-mean-square error, and pixel-to-pixel correlations for all evaluation datasets. When we calculate the TUMFs of both magnetograms, we use 10 G as a threshold to consider the noise level of line-of-sight SDO/HMI magnetograms ${ }^{12}$. The equation for the relative error $(R 1)$ of the TUMF $(\Phi)$ is given by

$$
R 1_{i}=\left(\Phi_{i}^{A I}-\Phi_{i}^{\mathrm{HMI}}\right) / \Phi_{i}^{\mathrm{HMI}}
$$
where $i$ is the serial number of the 825 evaluation samples. This value corresponds to the overestimation $(R 1_{i}>0)$ or underestimation $(R 1_{i}<0)$ that our method attributes to the TUMF. The equation for the normalized mean square error $(R 2_{i})$ of the magnetic field $\left(B_{i}\right)$ is given by

$$
R 2_{i}=\sum\left(B_{j}^{\mathrm{M}}-B_{i}^{\mathrm{HMI}}\right)^{2} / \sum\left(B_{j}^{\mathrm{HMI}}\right)^{2}
$$

where $i$ is the serial number of the 825 evaluation samples and $j$ is the pixel number of the solar disk. In addition, we did the same analysis for cropped patches of active and quiet regions within $\pm 60^{\circ}$ of the central meridian.

## Data availability

The code is available at https://github.com/tykimos/SolarMagGAN. In the readme file, we explain the architecture and selected hyperparameters. The SDO data are available from the SDO data centre (https://sdo.gsfc.nasa.gov/data/), the Joint Science Operations Center (http://jsoc.stanford.edu/) and the Korean Data Center for SDO (http://sdo.kasi.re.kr/). The STEREO data are available from the STEREO Science Center (https://stereo-ssc.nascom.nasa.gov/data.shtml). We used the following open-source packages: NumPy (http://www.numpy.org) and Keras (https://keras.io/).

## Received: 10 April 2018; Accepted: 24 January 2019;

Published online: 04 March 2019

## References

1. Solanki, S. K., Inhester, B. \& Schussler, M. The solar magnetic field. Rep. Prog. Phys. 69, 563-668 (2006).
2. Lindsey, C. \& Braun, D. C. Seismic images of the far side of the Sun. Science 287, 1799-1801 (2000).
3. Braun, D. C. \& Lindsey, C. Seismic imaging of the far hemisphere of the Sun. Astrophys. J. Lett. 560, 189-192 (2001).
4. Lindsey, C. \& Braun, D. C. Seismic imaging of the Sun's far hemisphere and its applications in space weather forecasting. Space Weather 15, $761-781$ (2017).
5. Goodfellow, I. et al. Generative adversarial nets. Adv. Neur. 2014, 2672-2680 (2014).
6. Isola, P., Zhu, J.-Y., Zhou, T. \& Efros, A. A. Image-to-image translation with conditional adversarial networks. IEEE Proc. Comput. Vision Pattern Recog. 2017, 1125-1134 (2017).
7. Pesnell, W. D., Thompson, B. J. \& Chamberlin, P. C. The Solar Dynamics Observatory (SDO). Sol. Phys. 275, 3-15 (2012).
8. Lemen, J. R. et al. The Atmospheric Imaging Assembly (AIA) on the Solar Dynamics Observatory (SDO). Sol. Phys. 275, 17-40 (2012).
9. Schou, J. et al. Design and ground calibration of the Helioseismic and Magnetic Imager (HMI) instrument on the Solar Dynamics Observatory (SDO). Sol. Phys. 275, 229-259 (2012).
10. Howard, R. A. et al. Sun Earth Connection Coronal and Heliospheric Investigation (SECCHI). Space Sci. Rev. 136, 67-115 (2008).
11. Kaiser, M. L. et al. The STEREO mission: an introduction. Space Sci. Rev. 136, $5-16$ (2008).
12. Dere, K. P. et al. CHIANTIan atomic database for emission lines. Astron. Astrophys. Suppl. 125, 149-173 (1997).
13. Landi, E., Zanna, G. D., Young, P. R., Dere, K. P. \& Mason, H. E. CHIANTIan atomic database for emission lines. XII. Version 7 of the database. Astrophys. J. 744, 99-107 (2012).
14. Nitta, N. V. et al. Soft X-ray fluxes of major flares far behind the limb as estimated using STEREO EUV images. Sol. Phys. 288, 241-254 (2013).
15. Liu, Y. et al. Comparison of line-of-sight magnetograms taken by the Solar Dynamics Observatory/Helioseismic and Magnetic Imager and Solar and Heliospheric Observatory/Michelson Doppler Imager. Sol. Phys. 279, 295-316 (2012).

## Acknowledgements

We thank the numerous team members who contributed to the success of the SDO mission, as well as the STEREO mission. We acknowledge the community efforts devoted to developing the open-source packages that were used in this work (NumPy and Keras). This work was supported by the BK21+ Program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131), a grant from the NRF funded by the Korean government (number NRF-2013M1A3A3A02042232), the Korea Astronomy and Space Science Institute under the R\&D programme supervised by the Ministry of Science and ICT, the Korea Astronomy and Space Science Institute under the R\&D programme 'Development of a Solar Coronagraph on International Space Station (Project No. 2019-1-850-02)' supervised by the Ministry of Science and ICT, a grant from the Institute for Information \& Communications Technology Promotion (IITP) funded by the Korea government (MSIP) (number 2018-0-01422, 'Study on Analysis and Prediction Technique of Solar Flares'), and the Artificial Intelligence Laboratory at the InSpace Co., Ltd. The SDO data were (partly) provided by the Korea Data Center (KDC) for SDO in cooperation with NASA and the SDO/HMI Team, which is operated by the KASI.

## Author contributions

T.K., E.P. and H.L. contributed equally to this study. T.K., E.P., H.L. and Y.-J.M. devised the method, analysed data and wrote the manuscript. S.-H.B. participated in discussing the results and contributed to improving the methodology. D.L., S.J., I.-H.C., L.K., M.C. and K.-S.C. participated in discussing the results.

## Competing interests

The authors declare no competing interests.

## Additional information

Reprints and permissions information is available at www.nature.com/reprints.
Correspondence and requests for materials should be addressed to Y.-J.M.
Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
(c) The Author(s), under exclusive licence to Springer Nature Limited 2019"
Jihyeon Son et al 2023 - Three-day Forecasting of Solar Wind Speed Using SDO AIA Extreme-ultraviolet Images by a Deep-learning Model.pdf,"# Three-day Forecasting of Solar Wind Speed Using SDO/AIA Extreme-ultraviolet Images by a Deep-learning Model 

Jihyeon Son ${ }^{1}$ (D) Suk-Kyung Sung ${ }^{2}$, Yong-Jae Moon ${ }^{1,2}$ (D), Harim Lee ${ }^{2}$ (D), and Hyun-Jin Jeong ${ }^{2}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2023 March 23; revised 2023 June 15; accepted 2023 July 6; published 2023 August 10


#### Abstract

In this study, we forecast solar wind speed for the next 3 days with a 6 hr cadence using a deep-learning model. For this we use Solar Dynamics Observatory/Atmospheric Imaging Assembly 211 and $193 \AA$ images together with solar wind speeds for the last 5 days as input data. The total period of the data is from 2010 May to 2020 December. We divide them into a training set (January-August), validation set (September), and test set (OctoberDecember), to consider the solar cycle effect. The deep-learning model consists of two networks: a convolutional layer-based network for images and a dense layer-based network for solar wind speeds. Our main results are as follows. First, our model successfully predicts the solar wind speed for the next 3 days. The rms error (RMSE) of our model is from $37.4 \mathrm{~km} \mathrm{~s}^{-1}$ (for the 6 hr prediction) to $68.2 \mathrm{~km} \mathrm{~s}^{-1}$ (for the 72 hr prediction), and the correlation coefficient is from 0.92 to 0.67 . These results are much better than those of previous studies. Second, the model can predict sudden increase of solar wind speeds caused by large equatorial coronal holes. Third, solar wind speeds predicted by our model are more consistent with observations than those by the Wang-Sheely-Arge-ENLIL model, especially in high-speed-stream regions. It is also noted that our model cannot predict solar wind speed enhancement by coronal mass ejections. Our study demonstrates the effectiveness of deep learning for solar wind speed prediction, with potential applications in space weather forecasting.


Unified Astronomy Thesaurus concepts: The Sun (1693); Convolutional neural networks (1938); Solar wind (1534)

## 1. Introduction

The study of space weather has become increasingly important nowadays because of its significant effects not only on satellites in space but also to humans on Earth. One of the key components determining the condition of space weather is the solar wind, which is an outward flow of charged particles released from the Sun. Typically, the solar wind can be classified into two types by speed: slow solar wind ( $v \leqslant 400$ $\mathrm{km} \mathrm{s}^{-1}$ ) and fast solar wind ( $v \geqslant 650 \mathrm{~km} \mathrm{~s}^{-1}$ ) (Bravo \& Stewart 1997). The interaction between the fast solar wind and the ambient slow solar wind produces a region known as the corotating interaction region (CIR). CIR is well known to be one of drivers of interplanetary shocks. At the leading edge and the trailing edge of the CIR, forward shocks and reverse shocks are formed, respectively (Gosling \& Pizzo 1999; Pitňa et al. 2021). The solar wind streams with high pressure in the CIR mainly contribute to the geomagnetic activity via magnetic reconnection with the magnetosphere of the Earth (Richardson et al. 2000; Tsurutani et al. 2006).

Coronal holes, which are an open magnetic field region, are the source of high-speed solar wind (Nolte et al. 1976; Zirker 1977; Hassler et al. 1999), and it is known that there is a relationship between coronal holes, CIRs, and geomagnetic disturbances (Vršnak et al. 2007a, 2007b; Choi et al. 2009). de Toma (2011) investigated the evolution of the coronal holes and their implications for high-speed solar wind between 2006 and 2009, and found that large and low-latitude coronal holes

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
are important sources of recurrent high-speed solar wind streams. Therefore, it is essential to forecast the solar wind speed by investigating the relationship between solar phenomena and solar wind.

There have been a lot of models to predict the solar wind speed using various methods such as physics-based models (Odstrcil 2003; Reiss et al. 2020; Elliott et al. 2022), empirical models (Owens et al. 2013; Bussy-Virat \& Ridley 2014; Rotter et al. 2015), and artificial intelligence-based models (Poggi et al. 2003; Liu et al. 2011; Yang et al. 2018). The Wang-Sheely-Arge (WSA) model (Wang \& Sheeley 1990, 1992; Arge \& Pizzo 2000) can predict the background solar wind speed at the outer boundary of the coronal magnetic field, employing a potential field source surface model (Schatten et al. 1969; Altschuler \& Newkirk 1969) and the Schatten current sheet model (Schatten 1971) of the steady-state corona. The WSA model is coupled with the ENLIL model (Odstrcil 2003), which enables simulations of solar wind disturbances using MHD approximations from the solar corona up to 1 au and beyond in the inner heliosphere. The WSACNLIL model has been operated in the Space Weather Prediction Center of the National Oceanic and Atmospheric Administration.

Recently, deep learning has been applied to various fields of space weather, including solar wind speed prediction. For example, Upendran et al. (2020) predicted solar wind speed of the next day using extreme ultraviolet (EUV) images and visualized how the model used data to make predictions. Sun et al. (2021) proposed a model based on the two-dimensional attention mechanism for forecasting the solar wind speed 24 hr ahead. Raju \& Das (2021) used a convolutional neural network-based model for daily solar wind speed prediction, introducing an adaptive time-delay method.
![img-0.jpeg](img-0.jpeg)

Figure 1. Architecture of our deep-learning model. It consists of two input layers, to which the solar images (Input I) and solar wind speed data (Input II) are respectively entered. The images pass several convolution networks and one LSTM layer, and the solar wind speed data flow into two dense layers. They are concatenated and pass one dense layer at the end of the network, and then the targets (sequential solar wind speed values) are predicted.

In this study, we propose a deep-learning model to predict the solar wind speed at Earth for the next 3 days with a 6 hr cadence. In contrast to previous studies that make predictions of solar wind speeds at a single time point, our model has the capability to sequentially predict them with a higher time resolution. We use solar EUV images, which can provide the characteristics of solar coronal holes, and solar wind speeds themselves as input data. The deep-learning model consists of two networks, including a convolutional layer-based network for images and a dense layer-based network for solar wind speeds. This paper is organized as follows. In Section 2 and Section 3, the data and method used in this study are described, respectively. In Section 4, we show the results of our model and discuss them. Finally, in Section 5, we summarize this study.

## 2. Data

For this work, we obtain the hourly averaged solar wind speed data from OMNIWeb<sup>3</sup> of the Space Physics Data Facility (King & Papitashvili 2005). And we use solar EUV images observed in 211 and 193 Å from the Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) indicating the characteristics of coronal holes. We make 6 hr averaged solar wind speed data and select the EUV images observed at the closest time of each solar wind speed data (00, 06, 12, and 18 UT). We use the data from 2010 May to 2020 December (total 15,203 data pairs), excluding cases where there is no observation or poor image quality (quality ≠ 0 in fits header). All EUV images are preprocessed by rotating, centering, exposure compensation, and degradation compensation using the SolarSoft library. Then the images are resized to 64 × 64 from the original size (4k × 4k) to optimize memory usage and converted into PNG files. We use 64 × 64 size images because we think that it would not be very necessary for the deep-learning model to capture all the complex shapes of coronal holes. Only their location and size information would be enough for solar wind prediction task. In fact, we have tried to train the model with images of 128 × 128 size; the performance of the model is almost same.

The input data of the previous 5 days from the prediction time are used as input data of the model by considering the traveling time of solar winds from the Sun to the Earth. It is determined by several trials changing the input length from 3 to 6 days. The solar wind speed data are entered with a 6 hr cadence, and solar images are done with a 12 hr cadence. Target data are the 3 day ahead solar wind speed with a 6 hr cadence. The total period of the data is from 2010 May to 2020 December, which covers one solar cycle. Considering the solar cycle effect, we divide the data set as follows: all January–August data are used for the training set (9992), all September data for the validation set (1321), and all October–December data for the test set (3890). We check each probability distribution of the training and test sets. It seems that the training data and test data have almost the same distributions, and we can say that there are few biases in our data set. And we carefully determined the periods of the training and test data sets. If the training and test sets are randomly divided, the dates of the training data and the test data may overlap too much. In that case, the input data are almost the same for both data sets so that the model results would be quite successful. Therefore, we think the way we divide the data set is the most appropriate to train the model. The data set we used in this study covers only solar cycle 24, which is a rather weak cycle, and our model may be biased toward the tendency of this period. When our model is applied in solar cycle 25, some additional training data may be used to improve the model.

## 3. Method

Since we use both the solar images and solar wind speed values as input data, we need two networks to handle different types of data. Figure 1 shows the architecture of our deep-learning model. For the network of the image data, we use the Inception block in GoogLenet (Szegedy et al. 2014) for our model. The Inception block was used and its performance was verified by Upendran et al. (2020) for the solar wind speed prediction task. The key advantage of the Inception block is using 1 × 1 convolutional layers. With this, the amount of computation can be reduced effectively. As a result,

<sup>3</sup> https://omniweb.gsfc.nasa.gov/
![img-1.jpeg](img-1.jpeg)

**Figure 2.** CC and RMSE of our model for given prediction times. The left axis is CC (black) and the right axis is RMSE (red).

Convolutional layers can be stacked deeper with fewer memories. In addition, the model can extract visual information in multiscale because different kernel sizes are used in parallel on the same input. The structure of the Inception block is shown in the blue box of Figure 1. The solar images pass through one three-dimensional convolutional layer followed by three Inception blocks, and one long short-term memory (LSTM; Hochreiter & Schmidhuber 1997). LSTM is a well-known layer to be effective for sequential data. The other network for solar wind speed data consists of two dense layers. At the end, the two networks are concatenated, and pass one dense layer, and then the model finally predicts the sequential solar wind speeds. Every convolutional layer is followed by a max pooling layer. Rectified linear unit (Nair & Hinton 2010) activation is used at the convolutional layers and LSTM layer, and scaled exponential linear unit (Klambauer et al. 2017) activation is used at the dense layers. We use mean square error (MSE) as a loss function for training, and an Adam optimizer (Kingma & Ba 2014) with a learning rate of 0.0001 to optimize the model.

### 4. Results and Discussion

We evaluate the performance of our model in view of the correlation coefficient (CC) and rms error (RMSE) between real values and predicted values by our model for given prediction times: 6, 12, 18, 24, ..., 72 hr. The CC and RMSE are given by

$$
\begin{aligned}
\text{CC} &= \frac{\sum_{i=1}^{N}(X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^{N}(X_i - \bar{X})^2} \sqrt{\sum_{i=1}^{N}(Y_i - \bar{Y})^2}}, \\
&\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{N}(X_i - Y_i)^2}{N}}
\end{aligned}
\tag{1}
$$

respectively. $X_i$, $Y_i$, $\bar{X}$, $\bar{Y}$, and $N$ denote $i$th predicted values, $i$th real values, their average values, and the total number of data, respectively. The RMSE shows overall differences between the observation values and the predicted values by the model, and the CC shows how well the model captures the pattern of the solar wind speeds. The lower the RMSE and the closer CC is to 1, the better the performance. As shown in Figure 2, the CC of our model is from 0.92 (for the 6 hr prediction) to 0.67 (for the 72 hr prediction), and the RMSE is from 37.4 to 68.2 km s−1. Table 1 shows comparison of the results of the 24 hr prediction by our model with those of other solar wind prediction models. Even though they cannot be compared directly to the results due to the difference of target time resolution and length, we can say that our model successfully predicts solar wind speeds in view of the metrics.

Figure 3 shows the RMSE and CC of our model for each year (2011–2020). During the solar maximum phase (2013–2014), the performance is not so good compared to other years, probably because of many coronal mass ejections (CMEs). After the maximum phase since 2015, the CCs are greater than 0.85 and RMSEs in speed are smaller than 70 km s−1, demonstrating that our prediction is quite successful.

Figure 4 shows good examples of the results of our model when the solar wind speeds increase, and Figure 5 shows when they decrease. These examples show that our model can predict both increase and decrease of solar wind speeds. Especially, our model successfully predicts not only the values, but also the timing of the rising of solar wind speeds. We can find the reason for the good prediction shown in Figure 4 from the input data, i.e., solar EUV images. Figures 6(a) and (b) shows the input images (193 and 211 Å images) of events in Figures 4(a) and (b), respectively. These two examples have in common that there are noticeable equatorial coronal holes appearing from the east limb and moving to the west limb. This fact implies that the model should successfully learn that the sources of fast solar winds are coronal holes (Nolte et al. 1976; Zirker 1977; Hassler et al. 1999).

We compare solar wind speeds predicted by our model with those by the WSA-ENLIL model. The WSA-ENLIL model produces simulation results of the solar wind using a radial magnetic field from a synoptic magnetogram and with coronal
![img-2.jpeg](img-2.jpeg)

**Figure 3.** CC and RMSE of our model for each year (2011–2020). The left axis is CC (black) and the right axis is RMSE (red).

![img-3.jpeg](img-3.jpeg)

**Figure 4.** Two good examples of results from our model when there is the enhancement of solar wind speeds caused by coronal holes. The gray line means observation data, and the orange one means prediction values by our model. The red dashed line is a forecast starting time.

Base plasma temperature and density as input data. Figure 7 shows the results during Carrington rotation (CR) 2209, which includes the period in Figure 4(a). Since we compare solar wind speeds at 1 au where it takes 3–4 days for the solar wind to arrive at the Earth, we show a series of 72 hr predictions with a 6 hr cadence by our model during the same period. Here the result of the WSA-ENLIL model is obtained from CORHELMAS_WSA_ENLIL by requesting it from the Community Coordinated Modeling Center (CCMC) at the Goddard Space Flight Center (GSFC). For this we use GONG data for input magnetic fields and all other optional parameters are default values. For comparison the one-run result of the WSA-ENLIL
![img-4.jpeg](img-4.jpeg)

Figure 5. Two good examples of results from our model when the solar wind speeds decrease. The gray line shows observation data, and the orange one shows prediction values by our model. The red dashed line is a forecast starting time.

model is given for the CR 2209, starting from 2018 September 29. It is noted that the results of the WSA-ENLIL model can be significantly improved by changing the input synoptic maps and parameters (Sachdeva et al. 2019; Odstrcil et al. 2020). In Figure 7, two regions of fast solar winds faster than 600 km s<sup>-1</sup> are shown in this period. At least we can say that our model can predict well both fast solar wind streams with high accuracy, when compared to the WSA-ENLIL model. More detailed comparisons should be given in the future. In addition, Owens et al. (2008) calculated performance of the WSA-ENLIL model with a 1 hr cadence over an 8 yr period (1995–2001) at L1 point. Their RMSE is 97.2 km s<sup>-1</sup> during that period, and the RMSE of our model for 72 hr prediction results is 68.2 km s<sup>-1</sup> during our test period (every October-December in 2010–2020). We may say that our model is comparable to or even better than the WSA-ENLIL model in view of near-Earth solar wind speed prediction.

During the period of the test data, we identify a total of 36 high-speed streams according to the criteria by Jian et al. (2015). Our model can capture the increase of solar wind speed for 25 of these events (about 70%). There are 12 interplanetary CMEs (ICMEs) resulting in increasing solar wind speeds, and our model cannot predict all of them. Figure 8 shows an example of poor prediction of our model. The model cannot catch the sudden increase of solar wind speed, because there are no significant coronal holes in the input images in this case. We find that the reason for this enhancement originates from coronal mass ejections (CMEs). According to the Coordinated Data Analysis Web (CDAW) CME online catalog, a halo CME occurred at 2013 September 29, 22:12 UT with a speed of 1179 km s<sup>-1</sup>. It arrived at the Earth at 2013 October 2 according to the Richardson–Cane ICME list (Richardson et al. 2000; Richardson & Cane 2010). This coincides with the time point at which the solar wind speed increases, so the ICME can be thought of as the reason for this. Our model cannot take the information about the CMEs from input data. To improve the performance of the model in this point, we try to modify the model with additional input data including CME information. We simply consider features of halo CMEs such as angular width, source regions, and speeds as input data with a 6 hr cadence. However, the performance of the model is not significantly different. We need to analyze the relationship between CMEs and solar wind speeds more deeply for a proper model design. We leave this work for a future study.

## 5. Summary

In this study, we make a solar wind speed prediction model up to 72 hr with 6 hr intervals using a deep-learning model. We use SDO/AIA 211 and 193 Å images and solar wind speeds themselves as input data for the model. The deep-learning model consists of two networks each for images and solar wind
![img-5.jpeg](img-5.jpeg)

Figure 6. Panel (a) shows examples of input images for Figure 4(a), and panel (b) shows the same for Figure 4(b). Top images of each panel are 193 Å and bottom ones are 211 Å images. The equatorial coronal holes, the source of high-speed streams, are well shown in these input images.

Comparison with WSA-ENLIL model (CR=2209)

![img-6.jpeg](img-6.jpeg)

Figure 7. Comparison of the results of the 72 hr prediction by our model (red line) with those of the WSA-ENLIL model (green line) in CR 2209 where the period of Figure 4(a) is included. The gray line indicates the observed solar wind speeds.

Data. The results of the study are as follows. The CC of our model is from 0.92 (for the 6 hr prediction) to 0.67 (for the 72 hr prediction), and the RMSE is from 37.4 to 68.2 km s⁻¹. These values are better than those of the previous models. The model successfully predicts solar wind speed increases such as corotating interaction regions, which are caused by equatorial coronal holes. The solar wind speed prediction results from our model are compared to those from the WSA-ENLIL model, and the results of our model show more consistency with near-Earth observations at CR 2209 than those of the WSA-ENLIL model. However, our model cannot predict solar wind speed enhancement caused by CMEs. For this, a deeper
![img-7.jpeg](img-7.jpeg)

Figure 8. An example of poor prediction by our model. The gray line shows observation data, and the orange one shows prediction values by our model. The red dashed line is a forecast start time. The black solid line indicates a halo CME occurred at 2013 September 29.

Understanding of the relationship between solar wind speed and CMEs will be needed. We leave this for a future work. We have proposed a deep-learning method using both image data and solar wind values in sequence simultaneously for the solar wind speed prediction, unlike the previous studies. Our model shows successful performance, which implies that our deep-learning method should be helpful for similar types of space weather forecasting.

Our study has a high potential for application in space weather forecasting. First, our study could be used for near-real-time forecasting. We think that there are two kinds of approaches for near-real-time applications. One approach is to make the data from OMNIWeb in near real time. The other approach is to make another deep-learning model using satellite observations directly, like from the Deep Space Climate Observatory. In addition, it is necessary to include how to handle missing or abnormal data. Our model also can be applied to the prediction at a point other than the Earth. It is possible with the same input data set if there are observational data of solar wind speeds at other stationary points (e.g., L5/L4 missions) with respect to the Earth. Including data from an L5 point (such as the Vigil mission) in our input data set may be helpful. Since coronal holes look different depending on the angle of view, the model will be able to get more detailed information on them.

### Acknowledgments

This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2021R1I1A1A01049615 and RS-2023-00248916), the Korea Astronomy and Space Science Institute under the R&D program (Project No. 2023-1-850-07) supervised by the Ministry of Science and ICT (MSIT), Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning), and National Meteorological Satellite Center (NMSC)/ Korea Meteorological Administration (KMA) (No. 33233031800). We acknowledge use of NASA/GSFC's Space Physics Data Facility's OMNIWeb (or CDAWeb or ftp) service, and OMNI data. We appreciate the GSFC/CCMC for providing a publicly available simulation service (https://ccmc.gsfc.nasa.gov). The CORHEL-MAS_WSA_ENLIL model was developed by the Jon Linker at Predictive Science. We acknowledge numerous team members who have contributed to the success of the SDO mission. We thank the community efforts devoted to developing of the open-source packages that were used in this work.

### ORCID iDs

- Jihyeon Son https://orcid.org/0000-0003-2678-5718
- Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
- Harim Lee https://orcid.org/0000-0002-9300-8073
- Hyun-Jin Jeong https://orcid.org/0000-0003-4616-947X

### References

- Altschuler, M. D., & Newkirk, G. 1969, *SoPh*, 9, 131
- Arge, C. N., & Pizzo, V. J. 2000, *JGR*, 105, 10465
- Bravo, S., & Stewart, G. A. 1997, *ApJ*, 489, 992
- Bussy-Virat, C. D., & Ridley, A. J. 2014, *SpWea*, 12, 337
- Choi, Y., Moon, Y. J., Choi, S., et al. 2009, *SoPh*, 254, 311
- de Toma, G. 2011, *SoPh*, 274, 195
- Elliott, H. A., Arge, C. N., Henney, C. J., et al. 2022, *SpWea*, 20, e2021SW002868
- Gosling, J. T., & Pizzo, V. J. 1999, in *Corotating Interaction Regions*, ed. A. Balogh et al. (Dordrecht: Springer Netherlands), 21
- Hassler, D. M., Dammasch, I. E., Lemaire, P., et al. 1999, *Sci*, 283, 810
- Hochreiter, S., & Schmidhuber, J. 1997, *Neural Comput.*, 9, 1735
- Jian, L. K., MacNeice, P. J., Taktakishvili, A., et al. 2015, *SpWea*, 13, 316
- King, J., & Papitashvili, N. 2005, *JGRA*, 110, A02104
- Kingma, D. P., & Ba, J. 2014, arXiv:1412.6980
- Kambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. 2017, *Advances in Neural Information Processing Systems 30*, ed. I. Guyon et al. (Red Hook, NY: Curran Associates) https://papers.nips.cc/paper_files/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html
- Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, *SoPh*, 275, 17
- Liu, D. D., Huang, C., Lu, J. Y., & Wang, J. S. 2011, *MNRAS*, 413, 2877
- Nair, V., & Hinton, G. E. 2010, *Proc. 27th Int. Conf. on Machine Learning (Madison, WI: Omnipress), 807*, https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf
- Nolte, J. T., Krieger, A. S., Timothy, A. F., et al. 1976, *SoPh*, 46, 303
- Odstrcil, D. 2003, *AdSpR*, 32, 497
- Odstrcil, D., Mays, M. L., Hess, P., et al. 2020, *ApJS*, 246, 73
- Owens, M. J., Challen, R., Methven, J., Henley, E., & Jackson, D. R. 2013, *SpWea*, 11, 225
- Owens, M. J., Spence, H. E., McGregor, S., et al. 2008, *SpWea*, 6, S08001
- Pesnell, W. D., Thompson, B. J., & Chamberlin, P. C. 2012, *SoPh*, 275, 3
Pitňa, A., Šafránková, J., Němeček, Z., Ďurovcová, T., \& Kis, A. 2021, FrP, 8, 654
Poggi, P., Muselli, M., Notton, G., Cristofari, C., \& Louche, A. 2003, ECM, 44, 3177
Raju, H., \& Das, S. 2021, SoPh, 296, 134
Reiss, M. A., MacNeice, P. J., Muglach, K., et al. 2020, ApJ, 891, 165
Richardson, I. G., \& Cane, H. V. 2010, SoPh, 264, 189
Richardson, I. G., Cliver, E. W., \& Cane, H. V. 2000, JGR, 105, 18203
Rotter, T., Veronig, A., Temmer, M., \& Vršnak, B. 2015, SoPh, 290, 1355
Sachdeva, N., van der Holst, B., Manchester, W. B., et al. 2019, ApJ, 887, 83
Schatten, K. H. 1971, CosEl, 2, 232
Schatten, K. H., Wilcox, J. M., \& Ness, N. F. 1969, SoPh, 6, 442

Sun, Y., Xie, Z., Chen, Y., Huang, X., \& Hu, Q. 2021, SpWea, 19, e2020SW002707
Szegedy, C., Liu, W., Jia, Y., et al. 2014, arXiv:1409.4842
Tsurutani, B. T., Gonzalez, W. D., Gonzalez, A. L. C., et al. 2006, JGRA, 111, A07S01
Upendran, V., Cheung, M. C., Hanasoge, S., \& Krishnamurthi, G. 2020, SpWea, 18, e2020SW002478
Vršnak, B., Temmer, M., \& Veronig, A. M. 2007a, SoPh, 240, 315
Vršnak, B., Temmer, M., \& Veronig, A. M. 2007b, SoPh, 240, 331
Wang, Y. M., \& Sheeley, N. R. J. 1990, ApJ, 355, 726
Wang, Y. M., \& Sheeley, N. R. J. 1992, ApJ, 392, 310
Yang, Y., Shen, F., Yang, Z., \& Feng, X. 2018, SpWea, 16, 1227
Zirker, J. B. 1977, RvGSP, 15, 257"
Gyungin Shin et al 2020 - Generation of High-resolution Solar Pseudo-magnetograms from Ca ii K Images by Deep Learning.pdf,"# Generation of High-resolution Solar Pseudo-magnetograms from Ca II K Images by Deep Learning 

Gyungin Shin ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1}$ (D), Eunsu Park ${ }^{2}$ (D), Hyun-Jin Jeong ${ }^{1}$ (D), Harim Lee ${ }^{2}$ (D), and Sung-Ho Bae ${ }^{3}$<br>${ }^{1}$ School of Space Research, Kyung Hee University Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{3}$ Department of Computer Science and Engineering, College of Software, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2020 January 29; revised 2020 April 11; accepted 2020 May 2; published 2020 May 21


#### Abstract

In this Letter, we generate realistic high-resolution ( $1024 \times 1024$ pixels) pseudo-magnetograms from Ca II K images using a deep learning model based on conditional generative adversarial networks. For this, we consider a model ""pix2pixHD"" that is specifically devised for high-resolution image translation tasks. We use Ca II K 393.3 nm images from the Precision Solar Photometric Telescope at the Rome Observatory and line-of-sight magnetograms from the Helioseismic and Magnetic Imager (HMI) at the Solar Dynamics Observatory from 2011 January to 2015 June. 2465 pairs of Ca II K and HMI are used for training except for January and July data. The remaining 436 pairs are used for an evaluation of the model. Our model shows that the mean correlation coefficient (CC) of total unsigned magnetic flux between AI-generated and real ones is 0.99 and the mean pixel-to-pixel CC after $8 \times 8$ binning over the full disk is 0.74 . We find that the AI-generated absolute magnetic flux densities are highly consistent with real ones, even to the fine scale structures of quiet regions. On the other hand, the mean pixel-to-pixel correlations of magnetic flux densities strongly depend on a region of interest: 0.81 for active regions and 0.24 for quiet regions. Our results suggest a sufficient possibility that we can produce high-resolution solar magnetograms from historical Ca II data.


Unified Astronomy Thesaurus concepts: Solar physics (1476); The Sun (1693); Solar atmosphere (1477); Solar magnetic fields (1503); Astronomy data analysis (1858)

## 1. Introduction

Magnetic fields of the Sun are a fundamental component for its dynamic activities from small scales such as granules, to large scales such as flares and coronal mass ejections. To understand such solar activities, routine ground-based observations of full disk solar magnetic fields have been conducted since 1974 at the Kitt Peak Observatory (Livingston et al. 1976). The SOlar and Heliospheric Observatory (Domingo et al. 1995)/Michelson Doppler Imager (Scherrer et al. 1995) spacecraft started to offer line-of-sight full disk magnetograms of good quality in 1996 and the Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Helioseismic and Magnetic Imager (HMI; Scherrer et al. 2012; Schou et al. 2012) has offered better quality magnetograms since 2010. In fact, we have little information about how solar magnetic fields had evolved before 1974. A few attempts have been made to reconstruct past magnetic fields from Ca II full disk filtergrams, which is a well-known proxy for magnetic fields (Babcock \& Babcock 1955). Pevtsov et al. (2016) reconstructed synoptic pseudo-magnetograms with Ca II 854.2 nm spectral line full disk images from the Vector Stokes Magnetograph of the Synoptic Optical Long-term Investigation of the Sun (Balasubramaniam \& Pevtsov 2011) and sunspot magnetic field strength data from the Mount Wilson Observatory (Hale et al. 1919).

Convolutional Neural Networks (CNNs; LeCun \& Bengio 1998) are one specific type of artificial intelligence to mediate drawbacks of formerly used multilayer perceptrons. CNNs are composed of several convolutional filters that are supposed to mimic a human visual system so that a CNN model can distinguish important parts of an input image from the rest. Due to its high performance in handling big data,

CNNs have been broadly adopted as a research tool across various scientific fields. On the other hand, Generative Adversarial Networks (GANs; Goodfellow et al. 2014) are a type of generative model in neural networks. GANs consist of two major components: a generator and a discriminator that are both multilayer perceptrons. Given 1D random values, the generator tries to produce a realistic image that is regarded as real data. On the contrary, the discriminator attempts to distinguish generated images from images in the real data set. In fact, this game between the two reaches at a point where the discriminator cannot tell apart the generated images and the real ones. To accommodate a better performance of GANs, Mirza \& Osindero (2014) suggested inputting 1D values related to an output image that act as a condition, which is called conditional GAN (cGAN). Going a step further, Isola et al. (2016) proposed inputting an image in a generator instead of relevant 1D values and called their model ""pix2pix."" This pix2pix model translates an input image in one domain to the other image in a different domain that shares the same structure with the input data but has a different style. For instance, the pix2pix model can be trained to generate a color image from a black/ white image. Because of its generality and moderate performance, the pix2pix model has been successfully used for several applications in the field of astronomy (Kim et al. 2019; Park et al. 2019). On the other hand, it has been well noted that the pix2pix model produces apparent artifacts and fails to capture fine details from time to time for high-resolution image translation tasks, e.g., larger than $1024 \times 1024$ pixels. Wang et al. (2017) proposed networks that resolve this issue and named them ""pix2pixHD."" Unlike a pix2pix model, it is empirically proven that a pix2pixHD model can manufacture even small features almost without artifacts for high-resolution data sets.
In this Letter, we reconstruct HMI-like high-resolution pseudo-magnetograms with Ca II 393.3 nm spectral line full disk intensity images by training a deep learning model called ""pix2pixHD."" For this, we use Ca II K images from the Precision Solar Photometric Telescope at the Rome Observatory (Rome/PSPT, Ermolli et al. 1998, 2007) and line-of-sight magnetograms from SDO/HMI. We train the model to generate high-quality magnetograms without loss of physical information during translation from Ca II intensity images to HMI-like magnetograms. We explain data sets in Section 2. Details in our deep learning method are delivered in Section 3. Results and discussion are present in Section 4. Finally, a brief summary and conclusion are given in Section 5.

## 2. Data

We use Ca II K 393.3 nm spectroheliograms for input data and line-of-sight magnetograms for target data. As mentioned in Section 1, this is due to high correspondences between bright features of Ca II K images and magnetic features on magnetograms (Leighton 1959; Chatzistergos et al. 2019a). For training, we use 2465 Ca II K image/magnetogram pairs from 2011 January to 2015 June except for every January and July. For evaluating our model, the remaining 436 pairs are used. To facilitate a training and test process with our limited computational resources, we set the size of all data as a 1024 by 1024 pixel.

### 2.1. Ca II 393.3 nm Full Disk Filtergram

We use Ca II 393.3 nm full disk intensity images that were obtained from the Rome/PSPT. We align Ca II K Images with SDO/HMI ones. In order to select relatively clear Ca II K images, we make Ca II contrast images by computing a local absolute maximum gradient within a small disk at every pixel ${ }^{4}$ and discard those with mean contrast values lower than their average. Images containing obvious artifacts, e.g., deformed shape of the Sun or thick black line across the whole image, are manually removed.

### 2.2. HMI Magnetogram

We use line-of-sight magnetograms that were taken from SDO/HMI. Preprocessing contains aligning rotational axis to north up, locating solar center at the center of an image, and cleaning up. Magnetic flux densities are described within $\pm 1400 \mathrm{G}$. We adopt HMI data, which were observed within $\pm 12$ minutes from observation time of each Ca II image, in order to minimize the effect of solar rotation. It is noted that 12 minutes correspond to one pixel movement by solar rotation at the disk center.

## 3. Method

As our purpose is to translate high-resolution ( $1024 \times 1024$ pixels) Ca II K intensity images to the corresponding magnetograms, we adopt a deep learning model called ""pix2pixHD"" based on cGAN, which are widely acknowledged for its great performance in high-resolution image translation tasks. We train our model for 200 epochs as in Wang et al. (2017), which correspond to about 490,000 iterations.

[^0]""Iteration"" means the number of showing a Ca II/magnetogram pair to the model for training. An epoch refers to the count of showing the whole training data pairs, i.e., 2465 pairs. After training, we evaluate the trained model by comparing generated magnetograms and the corresponding SDO/HMI magnetograms. All the processes of training and testing our model are implemented on PyTorch (Paszke et al. 2019) developed by Facebook's Artificial Intelligence Research group, one of the popular deep learning frameworks. The official code used for the model, training, and test processes is publicly available at https://github.com/NoelShin/Ca2Mag. In the following, we describe the model architecture of pix2pixHD (see Figure 1) mainly in comparison with that of pix2pix. For more implementation details, please refer to Appendices B and C.
pix2pixHD is composed of a generator and two discriminators A and B. The generator is a CNN whose objective is to produce realistic magnetograms, which the discriminators fail to distinguish from corresponding real SDO/HMI magnetograms. On the other hand, given a Ca II/generated HMI pair or a Ca II real HMI pair, the objective of the discriminators is to find whether or not the pair contains real HMI data. As an optimization of the generator relies on performances of the two discriminators, it is important to make sure that the discriminators are good enough to distinguish (Arjovsky et al. 2017). In pix2pix, it introduces a ""patch discriminator"" whose receptive fields ${ }^{5}$ of a pixel in its final output correspond to a certain patch size of an input image. Isola et al. (2016) showed that a discriminator with a different size of a receptive field has divergent outcomes and that a discriminator with a $70 \times 70$ receptive field generally performs satisfactorily. Inspired by this, Wang et al. (2017) proposed the use of several $70 \times 70$ patch discriminators and to give input pairs of different spatial sizes to the discriminators. A discriminator which gets a smaller (larger) input pairs has larger (smaller) a receptive field size. In our model, we use two $70 \times 70$ patch discriminators A and B. For the discriminator A, input pairs of the original 1024 by 1024 pixel size are given. For the discriminator B, input pairs that are downsampled by half are given, which makes its effective receptive field size $141 \times 141$. This setup encourages the generator to produce realistic magnetograms in two different view sizes. For more detailed information about objectives of the generator and discriminators, please refer to Appendix A.

## 4. Results and Discussions

Figure 2 shows a comparison between a generated HMI-like magnetogram from a Ca II K intensity image and an SDO/HMI magnetogram observed at 18:48 UT on 2012 January 31. The magnetic field polarities of the generated HMI-like magnetogram are consistent with the ones in the corresponding magnetogram, even to small-scale magnetic features. However, in the case of complex magnetic fields near the polarity inversion line, the model struggles to assign proper polarities to active regions.

To investigate whether generated magnetograms reconstruct physical meanings as well, we derived ""Magnetic Range of Influence"" (MRoI; McIntosh et al. 2006, 2007) from generated and SDO/HMI magnetograms. MRoI is a simple means for

[^1]
[^0]:    4 We use the scikit-image package (van der Walt et al. 2014) which is a Python image processing library for making the contrast maps. We set a radius of the disk as 2 .

[^1]:    5 A receptive field refers to a size that each pixel of the final output from a convolutional neural network perceives on the input image. For example, if a receptive field of a network is $70 \times 70$, it means that each pixel in the final output from the network considers a field of $70 \times 70$ size on its input image.
![img-0.jpeg](img-0.jpeg)

Figure 1. Overview of pix2pixHD. A Ca II K intensity image of 1024 × 1024 pixels is given to the generator and it produces a corresponding HMI-like magnetogram. The generated magnetogram is paired with the input Ca II K image and given to the discriminators A and B. Also, another pair of the same Ca II K image and the corresponding SDO/HMI is given to the discriminators A and B. When they are given to discriminator B, the size of the pairs is downsampled by half so that it can be determined with a different receptive field. Then, the discriminators strive to distinguish which pair contains the real HMI image. Their output values are close to 1 if they suppose that the input pair contains the real HMI image and 0 for the opposite case. These results affect the generator such that it can produce a more realistic HMI-like magnetogram afterward.

Understanding a magnetic environment on the photosphere, delivering the radial distance required to balance magnetic flux densities on each pixel in a magnetogram (Leamon & McIntosh 2009). This is computed by increasing a radius of a circle by one pixel at a time until the contained fluxes are balanced. For a large MRoI, the magnetic field is interpreted to be unbalanced, and thus to be ""open."" We plot MRoI in Figure 3. The MRoI derived from the generated HMI is consistent with the one from SDO/HMI, describing that the model successfully reconstructs the physical meanings. Mean Pearson correlation coefficient between the MRoI from generated magnetograms and from the real ones is 0.82 after 8 by 8 binning.

To visualize how accurate absolute magnetic flux density distributions are alike, we plot three absolute magnetic flux maps: baseline, one from generated HMI, and one from SDO/HMI along with a difference map between ones from the generated and SDO/HMI magnetogram as shown in Figure 4. For baseline, we made a simple pixel to pixel calibration curve based on our training data set. We gathered pairs of Ca II K pixel intensity and unsigned magnetic flux in pixel level on the solar disk. We took a median value where there are multiple magnetic flux values for the same Ca II K intensity such that each intensity value has only one corresponding absolute magnetic flux density value.

As can be seen in the figure, predictions are highly reliable for quiet regions as well as active regions. This is because bright regions on Ca II K intensity images agree with magnetic features on magnetograms (Leighton 1959; Chatzistergos et al. 2019a).

To examine how accurately magnetograms are reconstructed, we derive four different metrics: total unsigned magnetic fluxes (TUMF), pixel-to-pixel Pearson correlation coefficient (CC), relative error (R), and structural similarity index (SSIM; Wang et al. 2004) by comparing real SDO/HMI magnetograms and the corresponding generated magnetograms. The equation of R is given by

$$R_i = (\Phi_i^{\text{generated}} - \Phi_i^{\text{real}}) / \Phi_i^{\text{real}}, \tag{1}$$

where $\Phi_i$ denotes total unsigned magnetic flux and the serial number of 436 test data. The equation of SSIM is given by

$$\text{SSIM}_i = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}, \tag{2}$$

where x, y, $\mu_x$, $\mu_y$, $\sigma_x^2$, $\sigma_y^2$, $\sigma_{xy}$, i represent two different data, the average of x and y, the variance of x and y and the covariance of x and y, and the serial number, respectively. $c_1$ and $c_2$ are two constants for stabilizing the division. Here, we set $c_1$ and $c_2$ to 6.50 and 58.52 as default. SSIM produces a value between 0 and 1. A value close to 1 means that the two data share common structures and 0 means the opposite case.

For estimation we count pixels over +10 G for positive flux and under -10 G for negative flux on the solar disk by noting that the noise level of HMI magnetograms is about 10 G (Liu et al. 2012). We compute the metrics for 436 full disk
![img-1.jpeg](img-1.jpeg)

Figure 2. (Upper left) a Ca II K input intensity image, (upper middle) a generated HMI-like magnetogram, and (upper right) the corresponding real SDO/HMI. Ca II K and real SDO/HMI are taken at 18:48 UT 2012 January 31. (Lower left column) enlarged views of active regions in the generated magnetogram. (Lower right column) those in the real magnetogram. The white area denotes positive polarities and the black area denotes negative ones. For visual purpose, magnetic flux densities are expressed within ±100 G.

Magnetograms, 510 active regions with 128 × 128 pixel size, and 436 quiet regions. In addition, we compute the pixel-to-pixel CC and the relative error of absolute magnetic flux density maps. When computing CC, we calculated CC from each pair of a generated and a real magnetogram and then average them. The results are shown in Table 1, which also includes the results of Kim et al. (2019) for comparison. In their work, they translated 304 Å images taken by the Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) at SDO to the corresponding SDO/HMI magnetograms based on the pix2pix model.

As can be seen in Table 1, CC values of TUMF are 0.99 for both full disk and active regions, and 0.95 for quiet regions. These CC values show that our model is able to produce reliable TUMF values.

Pixel-to-pixel CC values after 8 × 8 binning show 0.74, 0.81, and 0.24 for full disk, active, and quiet regions, respectively. The values with other binning size are given in Table 1. These values demonstrate that our model cannot only well produce shapes of magnetic features but also assign polarities, at least for active regions. In fact, the magnetic flux densities with polarities for quite regions are not successful. For more discussions of the results for active regions and quite regions, please refer to Kim et al. (2019). In the case of absolute flux density maps, CC values show 0.93, 0.94, and 0.81, respectively. These values strongly support that the generated absolute magnetic flux densities are highly consistent with SDO/HMI ones, even to the magnetic features in weak field regions. As for the relative error, its mean values are -0.085, -0.040, and -0.054 for full disk, active regions, and quiet regions, and its standard deviations are 0.032, 0.064, and 0.069, respectively. This implies that the generator slightly underestimates TUMF values more than the real ones but not significantly. For SSIM, its average values are 0.98, 0.80, and 0.97 for full disk, active, and quiet regions. This shows that the model is able to reconstruct magnetic structures in accordance with ones in SDO/HMI.

Our results of TUMF are better than those of baseline and Kim et al. (2019) for full disk and active regions, and much better for quiet regions. For pixel-to-pixel CC and the relative error, our results show better or comparable results with Kim et al. (2019). For example, the average pixel-to-pixel correlation with 8 by 8 binning (0.81) for active regions is better than that of Kim et al. (2019). These results demonstrate that it is
![img-2.jpeg](img-2.jpeg)

Figure 3. (Left) MRoI from the generated HMI and (right) MRoI from SDO/HMI at 18:48 UT on 2012 January 31. Small values present closed magnetic fields whereas large values present open magnetic fields.
useful to generate magnetograms from Ca II K images using pix2pixHD model. One impressive thing is that our model has been successfully applied to the translation from ground-based data to satellite data.

## 5. Conclusion

In this paper we have developed a deep learning model to translate from full disk Ca II K 393.3 nm intensity images to the corresponding high-resolution ( $1024 \times 1024$ ) HMI-like magnetograms. This model is called ""pix2pixHD"" which is specially devised for a high-resolution image translation task. We train the model with 2465 pairs of Ca II K images taken from PSPT in Rome and corresponding full disk line-of-sight magnetograms from SDO/HMI from 2011 January to 2015 June except for January and July. We evaluate the model with the remaining 436 pairs.

The main results of this study are summarized as follows. Our model can generate reliable HMI-like magnetograms from Ca II K images even to fine magnetic structures. The average pixel-to-pixel CC for MRoI after 8 by 8 binning is 0.82 . For TUMF CC, the generated magnetograms show higher than 0.95 for full disk, active, and quiet regions. The pixel-to-pixel CCs after $8 \times 8$ binning are $0.74,0.81$, and 0.24 for full disk, active, and quiet regions, respectively. The CCs for absolute magnetic flux density maps are $0.93,0.94$, and 0.81 , respectively. The mean relative errors are $-0.085,-0.040$, and -0.054 and their standard deviations are $0.032,0.064$, and 0.069 , respectively. The mean SSIM are $0.98,0.80$, and 0.97 , respectively. In comparison with results from Kim et al. (2019) which translated SDO/AIA $304 \AA$ images to HMI-like magnetograms using a pix2pix model, our results are similar to or even better than their results. Our preliminary experiments show that a pix2pixHD model is much better than a pix2pix model, especially for ground-based data whose quality strongly depends on atmospheric and/or instrumental conditions. For further improvements of the model in terms of reconstructing complex magnetic structures, we believe that introducing an additional loss from physical parameters such as MRoI to the discriminator can be beneficial. We leave this point to following studies.

Lastly, our study shows a sufficient feasibility to reconstruct reliable past magnetograms from historic Ca II K data. Routine observations of Ca II K started in 1904 (Kodaikanal Observatory in India, Foukal et al. 2009) and its data have been digitized (Chatzistergos et al. 2019b). By applying our model
to these data we hope that our model is able to produce pseudomagnetograms for this period. For this, cross-calibrations between past and modern Ca II data sets are essential (Chatzistergos et al. 2018). The modern magnetic field data such as Kitt Peak magnetograms started in 1974 (Livingston et al. 1976) can be used for evaluating our model. This kind of study is expected to offer more information on the long-term evolution of solar magnetic fields and their related studies such as long-term solar irradiance.

This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131, NRF-2019R1A2C1002634, NRF2019R1C1C1004778, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion(IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). We appreciate the team members who have contributed to the SDO mission and the operation at Rome/PSPT. We thank the community efforts for developing following open-software used for this work: PIL, NumPy (numpy.org), and PyTorch (pytorch.org).

## Appendix A Objective

The objective (sometimes called loss function or simply loss) of cGAN is given as

$$
\begin{aligned}
\mathcal{L}_{\mathrm{cGAN}}(G, D) & =\mathbb{E}_{(x, y)}[\log D(x, y)] \\
+ & \mathbb{E}_{x}[\log (1-D(x, G(x)))]
\end{aligned}
$$

where $G, D, x$, and $y$ denote a generator, a discriminator, input data, and target (real) data, respectively. $G(x)$ means an output from the generator for a given input image. While $G$ tries to minimize the objective, $D$ tries to maximize it. For the generator to get a lower value from the objective, it should generate an output in a way that $D(x, G(x))$ has a value close to 1 . For the discriminator to have a higher value, it should output a value close to 1 when it takes a real pair $(x, y)$ and a
![img-3.jpeg](img-3.jpeg)

Figure 4. (Left) An estimated absolute flux density map based on a simple pixel to pixel calibration curve, (mid-left) an absolute flux density map from a generated HMI-like magnetogram, (mid-right) an absolute flux density map from the real SDO/HMI taken at 18:48 UT on 2012 January 31, and (right) the difference map between the generated and SDO/HMI magnetogram. Absolute flux densities are clipped to be under 100 G for the purpose of presentation. The black area denotes where the generator predicts higher than the real value and white area for the opposite.

value close to 0 when it takes a fake pair (x, G(x)). In practice, we use a mean squared loss function instead of log function following Wang et al. (2017). It is a well-known technique that brings stability to training GAN (Mao et al. 2016; Zhu et al. 2017).

In order to facilitate better learning for the generator, we introduce an additional objective function LFM, which is called ""feature matching loss"" to the generator (Wang et al. 2017). This loss function is defined as

$$
\begin{aligned}
\mathcal{L}_{\text{FM}}(G, D) &= \mathbb{E}_{(x, y)} \sum_{i=1}^{T} \frac{1}{N_i} \\
&\times [\|D^{(i)}(x, y) - D^{(i)}(x, G(x))\|],
\end{aligned}
\tag{ A2 }
$$

where T, i, N_i denote the total number of layers in the discriminator, the serial number of the layers, and the number of pixels in output feature maps of each layer, respectively. The role of this objective is to regularize the fake pair to have more similar statistics with the real pair throughout the discriminator's intermediate layers. Our final objective is as follows:

$$
\begin{aligned}
\min_{G} & (\max_{D_{k}, D_{k_1}} \sum_{k=1,2} \mathcal{L}_{c\text{GAN}}(G, D_{k})) \\
& + \lambda \sum_{k=1,2} \mathcal{L}_{\text{FM}}(G, D_{k})))
\end{aligned}
\tag{ A3 }
$$

where k and λ denote the serial number of two discriminators and a hyperparameter which controls the importance of LcGAN and LFM. We use 10 for λ as in Wang et al. (2017).

## Appendix B Data Augmentation

In Kim et al. (2019), it was noticed that the generator struggles to properly reconstruct bipolar structures where an unusual tilt angle appears between a preceding sunspot and the following one. We see that this is due to a lack of such examples and augment data by allowing random rotation within ±30° in Cal K/magnetogram pairs in order to compensate for an insufficient number of data for the case. We find that the model trained with the rotational augmentation shows better results overall, especially in terms of pixel-to-pixel CC after 8 × 8 binning where the performance gain is around 0.04. This shows that a better result can be drawn with more number of data for rare cases and/or a proper data augmentation. Throughout this Letter, we use results from the model trained with the rotational augmentation method.

## Appendix C Implementation Details

### C.1. Training Process

The learning process of our model is made as follows (see Figure 1).

1. A Cal K image x is given to a generator and it produces an HMI-like magnetogram G(x).
2. The same Cal K image x and the generated magnetogram G(x) are combined and given to the discriminators A and B. The discriminators yield matrices D(x, G(x)) whose values are between 0 (fake) and 1 (real). Then we give a matrix, whose every element is 0 and whose size is the same with each of the discriminator's output, so that the discriminators get a loss from differences between their output D(x, G(x)) and the given matrix of 0 values. With the loss, the discriminators learn a relation between Cal images and generated magnetograms.
3. In order to make the discriminators learn a relation between Cal images and the corresponding SDO/HMI magnetograms, a Cal K image x and the target magnetogram y are combined and given to the discriminators A and B. Then we give matrices whose elements are all 1 and the discriminators are learned up to the difference between their output D(x, y) and the given matrix of 1 values.
4. The generator gets updated with losses from D(x, G(x)) and D(x, y).
5. We train the model by iterating from steps 1 to 4 about 490,000 times (about 200 epochs).

### C.2. Hyperparameters

We initialize weights in convolutional and transposed convolutional layers with a normal distribution whose mean is 0 and standard deviation is 0.02. For optimization, we use the Adam solver (Kingma & Ba 2014) with an initial learning rate of 2 × 10^-4, and set coefficients β1 and β2 as 0.5 and 0.999 for computing running averages of gradients and their squares.

### ORCID iDs

Gyungin Shin https://orcid.org/0000-0003-1793-665X
Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
Table 1
Model Evaluation with Total Unsigned Magnetic Flux CC, Pixel-to-pixel CC, and Relative Error

|  |  | Full Disk |  |  | Active Region |  |  | Quiet Region |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | 436 Images ( $1024 \times 1024)$ |  |  | 510 Images ( $128 \times 128)$ |  |  | 436 Images ( $128 \times 128)$ |  |  |
|  |  | Baseline | Ours | Kim et al. <br> (2019) | Baseline | Ours | Kim et al. (2019) | Baseline | Ours | Kim et al. (2019) |
| Total unsigned magnetic flux CC |  | $0.36^{\mathrm{a}}$ | 0.99 | 0.97 | $0.58^{\mathrm{a}}$ | 0.99 | 0.95 | $-0.047^{\mathrm{a}}$ | 0.95 | 0.74 |
| pixel-topixel CC | $1 \times 1$ binning | $(0.39)$ | $0.42(0.58)$ | $\cdots$ | $(0.40)$ | $0.57(0.66)$ | $\cdots$ | $(0.34)$ | $0.09(0.31)$ | $\cdots$ |
|  | $2 \times 2$ binning | $(0.47)$ | $0.55(0.75)$ | $\cdots$ | $(0.45)$ | $0.66(0.79)$ | $\cdots$ | $(0.49)$ | $0.15(0.53)$ | $\cdots$ |
|  | $4 \times 4$ binning | $(0.54)$ | $0.67(0.87)$ | $\cdots$ | $(0.51)$ | $0.75(0.88)$ | $\cdots$ | $(0.64)$ | $0.20(0.72)$ | $\cdots$ |
|  | $8 \times 8$ binning | $(0.62)$ | $0.74(0.93)$ | 0.77 | $(0.59)$ | $0.81(0.94)$ | 0.66 | $(0.74)$ | $0.24(0.81)$ | 0.21 |
|  | $\begin{gathered} 16 \times 16 \\ \text { binning } \end{gathered}$ | $(0.73)$ | $0.78(0.97)$ | $\cdots$ | $(0.71)$ | $0.85(0.97)$ | $\cdots$ | $(0.78)$ | $0.28(0.84)$ | $\cdots$ |
| Relative error | Mean | $-0.79^{\mathrm{a}}$ | $-0.085$ | 0.067 | $-0.72^{\mathrm{a}}$ | $-0.040$ | 0.072 | $-0.78^{\mathrm{a}}$ | $-0.054$ | 0.091 |
|  | Standard deviation | $0.23^{\mathrm{a}}$ | 0.032 | 0.036 | $0.13^{\mathrm{a}}$ | 0.064 | 0.019 | $1.10^{\mathrm{a}}$ | 0.069 | 0.075 |
| Structural similarity index |  | $\cdots$ | 0.98 | $\cdots$ | $\cdots$ | 0.80 | $\cdots$ | $\cdots$ | 0.97 | $\cdots$ |

Notes. For pixel-to-pixel CC and relative error, we also evaluate them from absolute magnetic flux density maps (written in parenthesis). Every right column in each area subsection are taken from Kim et al. (2019) for comparison.
${ }^{\mathrm{a}}$ We count pixels over +30 G for evaluating baseline as we found the simple pixel to pixel calibration curve does not properly present weak fields.

Eunsu Park (1) https://orcid.org/0000-0003-0969-286X
Hyunjin Jeong (1) https://orcid.org/0000-0003-4616-947X
Harim Lee (1) https://orcid.org/0000-0002-9300-8073

## References

Arjovsky, M., Chintala, S., \& Bottou, L. 2017, arXiv:1701.07875
Babcock, H. W., \& Babcock, H. D. 1955, ApJ, 121, 349
Balasubramaniam, K. S., \& Pevtsov, A. 2011, Proc. SPIE, 8148, 814809
Chatzistergos, T., Ermolli, I., Solanki, S. K., et al. 2019a, A\&A, 626, A114
Chatzistergos, T., Ermolli, I., Solanki, S. K., et al. 2019b, SoPh, 294, 145
Chatzistergos, T., Ermolli, I., Solanki, S. K., \& Krivova, N. A. 2018, A\&A, 609, A92
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Ermolli, I., Criscuoli, S., Centrone, M., Giorgi, F., \& Penza, V. 2007, A\&A, 465,305
Ermolli, I., Fofi, M., Bernacchia, C., et al. 1998, SoPh, 177, 1
Foukal, P., Bertello, L., Livingston, W. C., et al. 2009, SoPh, 255, 229
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Hale, G. E., Ellerman, F., Nicholson, S. B., \& Joy, A. H. 1919, ApJ, 49, 153
Isola, P., Zhu, J., Zhou, T., \& Efros, A. A. 2016, CoRR, arXiv:1611.07004
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Leamon, R. J., \& McIntosh, S. W. 2009, ApJL, 697, L28
LeCun, Y., \& Bengio, Y. 1998, The Handbook of Brain Theory and Neural Networks (Cambridge, MA: MIT Press)

Leighton, R. B. 1959, ApJ, 130, 366
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Liu, Y., Hoeksema, J. T., Scherrer, P. H., et al. 2012, SoPh, 279, 295
Livingston, W. C., Harvey, J., Slaughter, C., \& Trumbo, D. 1976, ApOpt, 15,40
Mao, X., Li, Q., Xie, H., Lau, R. Y. K., \& Wang, Z. 2016, CoRR, arXiv:1611. 04076
McIntosh, S. W., Davey, A. R., \& Hassler, D. M. 2006, ApJL, 644, L87
McIntosh, S. W., Leamon, R. J., Davey, A. R., \& Wills-Davey, M. J. 2007, ApJ, 660, 1653
Mirza, M., \& Osindero, S. 2014, CoRR, arXiv:1411.1784
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Paszke, A., Gross, S., Massa, F., et al. 2019, in Advances in Neural Information Processing Systems 32, ed. H. Wallach et al., 32 (Red Hook, NY: Curran Associates, Inc.), 8026
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 1
Pevtsov, A. A., Virtanen, I., Mursula, K., Tlatov, A., \& Bertello, L. 2016, A\&A, 585, A40
Scherrer, P. H., Bogart, R. S., Bush, R. I., et al. 1995, SoPh, 162, 129
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
van der Walt, S., Schönberger, J. L., Nunez-Iglesias, J., et al. 2014, PeerJ, 2, e453
Wang, T., Liu, M., Zhu, J., et al. 2017, CoRR, arXiv:1711.11585
Wang, Z., Bovik, A., Sheikh, H., \& Simoncelli, E. 2004, ITIP, 13, 600
Zhu, J., Park, T., Isola, P., \& Efros, A. A. 2017, CoRR, arXiv:1703.10593"
Sumiaya Rahman et al 2024 - Near-real-time 3D Reconstruction of the Solar Coronal Parameters Based on the Magnetohydrodynamic Algorithm outside a Sphere Using Deep Learning.pdf,"# Near-real-time 3D Reconstruction of the Solar Coronal Parameters Based on the Magnetohydrodynamic Algorithm outside a Sphere Using Deep Learning 

Sumiaya Rahman ${ }^{1}$ (D) Hyun-Jin Jeong ${ }^{2}$ (D), Ashraf Siddique ${ }^{3}$ (D), Yong-Jae Moon ${ }^{1,2}$ (D), and Bendict Lawrance ${ }^{2}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{3}$ Department of Computer Science Engineering, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2023 August 13; revised 2023 December 5; accepted 2023 December 22; published 2024 February 21


#### Abstract

For the first time, we generate solar coronal parameters (density, magnetic field, radial velocity, and temperature) on a near-real-time basis by deep learning. For this, we apply the Pix2PixCC deep-learning model to threedimensional (3D) distributions of these parameters: synoptic maps of the photospheric magnetic field as an input and the magnetohydrodynamic algorithm outside a sphere (MAS) results as an output. To generate the 3D structure of the solar coronal parameters from 1 to 30 solar radii, we train and evaluate 152 distinct deep-learning models. For each parameter, we consider the data of 169 Carrington rotations from 2010 June to 2023 February: 132 for training and 37 for testing. The key findings of our study are as follows: First, our deep-learning models successfully reconstruct the 3D distributions of coronal parameters from 1 to 30 solar radii with an average correlation coefficient of 0.98 . Second, during the solar active and quiet periods, the AI-generated data exhibits consistency with the target MAS simulation data. Third, our deep-learning models for each parameter took a remarkably short time (about 16 s for each parameter) to generate the results with an NVIDIA Titan XP GPU. As the MAS simulation is a regularization model, we may significantly reduce the simulation time by using our results as an initial configuration to obtain an equilibrium condition. We hope that the generated 3D solar coronal parameters can be used for the near-real-time forecasting of heliospheric propagation of solar eruptions.


Unified Astronomy Thesaurus concepts: Active solar corona (1988); Astronomy data analysis (1858); Astronomy image processing (2306); The Sun (1693)

## 1. Introduction

The solar coronal parameters such as density, magnetic field, radial velocity, and temperature are fundamental variables for describing the physical processes that occur in the solar corona and the inner heliosphere. In fact, those parameters have a dominant influence on the dynamics, structures, and evolutionary phenomena observed in the solar atmosphere, particle accelerations, and propagation. These coronal parameters characterize the overall structure of the solar corona and the position of the heliospheric current sheet, influence the regions of fast and slow solar wind, and predict the probable locations of coronal mass ejections (Linker 1998). Ultimately, these solar coronal parameters control the evolution of physical phenomena in the solar system, including the Sun itself and the disastrous space weather events in the solarterrestrial and interplanetary space environment (Tan 2022). The magnetohydrodynamic algorithm outside a sphere (MAS; Linker et al. 1999; Mikić et al. 1999) model allows us to determine the large-scale structure of the magnetic field in the corona, along with the distribution of the solar wind velocity, plasma density, and temperature. It is a 3D magnetohydrodynamic (MHD) simulation model that is built using computational physics and solar corona modeling, and it covers from 1 to 30 solar radii and the inner heliosphere from 30 solar radii to 5 au . The MAS model, being a time-dependent resistive polytropic MHD model, utilizes suitable boundary conditions and advances the MHD equations forward in time (Riley et al. 2011). Typically, the implementation of this resistive MHD model involves significant preprocessing and

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
postprocessing. Moreover, due to user time and queue limits at computer centers, running the MAS code with high spatial resolution and long-duration evolution parameters may necessitate additional computational resources (Caplan et al. 2019).

Generative adversarial network (GAN; Goodfellow et al. 2014) models are widely used for structured data, particularly in tasks such as image translation. The Pix2Pix (Isola et al. 2017) and the Pix2PixHD (Wang et al. 2018) are supervised models based on a GAN. Those models have been applied in various areas of astronomy and space weather applications: (1) image translation among mutiwavelength images (Kim et al. 2019; Park et al. 2019; Jeong et al. 2020; Shin et al. 2020; Jeong et al. 2022; Lawrance et al. 2022), (2) denoising the solar magnetogram (Park et al. 2020), (3) superresolution of the solar data (Jia et al. 2019; Rahman et al. 2020), and (4) the 3D reconstruction of coronal electron density (Jang et al. 2021; Rahman et al. 2023).

In our previous paper (Rahman et al. 2023), we successfully produced a 3D density structure of the solar corona from 1 to 30 solar radii using the Pix2PixHD model (Wang et al. 2018) whose computing time was much shorter than the MAS simulation. The synthetic coronagraphic data estimated from the results are consistent with the real coronagraph image of the Solar Heliospheric Observatory/Large Angle Spectroscopic Coronagraph C3 coronagraph data. Following Rahman et al. (2023), we generate solar coronal parameters (magnetic field, radial velocity, and temperature) at a near-real-time basis using the Pix2PixCC model. Along with the density results, the AIgenerated 3D structure of the solar coronal magnetic field, radial velocity, and temperature from our Pix2PixCC models can be employed for the inner boundary conditions for heliospheric simulation models such as the MAS heliospheric
![img-0.jpeg](img-0.jpeg)

Figure 1. The flowchart of the Pix2PixCC model and the loss functions.
models (Lionello et al. 2009) and the Wang-Sheeley-Arge model (Arge et al. 2004).

The paper is organized as follows. The data and the methods of the Pix2PixCC model are described in Sections 2 and 3. The results and discussions are given in Section 4. The summary of our study is given in Section 5.

## 2. Data

The MAS simulation model, developed by CORHEL (for CORona-HELiosphere), is a suite of models and tools dedicated to modeling the solar corona and inner heliosphere. CORHEL delivers solutions to the community via Predictive Science Inc. (PSI). The PSI coronal code MAS solves timedependent MHD equations until a steady state is reached, with the boundary condition defined by a magnetic map (Linker 2011). For this study, we collected the solar coronal parameter
data of 169 Carrington rotations (CRs) from 2010 June to 2023 February for training and testing. We took the data regarding these parameters from the PSI website. ${ }^{4}$ Following our previous work (Rahman et al. 2023), we incorporate data augmentation techniques to enhance the amount of data. These provide diversity into the data set during the training process and help to address potential artifact issues during testing (Shorten \& Khoshgoftaar 2019). After the data augmentation, the number of training data sets increases from 132 to 9504 for each parameter. The data augmentation is not applied to the test data sets.

We train the deep-learning models using the synoptic maps of the photospheric magnetic field as input data to generate the 3D solar coronal parameters (magnetic field, radial velocity,

[^0]
[^0]:    4 http://www.predsci.com/data
![img-1.jpeg](img-1.jpeg)

Figure 2. Visual comparisons of the solar coronal magnetic field structure between the MAS simulation results and AI-generated during CR 2194 at 2.04, 12.63, 25.40, and 30.51 solar radii, respectively. Left column: synoptic map of the photospheric magnetic field. Middle column: the magnetic field maps from the MAS simulation model. Right column: AI-generated ones.

and temperature) as target data from 1 to 30 solar radii. We employ an interpolation technique that considers the latitude and longitude position for the alignment of the parameters. By considering the distances between neighboring latitude and longitude positions, we interpolate both the input and the target. The size of our input and target data sets are 182 × 101 (longitude, latitude) and 182 × 101 × 150 (longitude, latitude, and altitude from the solar surface), respectively. We also normalize the data sets according to Rahman et al. (2023). For analyzing the results of the test data sets, we denormalized and converted the MAS code units to cgs units, following the process described in Mikić et al. (2018).

### 3. Method

Following our previous study (Rahman et al. 2023), in this study we apply the deep-learning model Pix2PixCC (Jeong et al. 2022) which is an improved model of Pix2PixHD, to generate the major solar coronal parameters (magnetic field, radial velocity, and temperature) from the photospheric magnetic field. Jeong et al. (2022) showed that they improved the generation of synthetic solar farside magnetograms from Solar Terrestrial Relations Observatory (Kaiser et al. 2008) and Solar Dynamics Observatory (Pesnell et al. 2012) data sets using the Pix2PixCC model. Along with the generator (G) and the discriminator (D), the Pix2PixCC model incorporates an additional component called the inspector (I). The generative network is composed of several convolutional layers and transposed convolutional layers. A convolutional layer automatically extracts features from the input data, while a transposed convolutional layer attempts to reconstruct the output using the extracted features. The discriminator (D) is constructed using multiple convolutional layers. It incorporates two specific loss functions: the feature matching (FM) loss and the least-squares GAN (LSGAN) loss (Mao et al. 2016). The FM loss optimizes the parameters of the generator based on the generated feature map by convolution layers. On the other hand, the LSGAN penalizes the fake samples and pushes the generator to produce outputs that are closer to the decision boundary, making them more realistic.

The objective function of loss functions can be represented as follows:

$$
\mathcal{L}_{\text{FM}}(G, D) = \sum_{i=1}^{T} \frac{1}{N_i} \left\| [D^{(i)}(x, y) - D^{(i)}(x, G(x))] \right\|, \tag{1}
$$

$$
\mathcal{L}_{\text{LSGAN}}^D(G, D) = \frac{1}{2} \left[ (D(x, y) - y)^2 \right] + \frac{1}{2} \left[ (D(x, G(x)))^2 \right], \tag{2}
$$

$$
\mathcal{L}_{\text{LSGAN}}^G(G, D) = \frac{1}{2} \left[ (D(x, G(x) - y)^2) \right], \tag{3}
$$
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparisons of the solar coronal radial velocity map between the MAS simulation results and AI-generated ones on CR 2178 at 2.04, 12.63, 25.40, and 30.51 solar radii, respectively. Left column: synoptic map of the photospheric magnetic field. Middle column: the radial velocity maps from the MAS simulation model. Right column: AI-generated ones.

where *x* and *y* denote the input (photospheric magnetic field) and target (magnetic field, radial velocity, and temperature) data, respectively. *T*, *N<sub>i</sub>* denotes the total number of convolution layers in the discriminator, and the number of pixels in the feature maps from a convolution layer. *G*(*x*) is the generated output. *D*(*x*, *y*) and *D*(*x*, *G*(*x*)) are probabilities of the real and AI-generated pairs.

The inspector (*I*) tries to stabilize the training of the generator by applying an additional correlation coefficient (CC) loss function. It is based on Lin's concordance CC, which measures the degree to which pairs of data points align with the 45° line passing through the origin. It is shown that the Pix2PixCC model with this CC-based loss function has better performance for large-dynamic-range data (Jeong et al. 2022) than Pix2PixHD. The CC loss function is defined as

$$
\mathcal{L}_{\text{CC}}(G) = \sum_{i=0}^{T} \frac{1}{T+1} \left[ (1 - \text{CC}_i(y, G(x))), \tag{4}
$$

where *T* is the total number of downsampling by a factor of 2 and *i* is the number of downsampling, respectively. The CC value between 2<sup>*i*</sup> times the downsampled target and AI-generated data is represented by *CC<sub>i</sub>*.

Figure 1 represents the overall model and the loss function architectures. In both training and testing processes, the photospheric magnetic field (*x*) is provided as an input to the deep-learning models. The generator aims to generate the MAS simulation–like 3D magnetic field, radial velocity, and temperature map *G*(*x*) from inputs and then provides *G*(*x*) data to the discriminator and the inspector. The discriminator differentiates between the AI-generated pair (*G*(*x*), *x*) and real pair (*y*, *x*), and the inspector calculates CCs between the target *y* and *G*(*x*).

The final objective of the Pix2PixCC is as follows:

$$
\min\_{G} \lambda\_1 \mathcal{L}\_{\text{LSGAN}}^{G}(G, D) + \lambda\_2 \mathcal{L}\_{\text{FM}}(G, D) + \lambda\_3 \mathcal{L}\_{\text{CC}}(G, D), \tag{5}
$$

$$
\min\_{D} \mathcal{L}\_{\text{LSGAN}}^{D}(G, D), \tag{6}
$$

where *λ<sub>1</sub>*, *λ<sub>2</sub>*, and *λ<sub>3</sub>* are hyperparameters that assigned the weights to *L<sub>LSGAN</sub>*, *L<sub>FM</sub>*, and *L<sub>CC</sub>*, respectively. The values are given as 2 for *λ<sub>1</sub>*, 10 for *λ<sub>2</sub>*, and 5 for *λ<sub>3</sub>*.

To obtain the 3D structure of the parameters across the range of 1–30 solar radii, we train a total of 152 deep-learning models for 152 solar radii separately for each parameter. During the training process, we get best results for the magnetic field and radial velocity at epoch 10,000, while for the temperature, the best results are achieved at epoch 300.
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparison of solar coronal temperature maps between the MAS simulation results and AI-generated ones during CR 2220. Each column is the same as in Figures 2 and 3.

Adam (Kingma & Ba 2014) optimizer is used with the initial learning rate of 2 × 10<sup>−4</sup>.

## 4. Results and Discussion

### 4.1. Qualitative Comparison

Figures 2, 3, and 4 represent three examples of image generation using the deep-learning models of the three parameters at distances of 2.04, 12.63, 25.40, and 30.51 solar radii, respectively. Figure 2 demonstrates a comparison of coronal magnetic fields between MAS simulation results and the AI-generated ones during CR 2194. Similarly, the comparison between the MAS simulation results and the AI-generated 3D radial velocity and temperature map during CR 2178 and CR 2220 is shown in Figures 3 and 4, respectively. We note that across a wide range of radial distances, the deep-learning models successfully generate the overall 3D structures and the complex nature of solar coronal dynamics. From the color bars of the middle and right columns of each figure, we observe that our deep-learning models can reproduce the topology and dynamics of the solar coronal parameters.

Next, we provide a comparison between the MAS simulation and AI-generated results of the parameters along the equator (0° latitude) of CR 2174 and CR 2220. The comparison is focused on two specific distances at 2.6 and 21.30 solar radii, which are shown in Figures 5 and 6, respectively. The active regions mostly appear at low-latitude regions (Linker et al. 1999), and the heliospheric current sheet is also produced at the solar magnetic equator (Desai et al. 2020); the parameters also show different properties in this region. At those distances, the solar parameters' behavior transitions from being primarily influenced by the solar magnetic field and coronal dynamics to being more affected by the interplanetary magnetic field and the dynamics of the heliosphere. In the first two rows of Figure 5, we observe that at a distance of 2.6 solar radii along the 0° latitude, the deep-learning models accurately generate the positions of the neutral line, as well as the positive and negative magnetic field values. The third and fourth rows display the AI-generated radial velocity profiles for both CRs, which closely resemble those obtained from the MAS simulation. The intensity plot in the last two rows in Figure 5 also shows consistency both in high- and low-temperature regions. However, certain errors occur at some positions of the AI-generated temperature maps. It is important to note that those errors can be attributed to the inherent limitations and uncertainties of our method. In Figure 6, the results at 21.30 solar radii demonstrate the consistent structure of parameters near the heliospheric regions. This consistency is particularly crucial because the interface between the coronal and
![img-4.jpeg](img-4.jpeg)

Figure 5. Two examples of comparisons of the MAS simulation results, which include the magnetic field, radial velocity, and temperature map plus AI-generated ones of the parameters along the equator (0° latitude) on CRs 2174 and 2220 at 2.6 solar radii. Left to right: MAS simulation results, AI-generated ones, the values of the three parameters at 0° latitude of CRs 2174 and 2220, and the 2D histograms, respectively.

The heliospheric regions is typically situated in the supercritical-flow region, which is near 21.5 solar radii or 0.1 au (Linker 2011).

Next, we compare the MAS simulation and AI-generated results during the solar minimum (Figure 7) and solar maximum (Figure 8) periods at 21.30 solar radii. The comparisons in Figure 7 shows that the deep-learning models are able to reproduce the structure of the coronal parameters remarkably well during the solar minimum. It is shown that long-lived helmet streamers and coronal holes may persist over several solar rotations. Solar minimum conditions give us the opportunity to distinguish between the fundamental coronal structure and the solar active phenomena (Linker et al. 1999). This is not surprising as the match between the MAS simulation and the deep-learning models is poorer during high solar activity than at the solar minimum. The temporal variability of the photospheric magnetic field increases as the Sun approaches solar maximum (Riley et al. 2001). It is also noted that the synoptic map of the photospheric magnetic field largely affects the simulation results (Gressl et al. 2014).

### 4.2. Quantitative Comparison

The averaged pixel-to-pixel Pearson's CC (higher is better), normalized rms (NRMSE; smaller is better), and normalized mean absolute error (NMAE; smaller is better) values between those of the MAS simulation and the AI-generated values of the testing data set are calculated and presented in Table 1. Here, we calculate those three metrics for magnetic field, radial velocity, and temperature at specific heights (2.07, 8.22, 14.33, 21.30, and 26.42 solar radii). The additional CC loss
![img-5.jpeg](img-5.jpeg)

Figure 6. Comparisons of the MAS simulation results and AI-generated ones along the equator (0° latitude) during CRs 2174 and 2220 at 21.30 solar radii. The first column shows the MAS simulation results. The second column displays the corresponding AI-generated results for the magnetic field, radial velocity, and temperature, the third column shows the values of three parameters at 0° latitude of CRs 2174 and 2220, respectively, and the fourth column represents the 2D histograms.

The function used in our deep-learning models has contributed to achieving higher CC values for the parameters not only in lower solar radii but also in higher solar radii. Furthermore, it is to be noted that the NRMSE and NMAE values of the magnetic field and the radial velocity are smaller compared to the temperature.

We use a single NVIDIA TITAN XP GPU, CUDA 11.4 and 12 GB, to train and test the 152 deep-learning models for each parameter. An AMD Ryzen 5 3600 6-core processor with a 16.0 GB RAM CPU is used for loading and preprocessing the data sets. Following our previous study (Rahman et al. 2023), the computing times for the parameters are much faster than the usual computational time on the MAS simulation (Caplan et al. 2019). For each parameter, it takes approximately 16 s to compute 152 deep-learning models of the test data set with a resolution of 182 × 96 × 152 up to 30 solar radii.

### 5. Conclusion and Summary

In this study, we have applied Pix2PixCC models to generate the 3D structure of major solar coronal parameters from photospheric magnetic field data. We trained and tested 152 deep-learning models using a data set consisting of 169 pairs of CRs from 2010 June to 2023 February. The major findings of this study are summarized as follows. First, the Pix2PixCC models can efficiently and successfully generate the 3D solar coronal magnetic field, radial velocity, and temperature map with significantly high CC values. Such 3D structures of solar coronal parameters will be useful in analyzing solar observations and developing up-to-date models of the solar corona and inner heliosphere. Many multispectral properties of the corona observed in extreme ultraviolet and X-ray emission can be analyzed utilizing our AI-generated results along with our earlier AI-generated electron density results (Rahman et al. 2023).
![img-6.jpeg](img-6.jpeg)

Figure 7. Comparisons between the MAS simulation results and AI-generated ones at a distance of 21.30 solar radii during a solar minimum period. From left to right the figure shows the MAS simulation results (left), AI-generated ones (middle), and the 2D histograms between the target and AI-generated ones (right) on CR 2224 (2019 December 10).

Table 1

The Pearson's Correlation Coefficient (CC), Normalized rms Error (NRMSE), and Normalized Mean Absolute Error (NMAE) between AI-generated Data and Target Values for the Testing Data Set

|  Solar Radius | Magnetic Field (G) |  |  | Velocity (km s⁻¹) |  |  | Temperature (K) |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|   | CC | NRMSE | NMAE | CC | NRMSE | NMAE | CC | NRMSE | NMAE  |
|  2.07 | 0.99 | 0.10 | 0.06 | 0.98 | 0.09 | 0.16 | 0.99 | 0.13 | 0.09  |
|  8.22 | 0.98 | 0.13 | 0.08 | 0.99 | 0.08 | 0.14 | 0.98 | 0.17 | 0.11  |
|  14.33 | 0.99 | 0.14 | 0.09 | 0.98 | 0.07 | 0.13 | 0.98 | 0.16 | 0.11  |
|  21.30 | 0.99 | 0.14 | 0.08 | 0.99 | 0.08 | 0.13 | 0.99 | 0.18 | 0.12  |
|  26.42 | 0.98 | 0.12 | 0.09 | 0.99 | 0.08 | 0.15 | 0.99 | 0.16 | 0.11  |

Second, our study also demonstrates good consistency between the MAS simulation and AI-generated results in the solar equatorial region at 2.6 and 21.30 solar radii. As we know, most heliospheric models like the Wang–Sheeley–Arge model (Arge et al. 2004) have inputted boundary conditions as data at 21.5 solar radii (0.1 au). Third, while the AI-generated results during the solar minimum period agree well with the MAS ones for three coronal parameters, those during the solar maximum period have some inconsistencies, especially for temperature. Finally, the evaluation metrics show high CC values and low NRMSE and NMAE values for both low and high solar radii. The overall computational time required for testing the 152 models for both parameters is significantly shorter compared to the MAS simulation.

As the MAS simulation is a regularization model, we may effectively reduce the simulation time by using our results together with the electron density results obtained from our prior study (Rahman et al. 2023) as an initial configuration to obtain an equilibrium condition. The AI-generated results can be potentially used for specifying the condition on global magnetohydrodynamic models to reproduce large-scale coronal phenomena, heliospheric structures, and solar wind modeling. The ambient solar coronal parameter structures can be applied for the near-real-time forecasting of the heliospheric propagation of solar eruptions. In the future, we plan to predict heliospheric parameters from 21.5 solar radii (0.1 au) to 1 au and compare our deep-learning model results with the MAS heliospheric models (Lionello et al. 2009), the Wang–Sheeley–
![img-7.jpeg](img-7.jpeg)

Figure 8. Comparisons between the MAS simulation results and AI-generated ones at 21.30 solar radii during a solar maximum period. From left to right the figure shows the MAS simulation results, AI-generated ones, and the 2D histograms between the target and AI-generated ones during CR 2170 (2015 November 28).

Arge model (Arge et al. 2004), and recently published deep-learning base model (Son et al. 2023). Furthermore, we will compare our AI-generated results with observations (e.g., Bemporad 2017; Cho et al. 2018).

## Acknowledgments

This research was supported by a Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2023-00248916), the Korea Astronomy and Space Science Institute under the R&D program (Project No. 2024-1-850-07) supervised by the Ministry of Science and ICT (MSIT), and an Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15%). We thank the numerous team members who have contributed to the MAS simulation. We acknowledge the community effort devoted to developing the following open-source packages used in this work.

**Software:** PyTorch (Paszke et al. 2019), NumPy (Harris et al. 2020), Matplotlib (Hunter 2007), Skimage (Van der Walt et al. 2014), SunPy (The SunPy Community et al. 2020).

## ORCID IDs

- Sumiaya Rahman https://orcid.org/0009-0004-9329-9474
- Hyun-Jin Jeong https://orcid.org/0000-0003-4616-947X
- Ashraf Siddique https://orcid.org/0000-0003-2186-5735
- Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
- BENDITZ Lawrance https://orcid.org/0000-0001-6648-0500

## References

- Arge, C., Luhmann, J., Odstrcil, D., et al. 2004, *JASTP*, 66, 1295
- Bemporad, A. 2017, *ApJ*, 846, 86
- Caplan, R. M., Linker, J. A., Mikić, Z., et al. 2019, *JPhCS*, 1225, 012012
- Cho, I.-H., Moon, Y.-J., Nakariakov, V. M., et al. 2018, *PhRvL*, 121, 075101
- Desai, R. T., Zhang, H., Davies, E. E., et al. 2020, *SoPh*, 295, 130
- Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, *arXiv:1406.2661*
- Gressl, C., Veronig, A. M., Temmer, M., et al. 2014, *SoPh*, 289, 1783
- Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, *Natur*, 585, 357
- Hunter, J. D. 2007, *CSE*, 9, 90
- Isola, P., Zhu, J.-Y., Tinghui, E., et al. 2017, in 2017 IEEE Conf. on Computer Vision and Pattern Recognition (Piscataway, NJ: IEEE), 5967
- Jang, S., Kwon, R.-Y., Linker, J. A., et al. 2021, *ApJL*, 920, L30
- Jeong, H.-J., Moon, Y.-J., Park, E., et al. 2020, *ApJL*, 903, L25
- Jeong, H.-J., Moon, Y.-J., Park, E., et al. 2022, *ApJS*, 262, 50
- Jia, P., Huang, Y., Cai, B., et al. 2019, *ApJL*, 881, L30
- Kaiser, M. L., Kucera, T. A., Davila, J. M., et al. 2008, *SSRV*, 136, 5
- Kim, T., Park, E., Lee, H., et al. 2019, *NatAs*, 3, 397
- Kingma, D. P., & Ba, J. 2014, *arXiv:1412.6980*
- Lawrance, B., Lee, H., Park, E., et al. 2022, *ApJ*, 937, 111
- Linker, J. A. 2011, A Next-Generation Model of the Corona and SolarWind: Final Report AFRL-OSR-VA-TR-2012-0199, Predictive Science Inc., https://apps.dtic.mil/sti/tr/pdf/ADA563658.pdf
- Linker, J. A., Mikić, Z., Biesecker, D. A., et al. 1999, *JGR*, 104, 9809
- Linker, J. A. 1998, Global Magnetohydrodynamic Modeling of the Solar Corona, Contractor Report 19980107903, Science Applications International Corporation, https://ntrs.nasa.gov/citations/19980107903
- Lionello, R., Linker, J. A., & Mikić, Z. 2009, *ApJ*, 690, 902
- Mao, Y., Zang, J., & Letaief, K. B. 2016, *USAC*, 34, 3590
- Mikić, Z., Downs, C., Linker, J., et al. 2018, *NatAs*, 2, 913
Mikić, Z., Linker, J. A., Schnack, D. D., et al. 1999, PhPI, 6, 2217
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, E., Moon, Y.-J., Lim, Daye, et al. 2020, ApJL, 891, L4
Pauzke, A., Gross, S., Massa, F., et al. 2019, Advances in Neural Information Processing Systems 32, ed. H. Wallach et al. (NeurIPS), https://papers.nips. cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740Abstract.html
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Rahman, S., Moon, Y.-J., Park, E., et al. 2020, ApJL, 897, L32
Rahman, S., Shin, S., Jeong, H.-j., et al. 2023, ApJ, 948, 21
Riley, P., Linker, J., Mikić, Z., et al. 2001, Space Weather (Washington, DC: American Geophysical Union), 159

Riley, P., Lionello, R., Linker, J., et al. 2011, SoPh, 274, 361
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Shorten, C., \& Khoshgoftaar, T. M. 2019, J. Big Data, 6, 1
Son, J., Sung, S.-K., Moon, Y.-J., et al. 2023, ApJS, 267, 45
Tan, B. 2022, RAA, 22, 072001
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Van der Walt, S., Schönberger, J. L., Nunez-Iglesias, J., et al. 2014, PeerJ, 2, e453
Wang, T.-c., Liu, M.-Y., Jun-Yan, T., et al. 2018, in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (Los Alamitos, CA: IEEE Computer Society), 8798"
Kangwoo Yi et al 2023 - Application of Deep Reinforcement Learning to Major Solar Flare Forecasting.pdf,"# Application of Deep Reinforcement Learning to Major Solar Flare Forecasting 

Kangwoo $\mathrm{Yi}^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), and Hyun-Jin Jeong ${ }^{2}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea; moongj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea Received 2022 September 8; revised 2023 January 6; accepted 2023 January 29; published 2023 March 15


#### Abstract

In this study, we present the application of deep reinforcement learning to the forecasting of major solar flares. For this, we consider full-disk magnetograms at 00:00 UT from the Solar and Heliospheric Observatory/Michelson Doppler Imager (1996-2010) and the Solar Dynamics Observatory/Helioseismic and Magnetic Imager (2011-2019), as well as Geostationary Operational Environmental Satellite X-ray flare data. We apply Deep Q-Network (DQN) and Double DQN, which are popular deep reinforcement learning methods, to predict ""Yes or No"" for daily M- and X-class flare occurrence. The reward functions, consisting of four rewards for true positive, false positive, false negative, and true negative, are used for our models. The major results of this study are as follows. First, our deep-learning models successfully predict major solar flares with good skill scores, such as HSS, F1, TSS, and ApSS. Second, the performance of our models depends on the reward function, learning method, and target agent update time. Third, the performance of our deep-learning models is noticeably better than that of a convolutional neural network (CNN) model with the same structure: 0.38 (CNN) to 0.44 (ours) for HSS, 0.47 to 0.52 for F1, 0.53 to 0.59 for TSS, and 0.09 to 0.12 for ApSS.

Unified Astronomy Thesaurus concepts: Solar flares (1496); Neural networks (1933); The Sun (1693)


## 1. Introduction

A solar flare is a sudden flash on the Sun, releasing a huge amount of energy in a broad spectrum of emissions and accelerating particles to interplanetary space. The flare is categorized into five classes (A, B, C, M, and X, from smallest to biggest) according to its strength in the soft X-ray flux $(0.1-0.8 \mathrm{~nm})$. Each class represents a 10 -fold energy increase. Intense X-ray fluxes and accelerated particles from strong flares result in enormous economic losses such as satellite drag, GPS disruption, and radio fade-outs.

One of the major challenges in flare forecasting is handling class imbalance issues. The solar flare occurrence rate depends on its energy. Strong flares ( $\geqslant \mathrm{M}$ class) rarely occur while weak flares ( $\leqslant \mathrm{C}$ class) frequently. The flare data reported by the National Oceanic and Atmospheric Administration (NOAA) during solar cycle 23 (1996-2008) shows that the occurrence ratio between X-, M-, and C-class flares is around 1:10:100. Prediction methods take common events more frequently than rare ones and overfit to common events. To alleviate the class imbalance problem, previous studies use oversampling, undersampling, and weighted-loss methods (Bobra \& Couvidat 2015; Liu et al. 2017, 2019; Nishizuka et al. 2018; Zheng et al. 2019; Cinto et al. 2020; Jiao et al. 2020; Li et al. 2020; Deng et al. 2021).

The class imbalance issue is essential in not only solar flare forecasting but also many classification problems. In deep learning, Lin et al. (2020) suggested using deep reinforcement learning to handle the class imbalance problem. Reinforcement learning is a machine-learning technique used to learn how to choose actions in order to maximize a reward. They assigned different rewards for true positive (TP; predicting positive

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
when positive), false negative (FN; predicting negative when positive), false positive (FP; predicting positive when negative), and true negative (TN; predicting negative when negative) for the Deep Q-Network (DQN; Mnih et al. 2015) classification model. Their result suggests that deep reinforcement learning could be a more effective approach to imbalanced classification than other methods.

In this study, for the first time, we apply deep reinforcement learning to major solar flare forecasts. For this, we develop DQN and Double DQN (van Hasselt et al. 2016) flareforecasting models, which predict ""Yes or No"" for the daily occurrence of $\geqslant \mathrm{M}$-class flares. The solar full-disk line-of-sight magnetograms at 00:00 UT from the Solar and Heliospheric Observatory (SOHO; Domingo et al. 1995)/Michelson Doppler Imager (MDI; Scherrer et al. 1995) and Solar Dynamics Observatory (SDO; Pesnell et al. 2012)/Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) are used for model training and testing. We find that deep reinforcement learning with proper reward functions achieves high performance in view of the evaluation scores.

This paper is organized as follows. The data are described in Section 2. The structure of our model and reinforcement learning methods used for this research are described in Section 3. Details of the evaluation methods are described in Section 4. The results of the models are given in Section 5. Conclusions and discussions are presented in Section 6.

## 2. Data

Solar full-disk line-of-sight magnetograms at 00:00 UT from $\mathrm{SOHO} / \mathrm{MDI}$ and $\mathrm{SDO} / \mathrm{HMI}$ are used for model training and testing. The SOHO mission, a cooperative effort between the European Space Agency and NASA, was launched in 1995 December. One of the scientific instruments on SOHO, MDI, provides $1024 \times 1024$ full-disk solar magnetograms with $1 .{ }^{\circ} 98$ per pixel at a cadence of 96 minutes. The SDO mission, launched in 2010 February by NASA, is designed to provide
|  Table 1 |  |  |  |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Data Set Information |  |  |  |  |  |  |
|  | X |  | M | C | Non $(<\mathrm{C})$ | Total |
| Training | 93 |  | 647 | 1478 | 1696 | 3914 |
|  |  | 740 |  |  | 3174 |  |
| Test | 40 |  | 360 | 1240 | 1852 | 3492 |
|  |  | 400 |  |  | 3092 |  |

data to predict solar activity. One of the instruments on SDO, HMI, takes over the role of MDI. HMI produces the $4096 \times 4096$ full-disk magnetograms with $0 . .^{\circ} 5$ per pixel at 720 s cadence.

In order to merge the MDI data and HMI data, the spatial resolution of MDI and HMI are resized into $512 \times 512$ size by block average. In addition, we apply an equation of the relationship between MDI and HMI, proposed by Liu et al. (2012), to convert the MDI data to the HMI proxy data. The data values are expressed in a byte scale of $\pm 100 \mathrm{G}$, which shows well the magnetic features of the solar active area. The magnetograms are labeled separately as major flaring event days ( $\geqslant$ M1.0 Class) or non-major flare event days ( $<$ M1.0 Class) using Geostationary Operational Environmental Satellite (GOES) X-ray flare data.

Finally, we build the data set including solar cycle 23 data (3914 data; MDI from 1996 May to 2008 December) for training, and solar cycle 24 data ( 3492 data; 405 MDI from 2009 January to 2010 December; 3087 HMI from 2011 January to 2019 December) for the test. By using all periods of solar cycle 23 and solar cycle 24 , we fully consider the solar cycle effect on the solar activity in both the training and test periods. The data set is fully described in Table 1.

## 3. Methods

### 3.1. Deep Reinforcement Learning

### 3.1.1. Q-learning and Deep Q-learning Network

In this study, we apply DQN to solar flare forecasting for magnetograms that are environments of solar flares. DQN and its variants are widely used to select an optimal action from the input environment. It optimizes itself to get the maximum reward that depends on the reward functions. If we consider that strong flares ( $\geqslant \mathrm{M}$ class) are much more valuable than weak flares ( $\leqslant \mathrm{C}$ class) and assign higher rewards and stronger penalties to the strong flares than weak flares, the deep reinforcement learning flare model could be optimized by considering strong flares to be more important than weak ones.

DQN is a deep-learning application of Q-learning (Watkins \& Dayan 1992), which is a reinforcement learning algorithm that is used to learn the action value. In Q-learning, the learned action-value function $Q$ is approximated to the optimal actionvalue function (Sutton \& Barto 2018). Q-learning is an offpolicy method (Baird 1995) that makes it possible to learn from a different policy, not a current policy (Franćois-Lavet et al. 2018). Q-learning is also a model-free algorithm that uses a trial-and-error approach (Sutton \& Barto 2018). A function of the Q-learning is expressed as

$$
\begin{aligned}
& Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t}+\gamma Q\left(S_{t+1}, A_{t+1}^{\prime}\right)\right. \\
& \left.\quad-Q\left(S_{t}, A_{t}\right)\right)
\end{aligned}
$$

where $S_{t}, A_{t}$, and $R_{t}$ represent the state, the action, and the reward from the action, at a given time $t$, respectively. $\alpha$ is the learning rate. $\gamma$ is the discount factor for the estimated future value. $Q$ represents the action-value function of the state $S$ and the action $A$, which is calculated by the agent that decides what action to take. $A$ is the action selected by the $\epsilon$-greedy policy, and $A^{\prime}$ is the action selected by the greedy policy. The greedy policy is to select the most valuable action, and the $\epsilon$-greedy policy is to select a random action with a probability $\epsilon$ and to select the most valuable action with a probability $1-\epsilon$.

In order to apply Q-learning to deep learning, Mnih et al. (2015) developed DQN. Its flow chart is given in Figure 1. Mnih et al. (2015) constructed convolutional neural network (CNN; Lecun et al. 1998) agents to analyze the frames of the Atari game screen and to determine an optimal action. In addition, they used two key ideas for DQN. First, they separated the online agent $Q$ and the target agent $Q^{\prime}$ for the Q-learning update to improve the stability of the method. The online agent, which uses the $\epsilon$-greedy policy, calculates the $Q$ value of the action in the current state. The target agent, which uses the greedy policy, calculates the $Q$ value of the action in the next state. The online agent is updated at every training step, while the target agent is updated as a clone of the online agent after $N$ iterations. The agent to be used for the operation is the target agent. Second, they used the experience replay technique (Lin 1993). Game playing and agent updating are asynchronous processes. The agent plays the Atari game without updating. It stores its experiences $e_{t}=\left(S_{t}, A_{t}, R_{t}, S_{t+1}\right)$ at each time step $t$ into the replay memory. The replay memory stores only the last $N$ experience. The agent is updated using randomly selected experience data from the replay memory. By learning previous experiences stored in the replay memory, the agent can be trained more effectively and avoid issues with instability or divergence in the parameters (Mnih et al. 2013). With these ideas, the DQN model played Atari games at a human level.

### 3.1.2. Double Q-learning and Double DQN

Furthermore, we apply Double DQN to flare forecasting. Double DQN, which is a variant of DQN, is a deep-learning application of Double Q-learning (Hasselt 2010). One of Q-learning's problems is that it overestimates the action values because Q-learning uses the action with the maximum value to determine the value of the next state. To avoid overestimation, Hasselt (2010) developed Double Q-learning. Double Q-learning is similar to Q-learning but uses two agents, $Q^{A}$ and $Q^{B}$. A function of Double Q-learning is given by

$$
\begin{aligned}
& Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R_{t}+\gamma Q^{\prime}\left(S_{t+1}, A_{t+1}^{A}\right)\right. \\
& \left.\quad-Q\left(S_{t}, A_{t}\right)\right)
\end{aligned}
$$

where $Q$ is $Q^{A}$ and $Q^{\prime}$ is $Q^{B}$ when updating $Q^{A}$, and vice versa when updating $Q^{B}$. The update agent is selected randomly at every update. $A_{t+1}^{A}$ is the maximum value action selected by the agent $Q$. Taking an action from a different agent reduces overestimation and performs well in some settings in which Q-learning performs poorly. van Hasselt et al. (2016) presented Double DQN to reduce the overestimation of DQN. Double Q-learning uses two agents to avoid overestimation, and DQN also uses two agents, the online agent and the target agent. A
![img-0.jpeg](img-0.jpeg)

Figure 1. A flow chart of the DQN algorithm for flare forecasting. The model's experiences are stored in the replay memory. Randomly selected experiences in the replay memory are used for the DQN model training.

Function of Double DQN is expressed as:

$$
\begin{split}
& Q^O(S_t, A_t) \leftarrow Q^O(S_t, A_t^O) + \alpha(R_t + \gamma Q^T(S_{t+1}, A_{t+1}^O'))
\end{split}
$$

$$
- Q^O(S_t, A_t^O),
$$

where $Q^O$ and $Q^T$ are the online agent and the target agent, respectively. $A^O$ and $A^O'$ indicate actions selected by the online agent with the $\epsilon$-greedy policy and greedy policy, respectively. The target agent in a Double DQN estimates the value of the next state using the action $A_{t+1}^O'$ selected by the online agent. The main difference between Double Q-learning and Double DQN is that in Double DQN, the online agent and target agent do not replace each other. van Hasselt et al. (2016) showed that Double DQN better performs than DQN in many environments.

### 3.1.3. Reinforcement Learning for Imbalanced Classification

Lin et al. (2020) proposed a DQN model for imbalanced classification. Their model was evaluated with various data sets: IMDB (Maas et al. 2011), Cifar-10 (Krizhevsky & Hinton 2009), Mnist (Lecun et al. 1998), and Fashion-Mnist (Xiao et al. 2017). They showed that their DQN model performed better than other models that are based on imbalanced classification methods such as oversampling and undersampling.

### 3.2. Our Model

CNNs are widely used in 2D image processing for classification. CNNs use many convolutional filters that have different weights and are complexly connected with each other to analyze values and structures of input data. Many deep-learning solar flare models used CNNs to analyze magnetograms (Nishizuka et al. 2018; Park et al. 2018; Zheng et al. 2019; Tang et al. 2021a; Yi et al. 2021). By visual explanation using Grad-CAM (Selvaraju et al. 2017) and guided backpropagation (Springenberg et al. 2015), Yi et al. (2021) showed that CNNs can recognize an active region in a full-disk magnetogram and consider it and the polarity inversion line for flare prediction. They tested the deep-learning model, adjusting the number and size of convolutional filters, the number of dense blocks, and the pooling mechanism, and found that the proposed hyperparameters are good for magnetogram analysis for solar flare forecasting.

In this study, to examine the effect of deep reinforcement learning on flare forecasting, we follow Yi et al.'s (2021) model structure and hyperparameters. Figure 2 shows the adopted CNN model architecture. $K$ is kernel size and $D$ is the number of feature dimensions. The model consists of an initial block, five dense blocks using a dense connection (Huang et al. 2017), and a last block. The initial block consists of the $3 \times 3$ convolutional kernel and $2 \times 2$ max pooling layer. Dense blocks contain a batch normalization (BN; Ioffe & Szegedy 2015), a rectified linear unit (ReLU; Nair & Hinton 2010), a $1 \times 1$ convolutional kernel, a BN, a ReLU, a $3 \times 3$ convolutional kernel, a concatenation layer, and a $2 \times 2$ average pooling layer, in this order. The last block consists of a BN, a $2 \times 2$ average pooling layer, and a fully connected layer. Two outputs of the last block are scores for the flare and nonflare. A higher score is used as a result of the model prediction.

The CNN architecture (Figure 2) that we employ is used as an agent for our deep reinforcement learning models. The deep reinforcement learning models use RMSprop optimizer, proposed by Geoff Hinton in Lecture 6e of his Coursera
![img-1.jpeg](img-1.jpeg)

Figure 2. CNN architecture from Yi et al. (2021). K is kernel size. D is the number of feature dimensions and is displayed in the case that it is changed in the layer.

class,<sup>3</sup> and CNN models use the Adam optimizer (Kingma & Ba 2014).

Our models use mean square error loss functions. For a CNN, its loss function is

$$L\_{\rm CNN} = (x\_{i} - y\_{i})^2,\tag{4}$$

where $x\_{i}$ denotes observation, and $y\_{i}$ denotes model result. In DQN, its loss function is

$$L\_{\rm DQN} = (R\_{i} + \gamma Q\_{i} (S\_{i+1}, A\_{i+1}) - Q(S\_{i}, A\_{i}))^2.\tag{5}$$

In Double DQN, its loss function is

$$L\_{\rm DoubleDQN} = (R\_{i} + \gamma Q\_{i} (S\_{i+1}, A\_{i+1}) - Q\_{O}(S\_{i}, A\_{i}O))^2.\tag{6}$$

### 4. Evaluation Methods

#### 4.1. Evaluation Metrics

Different evaluation metrics indicate different aspects of model performance (Barnes et al. 2016; Leka et al. 2019). To evaluate the model fairly from various points of view, we consider four skill scores.

The Heidke skill score (HSS; Heidke 1926),

HSS

$$= \frac{2[(\text{TP} \times \text{TN}) - (\text{FP} \times \text{FN})]}{(\text{TP} + \text{FN}) \times (\text{FN} + \text{TN}) + (\text{TP} + \text{FP}) \times (\text{FP} + \text{TN})},\tag{7}$$

informs the ratio of improvement over random forecasting. It is frequently used in flare-forecasting evaluation; however, it has to be handled carefully because its range depends on the event rate. In an imbalanced data set, its range is from $-2(P \times N)/(P^2 + N^2)$ to 1, where $P$ is the number of positive data while $N$ is the negative data. A negative value means that random forecasting is better, 0 is no skill prediction, and 1 indicates perfect forecasting.

The F1 score, which is a harmonic mean between the precision $\left(\frac{\text{TP}}{\text{TP} + \text{FP}}\right)$ and recall $\left(\frac{\text{TP}}{\text{TP} + \text{FN}}\right)$, is given by

$$\text{F1} = \frac{\text{TP}}{\text{TP} + 0.5 \times (\text{FP} + \text{FN})}.\tag{8}$$

It is a measure of model performance that ranges from 0 to 1, assuming that precision and recall have the same gradient for the evaluation score when both are equal. The F1 score is not biased toward either precision or recall, making it a good evaluation index to analyze the performance of a prediction model. It is used for the evaluation of sunspot analysis for classification (Tang et al. 2021b).

The true skill statistic (TSS; Allouche et al. 2006) evaluates the model in the range -1 to 1. It is described as

$$\text{TSS} = \frac{\text{TP}}{\text{TP} + \text{FN}} - \frac{\text{FP}}{\text{FP} + \text{TN}}.\tag{9}$$

TSS has mainly been used to compare flare models since Bloomfield et al. (2012). They suggested that TSS be used as a standard metric for comparing flare models, as it is not impacted by changes in the ratio of positive to negative values in the test set (Woodcock 1976). However, flare models that overforecast can achieve a high score in TSS easily, while those that underforecast are less likely to (Leka et al. 2019). This overestimation problem becomes more serious as the data set is biased because the ratio of TP to positive data (TP+FN) becomes relatively large while the ratio of FP to negative data (FP+TN) becomes relatively small. For example, Nishizuka et al. (2018) achieved TSS = 0.80 while P:N was 0.03:0.97 and the false-alarm ratio $\left(\frac{\text{FP}}{\text{TP} + \text{FN}}\right)$ was 0.82. In addition, Lim et al. (2019) achieved 0.91 TSS while P:N was 0.01:0.99 and the FAR was 0.9.

Appleman's skill score (ApSS; Appleman 1960), which is similar to HSS except that it informs the improvement relative to unskilled forecasting, is given by

$$\text{ApSS} = \left\{ \frac{\text{TP} - \text{FP}}{\text{TP} + \text{FN}}, \text{ if event rate} < 0.5, \left\{ \frac{\text{TN} - \text{FN}}{\text{FP} + \text{TN}}, \text{ if event rate} \geq 0.5 \right\}.\right\},\tag{10}$$
where the event rate is the ratio of the number of flare events to the total number of data. The ApSS considers both FN and FP to be equal, providing a good overview of the performance of the methods. Its scale depends on the data distribution, from $-\frac{N}{P}$ to 1 . ApSS is a challenging evaluation metric because it becomes negative when FP is greater than TP, which is a common result of imbalanced data classification. It is common for models of major solar flares to achieve a high TSS while having a negative ApSS. For example, Nishizuka et al. (2018) achieved TSS $=0.80$ while ApSS was -3.36 . In addition, Wang et al. (2020) achieved TSS $=0.68$ while ApSS was -1.18 .

### 4.2. Strategy for Comparison

The focus of this study is to determine the effectiveness of reinforcement learning for major flare forecasting. For this, we analyze the performance of the models using four skill scores: HSS, F1, TSS, and ApSS. As we mention in Section 4.1, different skill scores indicate different aspects of model performance. To measure the effects of reinforcement learning on model performance, we present an ""optimum skill score"" for each model. The ""optimum skill score"" is a skill score measured from the optimized model for each skill score. To optimize models, we apply two methods to the models. First, to avoid the overfitting problem and find the best performance, we train deep-learning models for enough epochs and select the model with the best score in epochs. Second, we determine the prediction threshold to measure the optimum skill score. This strategy suggests proper model development options for a specific task.

For comparison, we make CNN models whose structures are described in Figure 2. The CNN models are optimized within 250 epochs. However, reinforcement learning models are trained for 100 epochs because of training time. The training time for 250 epoch CNN models is 4 hr while the training time for 100 epoch reinforcement learning models is 28 hr on the Nvidia RTX 2080 GPU. We compare the performance of the CNN models optimized within 250 epochs and 100 epochs and find that the former outperforms the latter. In addition, we find that our CNN models are overfitted after 250 epochs. In order to compare the results more rigorously, we train the CNN models for 250 epochs while the reinforcement learning models are trained for 100 epochs.

## 5. Results

We make 64 deep reinforcement learning models, based on 16 reward functions, 2 reinforcement learning methods (DQN and Double DQN), and 2 target agent update times (one epoch and two epochs). The 16 reward functions used in this study are listed in Table 2. The reward of TN is fixed at 1 , and other rewards indicate the ratio against TN. We assign different reward values for TP, FP, FN, and TN to find optimized reward functions. In order to increase the cost of rare events, many of the reward functions have a higher penalty for FN than FP.

Table 3 shows the results of the deep reinforcement learning flare models optimized for HSS, F1, TSS, and ApSS, respectively. Out of the 64 models, the highest and secondhighest performances are 0.44 and 0.43 for HSS, 0.52 and 0.51 for F1, 0.59 and 0.58 for TSS, and 0.12 and 0.11 for ApSS (shown in bold).

Table 2
Reward Functions Used in This Study

| TP | FP | FN | TN |
| :-- | :--: | :--: | :--: |
| 1 | -1 | -1 | 1 |
| 4 | -2 | -8 | 1 |
| 4 | -4 | -16 | 1 |
| 4 | -16 | -32 | 1 |
| 4 | -16 | -64 | 1 |
| 8 | -4 | -16 | 1 |
| 8 | -4 | -32 | 1 |
| 8 | -8 | -16 | 1 |
| 8 | -8 | -64 | 1 |
| 8 | -16 | -8 | 1 |
| 8 | -16 | -32 | 1 |
| 8 | -16 | -64 | 1 |
| 8 | -32 | -16 | 1 |
| 4 | -64 | -4 | 1 |
| 4 | -64 | -8 | 1 |
| 8 | -96 | -8 | 1 |

The models that achieve the highest or second-highest score in HSS, F1, or TSS are listed in Table 2. However, in ApSS, we present the models that achieved the highest score because many models were tied for the highest ApSS. The number of listed models for each evaluation metric has no influence on the model analysis. It is hard to evaluate which model is better than others considering reward functions, methods, and the update time, we present all models achieving the highest and the second-highest score for HSS, F1, and TSS, and achieving the highest score for ApSS.

Model 0 is the CNN flare model that has the same structure as the other deep reinforcement learning models. CNN model results are presented for comparison with the deep reinforcement learning models. The reinforcement learning models show better scores than the CNN model, with an increase from 0.38 to 0.44 in HSS, from 0.47 to 0.52 for F1, from 0.50 to 0.58 in TSS, and from 0.09 to 0.12 in ApSS. Models 1, 2, 3, and 4 show good performance in HSS, F1, and/or TSS. In HSS and TSS, the models achieve optimized performance in all four model settings. The optimized HSS models show good performance with different five reward functions, while the optimized TSS models achieve good performance with two reward functions only. All models that showed good performance in HSS, F1, and TSS include a reward of -64 or lower in FP or FN in their reward functions. It indicates that in order to achieve high HSS, F1, or TSS, the model requires a strong penalty for missed forecasting. In ApSS, many models that use Double DQN and have an update time of two epochs achieve the best results. The optimized ApSS models show similar or a little better optimum scores in the HSS than the CNN model, while many of them have lower TSS than the CNN model.
|  Model | Reward Function |  |  |  | Method | Update | Optimum Scores |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  Number | TP | FP | FN | TN |  | Time | HSS | F1 | TSS | ApSS  |
|  0 |  |  |  |  | CNN |  | 0.38 | 0.47 | 0.53 | 0.09  |
|  HSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  2 | 8 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.44 | 0.52 | 0.55 | 0.08  |
|  3 | 4 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.43 | 0.50 | 0.58 | 0.11  |
|  4 | 8 | -8 | -64 | 1 | DQN | 1 epoch | 0.43 | 0.51 | 0.57 | 0.09  |
|  5 | 8 | -96 | -8 | 1 | Double DQN | 2 epochs | 0.43 | 0.50 | 0.51 | 0.10  |
|  F1 optimization models |  |  |  |  |  |  |  |  |  |   |
|  2 | 8 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.44 | 0.52 | 0.55 | 0.08  |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  4 | 8 | -8 | -64 | 1 | DQN | 1 epoch | 0.43 | 0.51 | 0.57 | 0.09  |
|  TSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  3 | 4 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.43 | 0.50 | 0.58 | 0.11  |
|  6 | 8 | -64 | -8 | 1 | Double DQN | 2 epochs | 0.43 | 0.49 | 0.58 | 0.10  |
|  7 | 4 | -16 | -64 | 1 | DQN | 1 epoch | 0.39 | 0.47 | 0.58 | 0.09  |
|  ApSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  8 | 4 | -4 | -16 | 1 | Double DQN | 1 epoch | 0.41 | 0.49 | 0.56 | 0.12  |
|  9 | 8 | -96 | -8 | 1 | Double DQN | 1 epoch | 0.39 | 0.47 | 0.48 | 0.12  |
|  10 | 8 | -4 | -16 | 1 | DQN | 2 epochs | 0.40 | 0.47 | 0.54 | 0.12  |
|  11 | 4 | -2 | -8 | 1 | Double DQN | 2 epochs | 0.39 | 0.47 | 0.49 | 0.12  |
|  12 | 4 | -4 | -16 | 1 | Double DQN | 2 epochs | 0.40 | 0.46 | 0.47 | 0.12  |
|  13 | 8 | -4 | -16 | 1 | Double DQN | 2 epochs | 0.38 | 0.44 | 0.45 | 0.12  |
|  14 | 8 | -4 | -32 | 1 | Double DQN | 2 epochs | 0.41 | 0.49 | 0.53 | 0.12  |
|  15 | 8 | -8 | -64 | 1 | Double DQN | 2 epochs | 0.40 | 0.47 | 0.50 | 0.12  |

Table 3: Results of the Optimized Deep Reinforcement Learning Flare Models

|  Model | Reward Function |  |  |  | Method | Update | Optimum Scores |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  Number | TP | FP | FN | TN |  | Time | HSS | F1 | TSS | ApSS  |
|  0 |  |  |  |  | CNN |  | 0.38 | 0.47 | 0.53 | 0.09  |
|  HSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  2 | 8 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.44 | 0.52 | 0.55 | 0.08  |
|  3 | 4 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.43 | 0.50 | 0.58 | 0.11  |
|  4 | 8 | -8 | -64 | 1 | DQN | 1 epoch | 0.43 | 0.51 | 0.57 | 0.09  |
|  5 | 8 | -96 | -8 | 1 | Double DQN | 2 epochs | 0.43 | 0.50 | 0.51 | 0.10  |
|  F1 optimization models |  |  |  |  |  |  |  |  |  |   |
|  2 | 8 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.44 | 0.52 | 0.55 | 0.08  |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  4 | 8 | -8 | -64 | 1 | DQN | 1 epoch | 0.43 | 0.51 | 0.57 | 0.09  |
|  TSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  1 | 8 | -64 | -8 | 1 | DQN | 2 epochs | 0.44 | 0.51 | 0.59 | 0.10  |
|  2 | 8 | -16 | -64 | 1 | Double DQN | 1 epoch | 0.43 | 0.50 | 0.58 | 0.11  |
|  3 | 4 | -16 | -64 | 1 | Double DQN | 2 epochs | 0.43 | 0.49 | 0.58 | 0.10  |
|  4 | 8 | -8 | -64 | 1 | DQN | 1 epoch | 0.39 | 0.47 | 0.58 | 0.09  |
|  ApSS optimization models |  |  |  |  |  |  |  |  |  |   |
|  8 | 4 | -4 | -16 | 1 | Double DQN | 1 epoch | 0.41 | 0.49 | 0.56 | 0.12  |
|  9 | 8 | -96 | -8 | 1 | Double DQN | 1 epoch | 0.39 | 0.47 | 0.48 | 0.12  |
|  10 | 8 | -4 | -16 | 1 | DQN | 2 epochs | 0.40 | 0.47 | 0.54 | 0.12  |
|  11 | 4 | -2 | -8 | 1 | Double DQN | 2 epochs | 0.39 | 0.47 | 0.49 | 0.12  |
|  12 | 4 | -4 | -16 | 1 | Double DQN | 2 epochs | 0.40 | 0.46 | 0.47 | 0.12  |
|  13 | 8 | -4 | -16 | 1 | Double DQN | 2 epochs | 0.38 | 0.44 | 0.45 | 0.12  |
|  14 | 8 | -4 | -32 | 1 | Double DQN | 2 epochs | 0.41 | 0.49 | 0.53 | 0.12  |
|  15 | 8 | -8 | -64 | 1 | Double DQN | 2 epochs | 0.40 | 0.47 | 0.50 | 0.12  |

![img-2.jpeg](img-2.jpeg)

Figure 3. Training curve tracking the average rewards of the 10 models based on Model 3, using the test data set. Each value is the average reward of the models per epoch.

To estimate the uncertainty of the reinforcement learning flare model, we train Model 3 10 times with different random seeds and calculate averages and standard deviations of four skill scores. The results are 0.42 ± 0.02 for HSS, 0.49 ± 0.01 for F1, 0.58 ± 0.02 for TSS, and 0.10 ± 0.01 for ApSS.

In reinforcement learning research, fairly evaluating the progress of training is challenging. We follow the suggestion of
Bellemare et al. (2013), which calculates total rewards in an episode averaged over the number of games. The metric of the average total reward could be highly distributed because a small adjustment to the weights of the agent results in significant changes to the output of the agent (Mnih et al. 2013). Figure 3 provides the average training performance of the 10 models used to estimate the uncertainty. The plot is noisy, but as training progresses, the distribution of the plot decreases and converges to a high value.

## 6. Conclusion and Discussion

In this study, for the first time, we have presented the application of deep reinforcement learning to the flare forecast of a daily major solar flare. For this, we use full-disk magnetograms at 00:00 UT from SOHO/MDI (1996-2010) and SDO/HMI (2011-2019) and GOES X-ray flare data. We apply deep reinforcement learning to predict ""Yes or No"" for daily M- and X-class flare occurrences. The deep reinforcement learning flare models are trained using various reward functions, DQN and Double DQN methods, and one-epoch and two-epoch target agent update times. The performance of the model is presented using HSS, F1, TSS, and ApSS. The models are compared with other deep reinforcement learning models and the basic CNN models.

The major results of this study are as follows. First, our deep-learning models successfully predict major solar flares with good skill scores such as HSS, F1, TSS, and ApSS. Second, the performance of our models depends on the reward function, learning method, and target agent update time. Third, the performance of our deep-learning models is noticeably better than that of a CNN model with the same structure: 0.38 (CNN) to 0.44 (ours) for HSS, 0.47 to 0.52 for F1, 0.53 to 0.59 for TSS, and 0.09 to 0.12 for ApSS.

We make other major solar flare-forecasting models considering oversampling, undersampling, and a weighted-loss function. When sampling a data set, we keep the ratio between $\mathrm{X}-, \mathrm{M}-, \mathrm{C}$ - and nonflare events (including $\leqslant \mathrm{B}$ flare) to preserve the effects of flare climatology in the data set. Sampling methods are applied to the training set only, not the test set. The best model uses the weighted-loss function based on the weight formula proposed by Ahmadzadeh et al. (2021). Our models that show good results, such as model 1 or 3 in Table 3, are a little better than or similar to the best weighted-loss function model. In addition, we make a deep reinforcement learning flare model using the reward function suggested by Lin et al. (2020). It shows a similar performance to model 3 in Table 3, but with a lower ApSS of 0.08 .

The model structure and hyperparameters taken from Yi et al. (2021) are optimized for CNN-based flare forecasting. Forecasting performance could be improved if the model uses different structures and parameters optimized for deep reinforcement learning.

The performance of the flare-forecasting model depends on the solar cycle phase of the test data set (Wang et al. 2020). Many flare-forecasting researchers used one solar cycle data set (2010-2015 or 2010-2018) and divided it into the training data set and the test data set (Nishizuka et al. 2017; Liu et al. 2019; Zheng et al. 2019; Jiao et al. 2020; Li et al. 2020). To evaluate model performance considering solar cycle phase dependency, they used 10 -fold cross-validation. For this, they randomly separated their data set into 10 folds, trained the model 10 times with a different set of 9 folds each training, and tested the
model with the remaining fold. However, a randomly separated training data set and test data set could contain similar data observed at similar times, and the model performance could be overly evaluated. In this work, we do not use 10 -fold crossvalidation because our research fully considers solar cycle dependency (1996-2008 for training and 2009-2019 for testing). Instead, to evaluate uncertainty and generalize the performance of the model, we train the model 10 times with different random seeds and calculate the mean and standard deviation of the skill scores.

In this study, we show that many deep reinforcement learning flare models easily achieve better ApSS than the CNN model even when the other scores are low. This result suggests that deep reinforcement learning is useful for forecasting rare events precisely while reducing the false-alarm ratio. For example, deep reinforcement learning models could be useful in managing the schedule of flight attendants to minimize their annual cosmic radiation exposure.

We thank the numerous team members who have contributed to the success of the SDO mission, as well as the SOHO mission. This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2021R1A6A3A01088835) and the Korea Astronomy and Space Science Institute under the R\&D program (project No. 2022-1-850-05) supervised by the Ministry of Science and ICT.

Software: PyTorch (Paszke et al. 2019), NumPy (Harris et al. 2020), Matplotlib (Hunter 2007), SciPy (Virtanen et al. 2020), Astropy (Robitaille et al. 2013; Price-Whelan et al. 2018), SunPy (The SunPy Community et al. 2020).

## ORCID iDs

Kangwoo Yi (2) https://orcid.org/0000-0003-4342-9483
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Hyun-Jin Jeong (2) https://orcid.org/0000-0003-4616-947X

## References

Ahmadzadeh, A., Aydin, B., Georgoulis, M. K., et al. 2021, ApJS, 254, 23
Allouche, O., Tsoar, A., \& Kadmon, R. 2006, J. Appl. Ecol., 43, 1223
Appleman, H. S. 1960, BAMS, 41, 64
Baird, L. C. 1995, in Proc. Twelfth Int. Conf. on Machine Learning, ICML'95, ed. A. Prieditis \& S. Russell (San Francisco, CA: Morgan Kaufmann), 30
Barnes, G., Leka, K. D., Schrijver, C. J., et al. 2016, ApJ, 829, 89
Bellemare, M. G., Naddaf, Y., Veness, J., \& Bowling, M. 2013, J. Artif. Intell. Res., 47, 253
Bloomfield, D. S., Higgins, P. A., McAteer, R. T. J., \& Gallagher, P. T. 2012, ApJL, 747, L41
Bobra, M. G., \& Couvidat, S. 2015, ApJ, 798, 135
Cinto, T., Gradvohl, A. L. S., Coelho, G. P., \& da’Silva, A. E. A. 2020, MNRAS, 495, 3332
Deng, Z., Wang, F., Deng, H., et al. 2021, ApJ, 922, 232
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Franćois-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., \& Pineau, J. 2018, Found. Trends Mach. Learn., 11, 219
Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, Natur, 585, 357
Hasselt, H. 2010, in Advances in Neural Information Processing Systems 23, ed. J. Lafferty et al. (Red Hook, NY: Curran Associates), 2613, https:// papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9Abstract.html
Heidke, P. 1926, Geografiska Annaler, 8, 301
Huang, G., Liu, Z., Van Der Maaten, L., \& Weinberger, K. Q. 2017, in 2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (Los Alamitos, CA: IEEE Computer Society), 2261
Hunter, J. D. 2007, CSE, 9, 90
Ioffe, S., \& Szegedy, C. 2015, in Proc. Machine Learning Research 37, Proc. 32nd Int. Conf. on Machine Learning, ed. F. Bach \& D. Blei, 448, http:// proceedings.mlr.press/v37/ioffe15.html
Jiao, Z., Sun, H., Wang, X., et al. 2020, SpWea, 18, e2020SW002440
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Krizhevsky, A., \& Hinton, G. 2009, Learning Multiple Layers of Features from Tiny Images, Tech. Rep., Univ. of Toronto
Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, IEEEP, 86, 2278
Leka, K. D., Park, S.-H., Kusano, K., et al. 2019, ApJS, 243, 36
Li, X., Zheng, Y., Wang, X., \& Wang, L. 2020, ApJ, 891, 10
Lim, D., Moon, Y.-J., Park, J., et al. 2019, JKAS, 52, 133
Lin, E., Chen, Q., \& Qi, X. 2020, Appl. Intell., 50, 2488
Lin, L.-J. 1993, Reinforcement Learning for Robots Using Neural Networks., Tech. Rep., Carnegie Mellon Univ.
Liu, C., Deng, N., Wang, J. T. L., \& Wang, H. 2017, ApJ, 843, 104
Liu, H., Liu, C., Wang, J. T. L., \& Wang, H. 2019, ApJ, 877, 121
Liu, Y., Hoeksema, J. T., Scherrer, P. H., et al. 2012, SoPh, 279, 295
Maas, A. L., Daly, R. E., Pham, P. T., et al. 2011, in Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ed. D. Lin et al. (Portland, OR: Association for Computational Linguistics), 142, http://www.aclweb.org/anthology/ P11-1015
Mnih, V., Kavukcuoglu, K., Silver, D., et al. 2013, arXiv:1312.5602
Mnih, V., Kavukcuoglu, K., Silver, D., et al. 2015, Natur, 518, 529
Nair, V., \& Hinton, G. E. 2010, in Proc. 27th Int. Conf. on Machine Learning, ed. J. Fürnkranz \& T. Joachims (Madison, WI: Omnipress), 807, https:// icml.cc/Conferences/2010/papers/432.pdf
Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., \& Ishii, M. 2018, ApJ, 858,113

Nishizuka, N., Sugiura, K., Kubo, Y., et al. 2017, ApJ, 835, 156
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Paszke, A., Gross, S., Massa, F., et al. 2019, in Advances in Neural Information Processing Systems 32, ed. H. Wallach et al., 8026, https://papers.nips.cc/ paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Price-Whelan, A. M., Sipilcz, B., Günther, H., et al. 2018, AJ, 156, 123
Robitaille, T. P., Tollerud, E. J., Greenfield, P., et al. 2013, A\&A, 558, A33
Scherrer, P. H., Bogart, R. S., Bush, R. I., et al. 1995, SoPh, 162, 129
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Selvaraju, R. R., Cogswell, M., Das, A., et al. 2017, in 2017 IEEE Int. Conf. on Computer Vision (ICCV) (Los Alamitos, CA: IEEE Computer Society), 618
Springenberg, J., Dosovitskiy, A., Brox, T., \& Riedmiller, M. 2015, arXiv:1412.6806
Sutton, R. S., \& Barto, A. G. 2018, Reinforcement Learning: An Introduction (2nd ed.; Cambridge, MA: MIT Press)
Tang, R., Liao, W., Chen, Z., et al. 2021a, ApJS, 257, 50
Tang, R., Zeng, X., Chen, Z., et al. 2021b, ApJS, 257, 38
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
van Hasselt, H., Guez, A., \& Silver, D. 2016, in Proc. AAAI Conf. on Artificial Intelligence 2016 (Palo Alto, CA: AAAI Press), 2094
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Wang, X., Chen, Y., Toth, G., et al. 2020, ApJ, 895, 3
Watkins, C. J. C. H., \& Dayan, P. 1992, Mach. Learn., 8, 279
Woodcock, F. 1976, MWRv, 104, 1209
Xiao, H., Rasul, K., \& Vollgraf, R. 2017, arXiv:1708.07747
Yi, K., Moon, Y.-J., Lim, D., Park, E., \& Lee, H. 2021, ApJ, 910, 8
Zheng, Y., Li, X., \& Wang, X. 2019, ApJ, 885, 73"
Jihyeon Son et al 2025 - Six-hour Prediction of Interplanetary Magnetic Field Bz Profiles for Strong Southward Cases by Deep Learning.pdf,"# Six-hour Prediction of Interplanetary Magnetic Field $B_{z}$ Profiles for Strong Southward Cases by Deep Learning 

Jihyeon Son ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), Young-Sil Kwak ${ }^{3,4}$, Kyung Sun Park ${ }^{5}$, and Hyun-Jin Jeong ${ }^{2,6}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, Republic of Korea<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, Republic of Korea<br>${ }^{3}$ Division of Fundamental Astronomy \& Space Science, Korea Astronomy and Space Science Institute, Daejeon, Republic of Korea<br>${ }^{4}$ University of Science and Technology, Daejeon, Republic of Korea<br>${ }^{5}$ Department of Astronomy and Space Science, Chungbuk National University, Cheongju, Republic of Korea<br>${ }^{6}$ Centre for mathematical Plasma Astrophysics, Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium<br>Received 2024 October 18; revised 2025 March 19; accepted 2025 March 20; published 2025 April 28


#### Abstract

In this study, we develop deep learning models to forecast the 6 hr interplanetary magnetic field (IMF) $B_{z}$ component for southward cases. The models are based on a bidirectional long short-term memory method, and input parameters are solar wind data $(V, N, T)$ and IMF components $\left(B_{l}, B_{x}, B_{y}, B_{z}\right)$. The data are obtained from OMNI, whose period is from 2000 to 2022. We use the preceding 12 hr of data as input and the subsequent 6 hr of $B_{z}$ data as target. To focus on strong geomagnetic conditions, we consider periods where $B_{z}$ values drop below the negative standard deviation (approximately -3 nT ) for at least 6 hr . The models are trained and validated using a 12 -fold cross-validation process, with each model trained over 8 months of data and tested over 4 months. The ensemble model, which averages 12 -fold model results, achieves an RMSE ranging from 1.75 ( 30 minutes prediction) to 2.55 nT ( 6 hr prediction), significantly outperforming two baseline methods: multilayer perceptron and multiple linear regression. Our model can capture both decreasing and increasing phases of $B_{z}$, showing reliable performance across varying geomagnetic conditions. Our results suggest a sufficient possibility for predicting $B_{z}$ under noticeable southward conditions. We expect that our model can be used for subsequent space weather predictions, such as global magnetohydrodynamic simulations in the magnetosphere.


Unified Astronomy Thesaurus concepts: The Sun (1693); Space weather (2037); Interplanetary magnetic fields (824); Solar wind (1534); Neural networks (1933)

## 1. Introduction

Interplanetary magnetic field (IMF) refers to the Sun's magnetic field carried by the solar wind into the interplanetary space within the solar system. Among its components, the southward component of the IMF, denoted as $B_{z}$, plays a crucial role in space weather phenomena near the Earth. It determines how much energy from the solar wind is transferred to the Earth's magnetosphere through magnetic reconnection (J. W. Dungey 1961; R. L. Arnoldy 1971). This process is closely associated with geomagnetic storms, which can significantly affect satellites, navigation systems, communication networks, and power grids on Earth. Therefore, accurately predicting $B_{z}$ is essential for space weather alerts. However, despite its importance, predicting of the IMF $B_{z}$ near Earth remains a challenge due to the highly complex nature of solar winds.

Historically, various approaches have been employed to address the problem of $B_{z}$ prediction. Physical and empirical models, which utilized statistical correlations between solar wind parameters and geomagnetic activity indices, have been widely used, mainly for $B_{z}$ directions (J. Chen et al. 1996; N. P. Savani et al. 2015; D. Shiota \& R. Kataoka 2016; C. Möstl et al. 2017). For example, X. Zhao \& J. T. Hoeksema (1995) developed a method using the Current Sheet Source Surface model to extrapolate magnetic fields from the solar

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
surface to Earth, allowing for a basic forecast of the IMF components, including $B_{z}$. In recent years, advanced datadriven and machine learning techniques have significantly improved $B_{z}$ forecasting (P. Riley et al. 2017; B. Jackson et al. 2019). For instance, M. A. Reiss et al. (2021) explored the use of machine learning models trained on in situ measurements to predict the minimum $B_{z}$ component for a certain period and the maximum $B_{t}$ values embedded within interplanetary coronal mass ejections. To our knowledge, there has been no study to forecast the $B_{z}$ profile.

Deep learning, a branch of machine learning, excels at solving nonlinear problems by utilizing multiple layers of neural networks. Due to its ability to model complex nonlinear relationships, deep learning has been increasingly applied to space weather prediction tasks (Y.-J. Moon et al. 2022, and references therein). These approaches demonstrate potential for improving prediction accuracy, particularly in time series forecasting.

Our preliminary investigation on the forecasting of $B z$ profiles for several days shows that is so challenging, mostly impossible. Thus we focus on the short-term forecasting of $B z$ components for only noticeable southward cases, i.e., when $B_{z}$ falls below a certain threshold of approximately -3 nT , about the standard deviation of $B_{z}$ in our data set. Our goal is to explore the feasibility of using deep learning to predict $B_{z}$ during these critical periods.

This study represents the first attempt to forecast the profiles of $B z$. We choose a prediction target of 30 minute intervals over a 6 hr period due to the following reasons. In global magnetohydrodynamic (MHD) models in the magnetosphere,
![img-0.jpeg](img-0.jpeg)

**Figure 1.** Method for constructing the data set. The periods where *B<sub>z</sub>* remains below –std for at least 6 hr are identified. These periods are segmented into 6 hr intervals, represented by the dashed boxes, which serve as the target data. The sliding window approach is employed to shift these segments forward in time with a 30 minute window.

The solar wind data with high time resolution, typically covering several hours, are used as inputs (J. Lyon et al. 2004; G. Tóth et al. 2005). As we plan to use the output of our model as the input of global MHD model, it is necessary to align our approach with this requirement. Predicting the *BZ* profiles over longer periods is challenging due to the increasing complexity of changes in IMF *B<sub>z</sub>*. Similarly, higher time resolutions tend to introduce much more fluctuations, making accurate predictions more difficult. Therefore, we select a 6 hr prediction window with a 30 minute time resolution, which is empirically determined by several trials.

This paper is organized as follows: In Sections 2 and 3, we describe the data and methodology used, respectively. In Section 4, we present the results of our model and discuss them. Finally, a brief summary and conclusion are given in Section 5.

## 2. Data

We obtain the solar wind and IMF data from OMNIWeb (https://omniweb.gsfc.nasa.gov/), which provides time-shifted data to the Earth's bow shock nose, based on in situ measurements from Lagrangian point 1 satellites. Our model uses solar wind speed (*V*), density (*N*), temperature (*T*), IMF strength (|*B*|), and the components of the IMF (*B<sub>x</sub>*, *B<sub>y</sub>*, and *B<sub>z</sub>*). These parameters serve as input data, structured into sequences of 24 time points, corresponding to 12 hr of measurements averaged over 30 minute intervals. The target data consist of the IMF *B<sub>z</sub>* for the next 6 hr, averaged over 30 minute intervals. The IMF components are expressed in geocentric solar magnetospheric (GSM) coordinates, as IMF *B<sub>z</sub>* in GSM coordinates is known to be strongly associated with geomagnetic storms (W. D. Gonzalez & B. T. Tsurutani 1987). The total period of the data we use is from 2000 to 2022.

Based on our preliminary investigation and other studies, we realize that predicting all *B<sub>z</sub>* values is challenging due to the natural tendency of *B<sub>z</sub>* to converge toward an average of zero. This tendency causes deep learning models to similarly converge their predictions toward zero. This makes it difficult for the model to accurately predict strong southward components, which are of particular interest. To address this issue, we employ a new strategy, focusing on cases where *B<sub>z</sub>* values drop below a specific threshold, defined as the negative standard deviation (–std) of *B<sub>z</sub>*, approximately –3 nT. This threshold is determined as a reasonable compromise to balance the availability of the number of data and the need to focus on significant geomagnetic conditions.

Figure 1 illustrates the methodology used to construct the data set. We consider cases where the IMF *B<sub>z</sub>* remains below –std for at least 6 hr. These periods are segmented into 6 hr intervals, which are used as target data for our model. Each dashed box in Figure 1 represents one such segment of the target data. The sliding window approach is applied to shift these 6 hr segments forward in time with a 30 minute window. For example, if the event starts at 00:00 and lasts 8 hr, the data set will include overlapping intervals such as 00:00–06:00, 00:30–06:30, and 01:00–07:00, etc. This process results in a total of 4360 data pairs. While some overlap exists between adjacent 6 hr intervals, each interval represents a unique temporal context. For example, the intervals 00:00–06:00 and 00:30–06:30 contain some common data points, but they represent different temporal patterns. Although individual data points may appear in multiple intervals, the focus of the models is on learning the temporal dynamics rather than memorizing
![img-1.jpeg](img-1.jpeg)

Figure 2. Model architecture used in this study. The parentheses indicate the number of nodes of each layer.

Individual values. Thus, overlapping data points do not compromise the independence of each interval as the model perceives them as separate samples. This sliding window approach has been widely employed by several time series forecasting studies (K. Yi et al. 2020; A. Ji & B. Aydin 2023; X. Sun et al. 2024).

## 3. Methodology

### 3.1. Deep Learning Model

The long short-term memory (LSTM) network (S. Hochreiter & J. Schmidhuber 1997) was designed to address the issue of long-term dependencies that conventional recurrent neural networks (RNNs) face. LSTM networks are widely used in tasks such as language translation, speech recognition, and time series prediction. It has also been employed for space weather prediction tasks by several studies (Y. Tan et al. 2018; K. Yi et al. 2020; H. Zhang et al. 2022). However, both simple RNNs and LSTMs have a limitation: they tend to produce results that are heavily influenced by prior patterns, as they process input sequentially. To overcome this limitation, A. Graves & J. Schmidhuber (2005) suggested bidirectional LSTM (BiLSTM). A BiLSTM network consists of two LSTM layers, one processing the input sequence in the forward direction and the other in the backward direction. This bidirectional processing enables the model to better capture the contextual information in the data. BiLSTM networks have demonstrated improved performance across various tasks by capturing dependencies from both past and future contexts (A. Graves et al. 2013; A. Hu & K. Zhang 2018). In our examinations, the results of BiLSTM are much better than LSTM for our study.

In this study, we implement a model with two consecutive BiLSTM layers. Each BiLSTM layer consists of 128 hidden units and is followed by a rectified linear unit activation function (V. Nair & G. E. Hinton 2010), as shown in Figure 2. The input data of our model consist of sequences with a length of 24 and 7 features, and the output layer produces predicted *B<sub>z</sub>* values for the next 12 time steps, corresponding to the next 6 hr with a 30 minutes cadence. The model has a total of 536,588 trainable parameters. For training, we utilize the Adam (D. P. Kingma & J. Ba 2014) optimizer with a learning rate of 0.0001 and a batch size of 32. The loss function used is the mean squared error (MSE), which minimizes the average squared difference between the predicted and observed values. Although the model has a high parameter-to-data ratio, we carefully monitor the training and validation loss throughout the training process, and no significant increase in validation loss is observed. This indicates that the model has converged without overfitting. In addition, the models are trained for 100 epochs without early stopping. Since validation loss remains stable after an initial decrease, we conclude that continuing training 100 epochs is appropriate for optimal performance. To further validate this choice, we conduct additional training up

Table 1 Comparison of RMSE Values for BiLSTM Model and Two Baseline Models (MLP and MLR) over the Prediction Times

|  Prediction Time | BiLSTM | MLP | MLR  |
| --- | --- | --- | --- |
|  30 minutes | 1.75 | 1.63 | 1.51  |
|  1 hr | 1.92 | 2.02 | 2.03  |
|  1 hr 30 minutes | 2.07 | 2.22 | 2.31  |
|  2 hr | 2.05 | 2.44 | 2.58  |
|  2 hr 30 minutes | 2.10 | 2.63 | 2.78  |
|  3 hr | 2.12 | 2.82 | 2.97  |
|  3 hr 30 minutes | 2.18 | 2.96 | 3.08  |
|  4 hr | 2.25 | 3.05 | 3.16  |
|  4 hr 30 minutes | 2.29 | 3.14 | 3.22  |
|  5 hr | 2.30 | 3.18 | 3.23  |
|  5 hr 30 minutes | 2.35 | 3.24 | 3.26  |
|  6 hr | 2.55 | 3.27 | 3.30  |

**Note.** Bold values indicate when our model performs better than other models.
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison of the RMSE of the BiLSTM model and two baseline models (MLP and MLR) over the prediction time intervals.

To 600 epochs. Our results show that further training does not lead to noticeable improvements in model performance, confirming that 100 epochs are appropriate for achieving convergence. Nonetheless, we acknowledge that implementing an adaptive early stopping strategy could provide a more systematic approach to determining the optimal stopping point and further optimize the training process. To further enhance the robustness of the models, incorporating dropout and other regularization techniques could be considered in future work. The model code will be available in our GitHub repository (https://github.com/Jihyeon-ing/Bz_prediction).

### 3.2. K-fold Validation

Due to the insufficient data sets for training the deep learning model, we employ the K-fold validation method. K-fold validation is a technique that divides the data set into K parts (or ""folds""). For each iteration of training, one of the K folds is used as the test set, while the remaining folds are used as the training set. This process is repeated K times, with each fold being used exactly once as the test set. This method not only maximizes the use of available data but also helps to reduce the risk of overfitting and ensures that the model is robust and generalizes well.

Taking into solar cycle effects, we divide the entire data set as follows: 8 months of data for training and the remaining 4 months for testing. Additionally, the last 10% of the training set are further separated as the validation set to monitor the model's performance model during training. For example, in the first fold, data from January to August of each year are used as the training set, with the last 10% of this data designated as validation set. The remaining data from September to December of each year are used as the test set. In the subsequent fold, the split shifts forward by 1 month: the training set consists of data from February to September of every year, and the test set includes data from October to January of every year. This process is repeated across 12 folds.

Consequently, we train 12 independent models, each corresponding to a different fold in the K-fold cross-validation process. In each iteration, each of these models is trained and tested independently on different data splits. The final performance is evaluated by aggregating the results from all 12 models. This repeated training ensures that our model is tested against a variety of data splits, thereby enhancing its reliability and accuracy. To guarantee robust model evaluation and avoid data leakage, we further employ a monthly data exclusion and ensemble strategy. For each month, we exclude its data from the training set and trained models using the remaining months (e.g., for January predictions, models are trained on February–September, March–October, etc.). The final prediction for each month is obtained by ensembling the outputs of four models trained on different data splits. This strategy validates that the target month's data remain unseen during training, thus effectively validating the model's generalization ability.

### 4. Results and Discussion

To evaluate the performance of our proposed model, we use the root MSE (RMSE) given by

$$
\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{N} (X_i - Y_i)^2}{N}}
$$

The RMSE is compared against two baseline models: multiple linear regression (MLR) and multilayer perceptron (MLP). RMSE measures the average magnitude of the error between the predicted and observed values. The MLR is a linear model that predicts outcomes (Y = {y1, y2, ..., yn}) based on a linear combination of input features (X = {xi, xi1, ..., xin}). It is expressed as

$$
y_i = a_{1,i}x_1 + a_{2,i}x_2 + ... + a_{n,i}x_n + b_i,
$$

where a_{i}, a_{i} denotes the coefficient of the nth term for the linear function at time t and b_{i} denotes the intercept for the linear
![img-3.jpeg](img-3.jpeg)

**Figure 4.** Results of our model predictions across three geomagnetic conditions: (a) moderate case, (b) strong case, and (c) extreme case. The green solid line indicates the target data, and the red solid line shows the average prediction from our model ensemble of 12 individual models. The red shaded area represents the prediction range. The red dashed vertical line marks the prediction time onset.

The Astrophysical Journal, 984:67 (7pp), 2025 May 1

**Figure 4.** Results of our model predictions across three geomagnetic conditions: (a) moderate case, (b) strong case, and (c) extreme case. The green solid line indicates the target data, and the red solid line shows the average prediction from our model ensemble of 12 individual models. The red shaded area represents the prediction range. The red dashed vertical line marks the prediction time onset.

The astrophysical journal, 984:67 (7pp), 2025 May 1

**Figure 5.** The astrophysical time *t*. MLP is a type of neural network that consists of multiple layers of nodes, where each layer applies nonlinear activation functions to learn complex patterns. Unlike MLR, MLP can capture nonlinear relationships between inputs and outputs by utilizing hidden layers and backpropagation for training.

It is noted that the RMSE values shown here are averaged values of the ensemble model, over 12-fold results. As depicted in both the Table 1 and Figure 3, our BiLSTM model consistently outperforms the baseline models across most of the prediction intervals. For shorter prediction times, such as 30 minutes, the BiLSTM achieves an RMSE of 1.75 nT, which is slightly higher than that of the MLR (1.51 nT) but lower than the MLP (1.63 nT). However, as the prediction time passes, the performance of our model becomes more prominent. The RMSE increases more gradually for the BiLSTM model compared to the baseline models, particularly at longer prediction times. By the 6 hr prediction mark, the BiLSTM model maintains an RMSE of 2.55 nT, significantly better than the baseline model results: the MLR (3.30 nT) and MLP (3.27 nT). The consistent outperformance suggests that the BiLSTM model effectively captures the nonlinear dynamics inherent in solar wind data, which the baseline models fail to do.

We present the results of our model in three cases: a moderate case (B<sub>z</sub> ≈ −10 nT), a strong case (B<sub>z</sub> ≈ −25 nT), and an extreme condition (B<sub>z</sub> ≈ −50 nT; Figure 4). The red dashed line indicates the prediction time, and the green solid line indicates the target data. The results of the ensemble model are represented by the red solid line, with the shaded area indicating the prediction range. For each plot, only the models whose test sets overlap with the target period are included. The predictions of these overlapping models are averaged to generate the final results shown in Figure 4.

In the moderate condition, the B<sub>z</sub> component remains relatively stable with minor fluctuations. Our model closely tracks the observed B<sub>z</sub> values throughout the prediction period. The small deviations in the predictions are well made within acceptable limits, indicating that the model is effective at capturing the slight variations typical of quiet periods. In the strong case, where the B<sub>z</sub> component exhibits a more pronounced decline, our model continues to track the target closely, with slightly larger but still well-contained prediction intervals. The extreme condition presents a more challenging variation to predict, characterized by sharp declines and significant fluctuations in the B<sub>z</sub> component. Despite the increased complexity, the model demonstrates a strong ability to capture the overall downward trend. However, an error of approximately 20 nT at the peak reflects a limitation of the deep learning model. This is likely due to the rarity of such extreme cases in the training data. Nevertheless, the broader uncertainty range observed here points to the difficulty of predicting highly unstable conditions, though the model performs well in most other periods.

Overall, these results suggest that the model effectively captures information from the input solar wind parameters, including the previous near-Earth information just before, enabling the model to perform well even without direct data from the solar source region. The BiLSTM's architecture appears particularly well suited to modeling the nonlinear dependencies within the data, contributing to its robust performance across varying space weather conditions.

We evaluate our model using a separate test data set from the years 2023–2024, which is not used during training or validation in any of our models. This data set serves as a truly out-of-sample evaluation to assess the generalization performance of our model. The evaluation results are presented as boxplots in Figure 5. The central line inside each box represents the median, which offers a robust measure of central tendency. The box itself covers the interquartile range (IQR), representing the middle 50% of the data, while the whiskers extend to the minimum and maximum values within 1.5 times the IQR. The size of the box
![img-4.jpeg](img-4.jpeg)

Figure 5. Prediction performance of our model (red) compared to the baseline MLP model (blue) on an independent test data set from 2023 to 2024. (a) RMSE, (b) MAE, and (c) Pearson CC are shown for each prediction time.

indicates the variability of the predictions: smaller boxes represent more consistent predictions. These boxplots are the results from all 12 independent models, providing a comprehensive view of the model's performance across the multiple data splits. Our model consistently outperforms the baseline MLP across all prediction times in terms of RMSE, mean absolute error (MAE), and correlation coefficient (CC). While some variability remains, the superior median performance indicates that our model effectively captures complex temporal patterns in the data. These results confirm that our model generalizes well to unseen data.

The findings of this study have a potential applicability to predicting geomagnetic indices such as Dst, AE, and Kp indices. These indices can be expressed in terms of solar wind speed and southward component of IMF (R. K. Burton et al. 1975; I. Y. Plotnikov & E. Barkova 2007; R. Boroyev & M. Vasiliev 2018). We plan to integrate our model with the solar wind speed prediction model (J. Son et al. 2023) to develop a more comprehensive tool for geomagnetic index forecasting. Such an integrated model could provide earlier and more accurate warnings of geomagnetic storms. Additionally, our model could be applied to global empirical magnetic field models (e.g., N. Tsyganenko 2014) and MHD simulation models (e.g., K. S. Park 2021) of the geomagnetosphere. If the predicted *B<sub>z</sub>* values from our model are used as input data for these models, they would help in predicting the changes in the Earth's magnetosphere. These applications are left for our future work.

## 5. Conclusion

In this study, we have developed a deep learning model to predict the strong southward component of the IMF *B<sub>z</sub>* for the next 6 hr, particularly under conditions where *B<sub>z</sub>* values become negative. We have implemented a BiLSTM method using solar wind parameters (*V*, *N*, *T*) and IMF components (*B<sub>t</sub>*, *B<sub>x</sub>*, *B<sub>y</sub>*, *B<sub>z</sub>*) as input data. Our model's performance is compared to two baseline models: the MLR and MLP models. The results indicate that our model has a much better performance in view of the RMSE values than those of the baseline models. The RMSE of our model ranges from 1.75 (for 30 minute predictions) to 2.55 nT (for 6 hr predictions). Additionally, the model can track the downward trends of *B<sub>z</sub>* during not only moderate conditions but also strong and extreme conditions, highlighting its potential for short-term predictions.

Despite these promising results, one limitation of our study is that our model is trained and tested only on *B<sub>z</sub>* values below a certain threshold, specifically focusing on more negative trends. As a result, the model has not yet been validated for broader operational use in near-real-time space weather forecasting, where accurate predictions across the full range of *B<sub>z</sub>* values are needed. Nevertheless, it is important to emphasize that this research can be a good starting point in the development of deep learning models for *B<sub>z</sub>* prediction. Our study demonstrates the feasibility of using the deep learning method to predict *B<sub>z</sub>*.

Predicting the *B<sub>z</sub>* component is crucial for space weather forecasting as it plays a key role in driving geomagnetic storms that can affect various systems. By advancing the capability to predict *B<sub>z</sub>*, our research contributes to the broader goal of improving space weather forecasting, thereby enhancing preparedness and minimizing the potential impacts of space weather events.

## Acknowledgments

This work was supported by the Korea Astronomy and Space Science Institute under the R&D program (Project No. 2024-1-850-02), supervised by the Ministry of Science and ICT (MSIT), and by the Institute for Information and Communications Technology Promotion (IITP) grant, funded by the Korea government MSIT (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15%). It was also supported by the BK21 FOUR program through the National Research Foundation of Korea (NRF) under Ministry of Education (MoE; Kyung Hee University, Human Education Team for the Next Generation of Space Exploration) and by the NRF grant funded by the Korea government (MSIT; RS-2024-00346061), as well as the Basic Science Research Program through the NRF funded by the MoE (RS-2023-00248916). This research is (partially) funded by the BK21 FOUR program of Graduate School, Kyung Hee University (GS-1-JO-NON-20242364). We acknowledge the use of NASA/GSFC's Space Physics Data Facility's OMNIWeb service and OMNI data. We also thank the community for their efforts in developing the open-source packages utilized in this work.

## ORCID iDs

- Jihyeon Son https://orcid.org/0000-0003-2678-5718
- Yong-Jae Moon https://orcid.org/0000-0001-6216-6944
- Hyun-Jin Jeong https://orcid.org/0000-0003-4616-947X
## References

Arnoldy, R. L. 1971, JGR, 76, 5189
Boroyev, R., \& Vasiliev, M. 2018, AdSpR, 61, 348
Burton, R. K., McPherron, R., \& Russell, C. 1975, JGR, 80, 4204
Chen, J., Cargill, P. J., \& Palmadesso, P. J. 1996, GeoRL, 23, 625
Dungey, J. W. 1961, PhRvL, 6, 47
Gonzalez, W. D., \& Tsurutani, B. T. 1987, P\&SS, 35, 1101
Graves, A., Jaitly, N., \& Mohamed, A.-r. 2013, in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (Piscataway, NJ: IEEE), 273
Graves, A., \& Schmidhuber, J. 2005, NN, 18, 602
Hochreiter, S., \& Schmidhuber, J. 1997, Neural Computation, 9, 1735
Hu, A., \& Zhang, K. 2018, RemS, 10, 1658
Jackson, B., Yu, H.-S., Buffington, A., et al. 2019, SpWea, 17, 639
Ji, A., \& Aydin, B. 2023, in 2023 IEEE Int. Conf. on Big Data (BigData) (Piscataway, NJ: IEEE), 1519
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Lyon, J., Fedder, J., \& Mobarry, C. 2004, JASTP, 66, 1333
Moon, Y.-J., Lee, H., Son, J., et al. 2022, in IAU Symp. 372, The Era of MultiMessenger Solar Physics, ed. G. Cauzzi \& A. Tritschler (Cambridge: Cambridge Univ. Press), 131

Möstl, C., Isavnin, A., Boakes, P., et al. 2017, SpWea, 15, 955
Nair, V., \& Hinton, G. E. 2010, in Proc. 27th Int. Conf. on Machine Learning (ICML-10) (Madison, WI: Omnipress), 807, https://dl.acm.org/citation. cfm?id=3104322.3104425
Park, K. S. 2021, FrASS, 8, 758241
Plotnikov, I. Y., \& Barkova, E. 2007, AdSpR, 40, 1858
Reiss, M. A., Möstl, C., Bailey, R. L., et al. 2021, SpWea, 19, e2021SW002859
Riley, P., Ben-Nun, M., Linker, J. A., Owens, M. J., \& Horbury, T. 2017, SpWea, 15, 526
Savani, N. P., Vourlidas, A., Szabo, A., et al. 2015, SpWea, 13, 374
Shiota, D., \& Kataoka, R. 2016, SpWea, 14, 56
Son, J., Sung, S.-K., Moon, Y.-J., Lee, H., \& Jeong, H.-J. 2023, ApJS, 267, 45
Sun, X., Wang, D., Drozdov, A., et al. 2024, JSWSC, 14, 25
Tan, Y., Hu, Q., Wang, Z., \& Zhong, Q. 2018, SpWea, 16, 406
Tóth, G., Sokolov, I. V., Gombosi, T. I., et al. 2005, JGRA, 110, A12226
Tsyganenko, N. 2014, JGRA, 119, 335
Yi, K., Moon, Y.-J., Shin, G., \& Lim, D. 2020, ApJL, 890, L5
Zhang, H., Xu, H., Peng, G., et al. 2022, SpWea, 20, e2022SW003126
Zhao, X., \& Hoeksema, J. T. 1995, JGRA, 100, 19"
Sujin Lee et al 2020 - One-Day Forecasting of Global TEC Using a Novel Deep Learning Model.pdf,"# Space Weather 

## RESEARCH ARTICLE

10.1029/2020SW002600

## Key Points:

- We make a global total electron content (TEC) 1-day forecasting using a deep learning model based on conditional generative adversarial networks
- Our model shows better performance than 1-day Center for Orbit Determination in Europe prediction model during the solar maximum and solar minimum periods
- We successfully apply our model to the forecast of global TEC maps using only previous TEC data


## Correspondence to:

Y.-J. Moon,
moonyj@khu.ac.kr

## Citation:

Lee, S., Ji, E.-Y., Moon, Y.-J., \& Park, E. (2021). One-day forecasting of global TEC using a novel deep learning model. Space Weather, 19, e2020SW002600. https://doi.org/10.1029/2020SW002600

Received 3 AUG 2020
Accepted 5 DEC 2020

## One-Day Forecasting of Global TEC Using a Novel Deep Learning Model

Sujin Lee ${ }^{1}$ (D) Eun-Young Ji ${ }^{1}$ (D), Yong-Jae Moon ${ }^{1,2}$ (D), and Eunsu Park ${ }^{1}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, South Korea, ${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, South Korea


#### Abstract

In this study, we make a global total electron content (TEC) forecasting using a novel deep learning method, which is based on conditional generative adversarial networks. For training, we use the International GNSS Service (IGS) TEC maps from 2003 to 2012 with 2-h time cadence. Our model has two input images (IGS TEC map and 1-day difference map between the present day and the previous day) and one output image (1-day future map). The model is tested with two data sets: solar maximum period (2013-2014) and solar minimum period (2017-2018). Then, we compare the results of our model with those of 1-day Center for Orbit Determination in Europe (CODE) prediction model. Our major results can be summarized as follows. First, we successfully apply our model to the forecast of global TEC maps. Second, our model well predicts daily TEC maps with 1 day in advance using only previous TEC maps. The averaged root mean square error, bias, and standard deviation between AI-generated and IGS TEC maps are 2.74 TECU, -0.32 TECU, and 2.59 TECU, respectively. Third, our model generates some peak structures around equatorial regions. Fourth, our model shows better performance than 1-day CODE prediction model during both solar maximum and minimum periods. Fifth, another model with additional input data Kp index gives a slight improvement of the results. Our study shows that our deep learning model based on an image translation method will be effective for forecasting of future images using previous data.


Plain Language Summary The main causes of ionospheric disturbance are solar activity and geomagnetic activity. The total electron content (TEC) is one of the important parameters of ionosphere and it can be used to investigate ionospheric disturbances. We develop a global TEC 1-day forecasting model using a novel deep learning method. For training, we use the International GNSS Service TEC maps from 2003 to 2012 which include both solar maximum and minimum periods. Our model successfully forecasts the global TEC maps 1 day in advance.

## 1. Introduction

The total electron content (TEC) is the total number of electrons along a path between a radio transmitter and a receiver. The unit of TEC (TECU) is defined as $10^{16}$ electrons $/ \mathrm{m}^{2}$, and 1 TECU corresponds to a 0.163 m range delay on L1 frequency. TEC is used as one of the major parameters in space weather and an indicator of ionospheric disturbance. The accurate measurement of TEC provides us with useful information on satellite communications, navigation, national defense, and aviation.

The International Global Navigation Satellite Systems (GNSS) Service (IGS) TEC maps are calculated based on measured observational data at more than 380 IGS stations. They have been used as a reference data for comparison together with JASON satellite data (e.g., Hernández-Pajares, 2004; Jee et al., 2010; Ji et al., 2016; M. Li et al., 2018; Wang et al., 2018). These data have been made by the Ionosphere Working Group of the International GNSS Service (Iono-WG) since 1998 (Feltens \& Schaer, 1998), which are currently four IGS Ionosphere Associate Analysis Centers (IAACs): the Center for Orbit Determination in Europe (CODE), the Jet Propulsion Laboratory (JPL), the European Space Operations Center of European Space Agency (ESA/ESOC), and the Polytechnical University of Catalonia (UPC). IGS produces a rapid TEC and final TEC. Rapid TEC has a latency of less than 24 h , but its accuracy is about $10 \%-15 \%$ lower than the final TEC (Hernández-Pajares, 2004). The final TEC maps are more reliable and accurate than the individual IAACs maps and internal consistency of all IGS products (Feltens, 2003; Hernández-Pajares et al., 2009). However, it has a latency of $\sim 11$ days.
During the last few decades, global TEC forecasting models have been developed. For example, the UPC developed a global TEC prediction model, which is based on the discrete cosine transform and linear regression module (García-Rigo et al., 2011). The model predicts a global TEC map 2 days in advance. The Space Weather Application Center Ionosphere (SWACI) provide a global TEC forecasting model 1-h in advance (Jakowski et al., 2011). Their model is based on Neustrelitz TEC model, which is a polynomial consisting of linear terms. The CODE developed the global TEC prediction models, which are based on the extrapolation of spherical harmonic expansion referring to a solar-geomagnetic frame. They have 1-day and 2 days predictions of global TEC data (Schaer, 1999). Currently, these models provide global TEC forecasts on their web site. The data is obtained from the following website: UPC and CODE (ftp://cddis.nasa.gov/gnss/products/ionex/), SWACI (https://swaciweb.dlr.de/). In view of forecasting method, these approaches mostly are based on extrapolation methods or models with a physical concept (Feltens, 2003; Hernández-Pajares et al., 2009; Z. Li et al., 2015; Mannucci et al., 1993).

Recently, many studies based on deep learning, a subset of machine learning based on artificial neural networks, have been conducted in various fields. One of deep learning algorithms, Pix2Pix is a general solution for image-to-image translation problems (Isola et al., 2017). It consists of a conditional generative adversarial networks (cGAN) and a deep convolutional generative adversarial network (DCGAN). The cGAN is an improvement model of the generation model called GAN, which attaches condition to the generator and discriminator (Mirza \& Osindero, 2014). DCGAN is a deep convolution added to the generative adversarial networks (GAN), which extracts the characteristics of image and make an image similar to the input image (Radford et al., 2015).

There have been several attempts to translate images using Pix2Pix model in the field of astronomy and space science. Kim et al. (2019) successfully generated solar magnetograms from Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) images. Then they applied the model to STEREO/ Extreme UltraViolet Imager (EUVI) 304-Å images to produce farside solar magnetograms. Park et al. (2019) successfully generated solar ultraviolet (UV) and extreme UV (EUV) images from solar magnetograms. Ji et al. (2020) developed an improved IRI model by applying this method to translation from IRI TEC maps to IGS TEC maps. These studies demonstrate that such an image translation method can be successfully applied to scientific research.

The global TEC forecasting models using deep learning also show good accuracies. For example, Perez (2019) presented a global TEC forecast model based on a multilayer perceptron. The model used six input parameters such as solar flux, Kp index, day of year, time of day, longitude, and latitude. This model can predict the global TEC 1 day or more days ahead using predicted solar flux and Kp data. Cesaroni et al. (2020) developed a global TEC forecast model based on a nonlinear autoregressive network with exogenous inputs (NARX), which is characterized by its great power to analyze nonlinear time series data. Their model used the NeQuick2 model (Nave et al., 2008) parameters and an effective sunspot number R12 calculated with NARX. They showed that the model well predicts global TEC maps during the five selected geomagnetic storms. These models used TEC data with additional parameters (solar and geomagnetic activity index) as input data.

In this paper, we develop a global TEC map forecasting model using a novel deep learning method with conditional GAN, which has been successfully applied to different kinds of image translations. Our model is 1-day prediction model which is proper for the application of image translation deep learning models. We train our model using only the IGS final TEC maps (hereafter called IGS TEC map) without additional data such as solar and geomagnetic activity index. Then, we evaluate the results of our model using several metrics and compare them with 1-day CODE prediction model. This paper is organized as follows. The data are described in Section 2. The method is explained in Section 3. Results are presented in Section 4. A brief summary and conclusion are given in Section 5.

# 2. Data 

The IGS TEC data are obtained from the NASA's Crustal Dynamics Data Information System (CDDIS) website (ftp://cddis.nasa.gov/gnss/products/ionex/). It has a resolution $5^{\circ} \times 2.5^{\circ}$ in the geographic longitude and latitude ( 73 by 71 ). We perform the data preprocessing as follows: (1) we remove the overlapping values
![img-0.jpeg](img-0.jpeg)

Figure 1. The architecture of our model.
on the dateline so that we make 72 by 71 images, (2) we make them 128 by 128 images for computation by adding proper padding data (same data for left and right sides, and zero values for upper and lower sides). We also make the difference TEC maps, which are the difference between the present day IGS TEC maps and the previous day data. We use the daily IGS TEC maps and 1-day difference maps as input data of our model. Output data are 1-day future maps. For training, we use the 2-h resolution TEC maps from 2003 to 2012 ( 43,800 images) covering solar maximum and minimum. For test, we use two data sets: solar maximum period ( 8,736 images) from 2013 to 2014 and solar minimum period ( 8,736 images) from 2017 to 2018.

# 3. Method 

Figure 1 shows the main structure of our model. The model consists of two networks: one is a generator and the other is the discriminator. The purpose of the generator is to produce fake images similar to the output images. The purpose of the discriminator is to distinguish between real images and fake images. To train our model, we use three objectives (loss) function. The function of $L_{1}$ is expressed as:

$$
L_{1}(G)=E_{x, y}(\|y-G(x)\|)
$$

where $G$ is the generator, $x, y$, and $G(x)$ are input, target, and output by generator, respectively. The role of this objective is to minimize the error which is the absolute value differences between the target and output by generator. The function of $L_{\mathrm{cGAN}}$ is expressed as:

$$
L_{\mathrm{cGAN}}(G, D)=E_{x, y}[\log D(x, y)]+E_{x}[\log (1-D(x, G(x))]
$$

where $D$ is the discriminator. $D(x, y)$ is the discriminator's estimation of the probability that the real pair is real. $D(x, G(x))$ is the discriminator's estimation of the probability that the fake pair is real. $D$ calculates the probability that the image pair belongs to 0 (fake pair) or 1 (real pair). $D$ tries to maximize the objective while the $G$ tries to minimize it. As a result, $D$ and $G$ play a min-max game. Our final objective $\left(G^{*}\right)$ is expressed as:

$$
G^{*}=\arg \min _{G} \max _{D} L_{\mathrm{cGAN}}(G, D)+\lambda L_{1}(G)
$$

where $\lambda$ is the relative weight of $L_{\mathrm{cGAN}}$ loss and $L_{1}$ loss. In this study, we use 100 for the relative weight as Isola et al. (2017) used. We initialize the weights as follows: Convolution layers and Convolution-Transpose layers using a normal distribution with 0.0 mean and 0.02 standard deviation, Batch-Normalization layers using a normal distribution with 1.0 mean and 0.02 standard deviation (Isola et al., 2017). We use the
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparison between IGS TEC maps and the results of our model at 02 UT on October 1, 2013. (a) input image, (b) target image (+24 h IGS TEC), (c) output image generated by our model, (d) difference map (between [b and c]).

ADAM solver as the optimizer with a learning rate of $2 \times 10^{-4}$, momentum $\beta_{1}$ of 0.5 , and momentum $\beta_{2}$ of 0.999 (Isola et al., 2017; Kingma and Ba, 2014). We train our model 1,000,000 iterations for each of the solar maximum and solar minimum. Considering the CGAN loss and the statistical index, the best iteration of the solar maximum period is 800,000 iterations, and the solar minimum period is 760,000 iterations.

# 4. Results and Discussion 

Figure 2 shows a good example among from our model, which includes an input image, the corresponding target image ( +24 h IGS TEC), output image generated by our model, and the difference between the last two images at 02 UT on October 1, 2013. The Kp index when the input image was taken is 0 , and the Kp index when the target image was taken is 5 which corresponds to a geomagnetic storm. The output image is well consistent with the target one, IGS TEC. Especially, our model successfully produces the peak structures in the equatorial region. The root mean square error (RMSE), bias (BIAS), and standard deviation (STD) between the output image and the target one are 2.92 TECU, -0.52 TECU, and 2.87 TECU, respectively.

Figure 3 shows a bad example among from our model, which includes an input image, the corresponding target image ( +24 h IGS TEC), output image generated by our model, and the difference between the last two images at 02 UT on February 28, 2013. The Kp index when the input image was taken is 2 , and the Kp index when the target image was taken is 5 . The results of our model does not well generate peak structures in the equatorial region. The RMSE, BIAS, and STD between the output image and the target one are 4.43 TECU, -1.69 TECU, and 4.09 TECU, respectively.
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison between IGS TEC maps and the results of our model at 02 UT on February 28, 2013. (a) input image, (b) target image ( +24 h IGS TEC), (c) output image generated by our model, (d) difference map (between [b and c]).
Table 1
The Average RMSE, BIAS, and STD Values Between Forecasting Models (Ours and 1-Day CODE) and the IGS TEC During the Test Periods

|  | Our model—IGS TEC |  |  | 1-day CODE—IGS TEC |  |  |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: |
|  | RMSE | BIAS | STD | RMSE | BIAS | STD |
| All test data | 2.74 | -0.32 | 2.59 | 3.04 | -0.78 | 2.81 |
| Solar maximum | 3.72 | -0.46 | 3.52 | 4.12 | -0.75 | 3.91 |
| Solar minimum | 1.74 | -0.17 | 1.66 | 1.95 | -0.81 | 1.72 |

Even though the Kp values at the target time in Figures 2 and 3 are the same, the prediction accuracy is somewhat different. We examine the temporal variations of Kp index for two cases. The first case shows that Kp decreases and then increases, and the second case shows its steady increase. The performance of a deep learning model is largely affected by the training data. In the training data, there are many cases showing the trend of change in the Kp index that increases and decreases or decreases and increases for 3 days. On the other hand, there are not many cases where Kp index continues to increase or decrease for 3 days. Therefore, the first case is much more common than the second case, which means that the model can reproduce the first pattern better than the second.

We compare our results with those of 1-day CODE prediction model. CODE is one of the IAAC and is making global TEC prediction using GNSS data. Li et al. (2018) showed that the predicted global ionosphere maps (GIM) generated at CODE are better performance than those generated at UPC and ESA. For this study, we use the 1-day CODE predicted TEC values taken from the NASA's Crustal Dynamics Data Information System (CDDIS) website (ftp://cddis.nasa.gov/gnss/products/ionex/).

Table 1 shows the averaged RMSE, BIAS, and STD between output TEC maps generated by our model and IGS TEC ones as well as between 1-day CODE predicted TEC and IGS TEC ones. We consider three test data sets: all test data (2013-2014 and 2017-2018), solar maximum period (2013-2014), and solar minimum period (2017-2018). Table 1 shows that the results of our model are better than those of the 1-day CODE prediction model for all test data. The results of our model are also better than those of 1-day CODE prediction model not only during both solar maximum period and solar minimum period.

TEC has regular changes in diurnal, seasonal, annual, 27-day, and solar cycle variations (Liu \& Chen, 2009). Figure 4 shows the monthly averaged RMSE, BIAS, and STD between output TEC maps generated by our model and IGS TEC ones as well as between 1-day CODE predicted TEC and IGS TEC ones. Figure 4 shows that all our results are better than those of CODE 1-day prediction model except for two data points, which were taken in March and April 2014. These results may be related to the period of training data. In March and April 2014, solar activity is the most active during the solar cycle 24. Our training data has been included since 2003 so that our model has not been well trained for the most active case of solar cycle.

Considering the geomagnetic activity affecting the ionosphere, we make another model with additional input data, Kp index. We compare two models, one is Model A without Kp index and the other is Model B with Kp index. Table 2 shows the averaged RMSE and BIAS between output TEC maps generated by model A and B and IGS TEC ones. We consider three test data sets: all test data (2013-2014 and 2017-2018), solar maximum period (2013-2014), and solar minimum period (2017-2018). Table 2 shows that the results of Model B are slightly better than those of the Model A for all test data. As shown in the results, Model B is better. But it is noted that the Kp index used for Model B is not provided in real time. One may wonder why Model A is not bad even though we did not use Kp data. We guess that our input data ( 2 days TEC) already contain the effects of solar and geomagnetic activity. We expect that Model B can be used by using preliminary Kp data in the future.

## 5. Conclusion

In this paper, we have developed a global TEC 1-day forecasting model using deep learning based on cGAN. For training, we use the IGS TEC maps from 2003 to 2012 covering solar maximum and minimum. The model is tested with two data sets: solar maximum period (2013-2014) and solar minimum period (2017-2018).

Our major results are as follows. First, we successfully apply our deep learning model to the forecast of global TEC maps. Second, our model well predicts daily TEC maps using only previous TEC maps, without additional data such as solar and geomagnetic activity index. The averaged RMSE, BIAS, and STD between output TEC maps generated by our model and IGS TEC ones are 2.74 TECU, -0.32 TECU, and 2.59 TECU, respectively. Third, our model generates some peak structures in the equatorial regions, but not all equatorial anomalies are well predicted. Fourth, in view of the three metrics (RMSE, BIAS, and STD), the results of
![img-3.jpeg](img-3.jpeg)

Figure 4. The monthly averaged RMSE, BIAS, and STD values between forecasting models (ours and 1-day CODE) and the IGS TEC during the test periods. (a) RMSE (solar maximum), (b) RMSE (solar minimum), (c) BIAS (solar maximum), (d) BIAS (solar minimum), (e) STD (solar maximum), (f) STD (solar minimum).

Table 2
The Average RMSE and BIAS Values Between Forecasting Models (Models A and B) and the IGS TEC During the Test Periods

|  | Model A |  | Model B |  |
| :-- | :--: | :--: | :--: | :--: |
|  | RMSE | BIAS | RMSE | BIAS |
| All test data | 2.74 | -0.32 | 2.68 | -0.02 |
| Solar maximum | 3.72 | -0.46 | 3.63 | -0.08 |
| Solar minimum | 1.74 | -0.17 | 1.72 | 0.03 |

our model are better than those of 1-day CODE prediction model during both solar maximum and solar minimum periods. Fifth, another model with additional input data Kp index gives a slight improvement of the results.

Our study demonstrates that our deep learning model based on an image translation method will be effective for forecasting future images using previous data. This method could be applied to the forecasting of images in other scientific fields. It is difficult to forecast IGS TEC maps in real time using our model since they have a latency of 11 days. Recently, Ji et al. (2020) developed an improved version of IRI TEC maps, which is called DeepIRI, by deep learning. This model can generate a global TEC
# Data Availability Statement 

This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2019R1A2C1002634, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2020-1850-07)"" supervised by the Ministry of Science and ICT, and Institute for Information \& Communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-001422, Study on analysis and prediction technique of solar flares).
map in near real time if input parameters of IRI are properly predicted. We hope that our global model will be able to predicted in near real time with DeepIRI properly forecasted.

## References

Cesaroni, C., Spogli, L., Aragon-Angel, A., Fiocca, M., Dear, V., De Franceschi, G., \&(2020). Neural network based model for global Total Electron Content forecasting. Journal of Space Weather and Space Climate, 10, 11. https://doi.org/10.1051/swsc/2020013
Feltens, J. (2003). The activities of the Ionosphere Working Group of the International GPS Service (IGS). GPS Solutions, 7, 41-46. https:// doi.org/10.1007/s10291-003-0051-9
Feltens, J., \& Schaer, S. (1998). IGS products for the ionosphere, IGS position paper. In proceedings of the 1998 IGS Analysis Center Workshop, 225-232.
Garcia-Rigo, A., Monte, E., Hernandez-Pajares, M., Juan, J. M., Sanz, J., Aragon-Angel, A., \&(2011). Global prediction of the vertical total electron content of the ionosphere based on GPS data. Radio Science, 46, 1-3. https://doi.org/10.1029/2010RS004643
Hernández-Pajares, M. (2004). IGS Ionosphere WG Status Report: Performance of IGS Ionosphere TEC Maps-Position Paper. IGS Workshop, Barcelona, Spain.
Hernández-Pajares, M., Juan, J. M., Sanz, J., Orus, R., Garcia-Rigo, A., Feltens, J., et al. (2009). The IGS VTEC maps: A reliable source of ionospheric information since 1998. Journal of Geodesy, 83, 263-275. https://doi.org/10.1007/s00190-008-0266-1
Isola, P., Zhou, J. Y., Zhou, T., \& Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1125-1134.
Jakowski, N., Mayer, C., Hoque, M. M., \& Wilken, V. (2011). Total electron content models and their use in ionosphere monitoring. Radio Science, 46, 1-11. https://doi.org/10.1029/2010RS004620
Jee, G., Lee, H. B., Kim, Y. H., Chung, J. K., \& Cho, J. (2010). Assessment of GPS global ionosphere maps (GIM) by comparison between CODE GIM and TOPEX/Jason TEC data: Ionospheric perspective. Journal of Geophysical Research, 115, A10319. https://doi. org/10.1029/2010JA015432
Ji, E. Y., Jee, G., \& Lee, C. (2016). Comparison of IRI-2012 with JASON-1 TEC and incoherent scatter radar observations during the 20082009 solar minimum period. Journal of Atmospheric and Solar-Terrestrial Physics, 146, 81-88. https://doi.org/10.1016/j.jastp.2016.05.010
Ji, E. Y., Moon, Y. -J., \& Park, E. (2020). Improvement of IRI global TEC maps by deep learning based on conditional Generative Adversarial Networks. Space Weather, 18, e2019SW002411. https://doi.org/10.1029/2019SW002411
Kim, T., Park, E., Lee, H., Moon, Y.-J., Bae, S.-H., Lim, D., et al. (2019). Solar farside magnetograms from deep learning analysis of STEREO/EUVI data. Nature Astronomy, 3, 397-400. https://doi.org/10.1038/s41550-019-0711-5
Kingma, D. P., \& Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Li, M., Yuan, Y., Wang, N., Li, Z., \& Huo, X. (2018). Performance of various predicted GNSS global ionospheric maps relative to GPS and JASON TEC data. GPS Solutions, 22, 55. https://doi.org/10.1007/s10291-018-0721-2
Li, Z., Yuan, Y., Wang, N., Hernández-Pajares, M., \& Huo, X. (2015). SHPTS: Towards a new method for generating precise global ionospheric TEC map based on spherical harmonic and generalized trigonometric series functions. Journal of Geodesy, 89, 331-245. https:// doi.org/10.1007/s00190-014-0778-9
Liu, L., \& Chen, Y. (2009). Statistical analysis of solar activity variation of total electron content derived at Jet Propulsion Lavoratory from GPS observations. Journal of Geophysical Research, 114, A10. https://doi.org/10.1029/2009JA014533
Mannucci, A. J., Wilson, B. D., \& Edwards, C. D. (1993). A new method for monitoring the Earth's ionospheric total electron content using the GPS global network. In Proceedings of ION GPS-93, Institute of Navigation, Salt Lake City, UT, pp. 1323-1332.
Mirza, M., \& Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784.
Nava, B., Coisson, P., \& Radicella, S. M. (2008). A new version of the NeQuick ionosphere electron density model. Journal of Atmospheric and Solar-Terrestrial Physics, 70, 1856-1862. https://doi.org/10.1016/j.jastp.2008.01.015
Park, E., Moon, Y.-J., Lee, J.-Y., Kim, R.-S., Lee, H., Lim, D., et al. (2019). Generation of solar UV and EUV images from SDO/HMI magnetograms by deep learning. The Astrophysical Journal Letters, 884, L23. https://doi.org/10.3847/2041-8213/ab46bb
Perez, R. O. (2019). Using TensorFlow-based Neural Network to estimate GNSS single frequency ionospheric delay (IONONet). Advances in Space Research, 63, 1607-1618. https://doi.org/10.1016/j.asr.2018.11.011
Radford, A., Metz, L., \& Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
Schaer, S. (1999). Mapping and predicting the Earth's ionosphere using the global positioning system (PhD thesis dissertation). Bern, Switzerland: Univ. of Bern.
Wang, C., Xin, S., Liu, X., Shi, C., \& Fan, L. (2018). Prediction of global ionospheric VTEC maps using an adaptive autoregressive model. Earth Planets and Space, 70, 1-14. https://doi.org/10.1186/s40623-017-0762-8"
Mingyu Jeon et al 2025 - Real-time Extrapolation of Nonlinear Force-free Fields from Photospheric Vector Magnetic Fields Using a Physics-informed Neural Operator.pdf,"# Real-time Extrapolation of Nonlinear Force-free Fields from Photospheric Vector Magnetic Fields Using a Physics-informed Neural Operator 

Mingyu Jeon ${ }^{1}$ (D) Hyun-Jin Jeong ${ }^{2,1}$ (D), Yong-Jae Moon ${ }^{1,3}$ (D), Jihye Kang ${ }^{3}$ (D), and Kanya Kusano ${ }^{4}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Centre for mathematical Plasma Astrophysics, KU Leuven, Leuven, Belgium<br>${ }^{3}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, Republic of Korea<br>${ }^{4}$ Institute for Space-Earth Environmental Research, Nagoya University, Nagoya, Japan<br>Received 2024 October 20; revised 2025 February 19; accepted 2025 February 25; published 2025 April 1


#### Abstract

In this study, we develop a physics-informed neural operator (PINO) model that learns the solution operator from 2D photospheric vector magnetic fields to 3D nonlinear force-free fields (NLFFFs). We train our PINO model using physics loss from NLFFF partial differential equations, as well as data loss from target NLFFFs. We validate our method using an analytical NLFFF model. We then train and evaluate our PINO model with 2327 numerical NLFFFs of 211 active regions from the Institute for Space-Earth Environmental Research database. The results show that our trained PINO model can generate an NLFFF within 1 s for any active region on a single consumer GPU, making real-time extrapolation of NLFFFs possible. Our artificial intelligence (AI)-generated NLFFFs are qualitatively and quantitatively quite similar to the target NLFFFs for 30 active regions. The magnetic energy of the AI-generated NLFFFs of active region 11158 follows a similar trend to the target NLFFFs as well as other conventional methods.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); Solar active regions (1974); The Sun (1693); Neural networks (1933)

## 1. Introduction

Solar coronal magnetic fields play a key role in solar eruptions such as solar flares and coronal mass ejections (P. F. Chen 2011; K. Shibata \& T. Magara 2011). These energetic events can cause adverse space weather impacts on telecommunications and satellites around the Earth (M. Temmer 2021; J. Zhang et al. 2021). It is essential to understand the 3D coronal magnetic fields, but high spatiotemporal resolution measurements of vector magnetic fields are currently restricted to the photosphere (T. Wiegelmann et al. 2017). Nonlinear force-free field (NLFFF) extrapolations have been widely used for coronal magnetic field modeling, where we assume that nonmagnetic forces are negligible and the Lorentz force vanishes because the corona is a low plasma $\beta$ region (T. Wiegelmann et al. 2017; T. Wiegelmann \& T. Sakurai 2021). This requires solving partial differential equations (PDEs), defined as

$$
\begin{gathered}
(\nabla \times \boldsymbol{B}) \times \boldsymbol{B}=\mathbf{0} \\
\nabla \cdot \boldsymbol{B}=0
\end{gathered}
$$

where $\boldsymbol{B}$ is a 3D NLFFF. We use an observed photospheric vector magnetic field as a bottom boundary condition. The various methods, such as optimization methods (M. S. Wheatland et al. 2000; T. Wiegelmann et al. 2012) and magnetohydrodynamic (MHD) relaxation methods (Z. Mikić \& A. N. McClymont 1994; S. Inoue et al. 2013), have been suggested for NLFFF extrapolation.

Although an NLFFF represents a static magnetic field, it is possible to approximately track the temporal evolution of

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
coronal magnetic fields by calculating a series of NLFFFs from a temporal sequence of observed photospheric vector magnetic fields. By analyzing a series of NLFFFs, K. Kusano et al. (2020) introduced the $\kappa$-scheme, a physics-based flare prediction model through MHD instability triggered by magnetic reconnection. This suggests that real-time computing of a series of NLFFFs would be useful for understanding and predicting solar eruptions. However, conventional methods are time consuming; they typically take several hours to compute a single high-resolution NLFFF. A new method is needed for real-time extrapolation of high-resolution 3D coronal magnetic fields of active regions (ARs).

Recently, deep-learning methods have been used for the reconstruction of 3D magnetic fields in the solar corona (for a review, see A. Asensio Ramos et al. 2023). R. Jarolim et al. (2023) applied physics-informed neural networks (PINNs; M. Raissi et al. 2019) to calculate NLFFFs of ARs from photospheric magnetic fields, and R. Jarolim et al. (2024) extended this approach to use both photospheric and chromospheric magnetic fields via height-mapping models. They used a transfer-learning approach so that the computation time of the NLFFF for the first time step was about 1 hr , but the NLFFF of the next time step was calculated within 1 minute. The PINNs need to be retrained for new ARs because the inputs are coordinates in the computational domain. S. Rahman et al. (2024) generated global 3D solar coronal magnetic field, radial velocity, and temperature using pix2pixCC (H.-J. Jeong et al. 2022). They used synoptic maps of photospheric magnetic fields as inputs and simulation data of the MHD algorithm outside a sphere (J. A. Linker et al. 1999; Z. Mikić et al. 1999) as targets. Since the inputs are preprocessed observation data, they can use trained models for near real-time computation of the global 3D solar coronal parameters.

In this study, we consider NLFFF extrapolation as a parametric PDE problem where the parameter is a photospheric
![img-0.jpeg](img-0.jpeg)

Figure 1. Overview of our method. The white and black in the images indicate positive and negative polarities, respectively, with the color map range defined as ±(1000 G)/(z + 1). The neural operator, shown as an orange box, is a U-shaped neural operator with six Fourier layers. Here, L_{mse} is a mean-squared error loss, L_{cc} is a correlation coefficient loss, L_{ff} is a force-free loss, L_{div} is a divergence-free loss, and L_{bc} is a bottom boundary condition loss.

vector magnetic field. We use a physics-informed neural operator (PINO) model (Z. Li et al. 2024) for real-time extrapolation of NLFFFs using physics losses provided by PDEs (1) and (2), as well as data loss. Our PINO model can learn the solution operator from 2D photospheric vector magnetic fields to 3D NLFFFs. To validate our method, we train and evaluate our PINO model with a semianalytical NLFFF model (B. C. Low & Y. Q. Lou 1990). Then, we train and evaluate our PINO model with numerical NLFFFs from the Institute for Space-Earth Environmental Research (ISEE) database (K. Kusano et al. 2021) for real-time extrapolations of NLFFFs of solar ARs. The performance of our trained PINO may degrade when applied to ARs with out-of-distribution properties that differ significantly from those in the ISEE data set. We explain our method in Section 2. In Section 3, we describe the data used for our model. In Section 4, we show the results and discuss them. We summarize our study in Section 5.

## 2. Method

### 2.1. Physics-informed Neural Operator

We use a PINO model (Z. Li et al. 2024) for real-time extrapolation of NLFFFs from photospheric vector magnetic fields. Neural operators refer to neural networks that can learn nonlinear operators of parametric PDE problems (N. B. Kovachki et al. 2021; S. A. Faroughi et al. 2024). The two main neural operator architectures are Deep Operator Network (DeepONet), introduced by L. Lu et al. (2021, 2022), and Fourier Neural Operator (FNO), proposed by Z. Li et al. (2020). DeepONet and FNO were independently developed with different viewpoints on operator learning, but both show comparable performance for relatively simple problems (L. Lu et al. 2022). Although neural operators typically depend solely on input–output pairs for training, incorporating physics information into them offers the combined benefits of neural operators (e.g., real-time computation through inference without retraining) and PINNs (e.g., compatibility with underlying physics through regularization of physics-informed loss) (G. E. Karniadakis et al. 2021; S. Wang et al. 2021; S. Goswami et al. 2023; S. A. Faroughi et al. 2024; Z. Li et al. 2024). We calculate physics loss based on PDEs (1) and (2), as well as data loss using pairs of our artificial intelligence (AI)-generated NLFFFs and target ones.

Figure 1 shows an overview of our method. The architecture of our PINO model is a U-shaped neural operator architecture (M. A. Rahman et al. 2022) with six Fourier layers implemented in the neural operator library (Z. Li et al. 2020; N. B. Kovachki et al. 2021). It has useful features like enabling deeper neural operators, fast training, data efficiency, and robustness across various hyperparameter selections (M. A. Rahman et al. 2022). Both the lifting block and projection block have 256 hidden channels. The initial width of the U-shaped neural operator is 32. The numbers of output channels for each Fourier layer are 32, 64, 64, 64, 64, and 32. The numbers of Fourier modes are 16, 8, 8, 8, 8, and 16. The scaling factors are 1.0, 0.5, 1.0, 1.0, 2.0, and 1.0. Details about neural operator theory and the U-shaped neural operator can be found in M. A. Rahman et al. (2022) and Z. Li et al. (2024) and references therein.

Our PINO model generates a 3D NLFFF b(x, y, z) with a shape N_{x} × N_{y} × N_{z} × 3 from a photospheric vector magnetic field B_{obs}(x, y) with a shape N_{x} × N_{y} × 1 × 3, where 3 represents the number of vector components, 1 is the number of input channels, and N_{z} is the number of output channels. We put the bottom boundary on the z = 0 plane and the top boundary on the z = N_{z} − 1 plane. Since the magnetic field
strength decreases with the height from the bottom boundary, we normalize the target field $\boldsymbol{B}$ by dividing it by a factor $B_{0} /(z+1)$, which is empirically determined by several trials. Here, $B_{0}$ is a typical magnetic field strength of an AR, and we use $B_{0}=200 \mathrm{G}$ for the Low and Lou NLFFFs (B. C. Low \& Y. Q. Lou 1990) and $B_{0}=2500 \mathrm{G}$ for the ISEE NLFFFs (K. Kusano et al. 2021). This scaling is applied only for the calculation of data losses; it does not apply when calculating physics losses to ensure accurate derivative calculations.

Our PINO model uses data loss and physics loss during training. The data loss is calculated by comparing our AI-generated field $\boldsymbol{b}$ with the target field $\boldsymbol{B}$ using a meansquared error loss $L_{\text {mse }}$ and a correlation coefficient loss $L_{\mathrm{cc}}$, which is a mean-squared error between 1 and the concordance correlation coefficient. We calculate the data losses for each layer of a constant $z$ and average them. For the physics loss, we calculate the PDE loss and bottom boundary condition loss. The PDE loss is calculated using the force-free loss $L_{\mathrm{ff}}$ and the divergence-free loss $L_{\text {div }}$, which are computed using the PDEs (1) and (2), respectively. The derivatives are calculated using a second-order finite difference scheme. We assume a uniform grid spacing of length 1 . The bottom boundary condition loss $L_{\mathrm{bc}}$ is a mean-squared error between output data $\boldsymbol{b}(x, y, 0)$ and input data $\boldsymbol{B}_{\text {obs }}(x, y)$. The formulae for the losses are as follows:

$$
\begin{gathered}
L_{\mathrm{mse}}=\frac{1}{N_{z}} \sum_{z}\left\langle|\boldsymbol{b}(z)-\boldsymbol{B}(z)|^{2}\right\rangle \\
L_{\mathrm{cc}}=\frac{1}{N_{z}} \sum_{z}\left\langle\left|1-\operatorname{cc}(\boldsymbol{b}(z), \boldsymbol{B}(z))\right|^{2}\right\rangle \\
L_{\mathrm{ff}}=\left\langle\frac{|(\nabla \times \boldsymbol{b}) \times \boldsymbol{b}|^{2}}{|\boldsymbol{b}|^{2}+\epsilon}\right\rangle \\
L_{\mathrm{div}}=\left\langle|\nabla \cdot \boldsymbol{b}|^{2}\right\rangle \\
L_{\mathrm{bc}}=\left\langle\left|\boldsymbol{b}(z=0)-\boldsymbol{B}_{\text {obs }}\right|^{2}\right\rangle
\end{gathered}
$$

where $\langle\cdot\rangle$ denotes the average over the grid points, and $\epsilon=10^{-7}$ is a small number to avoid division by zero. To calculate the concordance correlation coefficient, we flatten the magnetic field data $\boldsymbol{b}(z)$ and $\boldsymbol{B}(z)$ for each layer of a constant $z$ and apply the following formula:

$$
\operatorname{cc}(x, y)=\frac{2 \rho \sigma_{x} \sigma_{y}}{\sigma_{x}^{2}+\sigma_{y}^{2}+\left(\mu_{x}-\mu_{y}\right)^{2}}
$$

where $\sigma_{x}$ and $\sigma_{y}$ are the corresponding standard deviations, $\mu_{x}$ and $\mu_{y}$ are the corresponding means, and $\rho$ is the Pearson correlation coefficient between the two variables $x$ and $y$, given by $\rho(x, y)=\operatorname{cov}(x, y) /\left(\sigma_{x} \sigma_{y}\right)$, where $\operatorname{cov}(x, y)$ is the covariance of the two variables. We use torchmetrics (N. S. Detlefsen et al. 2022) to calculate the data losses.

The total loss is the weighted sum of these losses:

$$
L=w_{\mathrm{mse}} L_{\mathrm{mse}}+w_{\mathrm{cc}} L_{\mathrm{cc}}+w_{\mathrm{ff}} L_{\mathrm{ff}}+w_{\mathrm{div}} L_{\mathrm{div}}+w_{\mathrm{bc}} L_{\mathrm{bc}}
$$

where we use $w_{\mathrm{mse}}=1, w_{\mathrm{cc}}=0.01, w_{\mathrm{ff}}=100, w_{\mathrm{div}}=100$, and $w_{\mathrm{bc}}=10$. These values are empiricially determined after several trials. The effect of changing the weights of physics losses is discussed for the semianalytical Low and Lou field (B. C. Low \& Y. Q. Lou 1990) in Section 4. To minimize the total loss, we use an Adam optimizer (D. P. Kingma \& J. Ba 2014) with a learning rate of $10^{-5}$. We use a batch size of one, and each batch consists of a pair comprising single vector
magnetic field data on the bottom boundary and the entire volume data of the corresponding 3D NLFFF. The total number of epochs is set to 100 . At the end of each epoch, we calculate the loss using the validation set and select the model with the smallest validation loss across the entire epoch. The composition of the training, validation, and test sets is described in Section 3. Training and inference are performed on an NVIDIA RTX 4070 Ti GPU. The source code used in this study is available at https://github.com/mgjeon/rtmag.

### 2.2. Evaluation Metrics

We use the metrics described in M. S. Wheatland et al. (2000) and C. J. Schrijver et al. (2006) to evaluate the performance of our PINO model. The magnetic field vectors at grid point $i$ are represented as $\boldsymbol{b}_{i}$ for our AI-generated field and $\boldsymbol{B}_{i}$ for the target field. The symbol $N$ denotes the total number of grid points in the computational volume. The following five comparison metrics compare our AI-generated field $\boldsymbol{b}$ with the target field $\boldsymbol{B}$ :

$$
\begin{gathered}
C_{\mathrm{vec}}=\frac{\sum_{i} \boldsymbol{b}_{i} \cdot \boldsymbol{B}_{i}}{\left(\sum_{i}\left|\boldsymbol{b}_{i}\right|^{2} \sum_{i}\left|\boldsymbol{B}_{i}\right|^{2}\right)^{1 / 2}} \\
C_{\mathrm{CS}}=\frac{1}{N} \sum_{i} \frac{\boldsymbol{b}_{i} \cdot \boldsymbol{B}_{i}}{\left|\boldsymbol{b}_{i}\right|\left|\boldsymbol{B}_{i}\right|} \\
E_{n}=\frac{\sum_{i}\left|\boldsymbol{b}_{i}-\boldsymbol{B}_{i}\right|}{\sum_{i}\left|\boldsymbol{B}_{i}\right|} \\
E_{m}=\frac{1}{N} \sum_{i} \frac{\left|\boldsymbol{b}_{i}-\boldsymbol{B}_{i}\right|}{\left|\boldsymbol{B}_{i}\right|} \\
\epsilon=\frac{\sum_{i}\left|\boldsymbol{b}_{i}\right|^{2}}{\sum_{i}\left|\boldsymbol{B}_{i}\right|^{2}}
\end{gathered}
$$

where $C_{\text {vec }}$ is the vector correlation metric, $C_{\mathrm{CS}}$ is the CauchySchwartz metric, $E_{n}$ is the normalized vector error, $E_{m}$ is the mean vector error, and $\epsilon$ is the ratio of the magnetic energy of our AI-generated field $\boldsymbol{b}$ to the target field $\boldsymbol{B}$. We use $E_{n}^{\prime}=1-E_{n}$ and $E_{m}^{\prime}=1-E_{m}$ such that all comparison metrics have a value of 1 when $\boldsymbol{b}$ and $\boldsymbol{B}$ are identical.

The following two quality metrics measure the force freeness and divergence freeness of a field, $\boldsymbol{B}$ :

$$
\begin{gathered}
\sigma_{\boldsymbol{J}}=\left(\sum_{i} \frac{\left|\boldsymbol{J}_{i} \times \boldsymbol{B}_{i}\right|}{\left|\boldsymbol{B}_{i}\right|}\right) /\left(\sum_{i}\left|\boldsymbol{J}_{i}\right|\right) \\
\langle | \boldsymbol{J}_{i} \mid\rangle=\frac{1}{N} \sum_{i} \frac{\mid \nabla \cdot \boldsymbol{B}_{i} \mid \Delta V_{i}}{\left|\boldsymbol{B}_{i}\right| A_{i}}
\end{gathered}
$$

where $\boldsymbol{J}=\nabla \times \boldsymbol{B} / 4 \pi$ is the current density in Gaussian units, $\sigma_{J}$ is the current-weighted average of the sine of the angle between the magnetic field and current density, and $\langle\left|f_{i}\right|\rangle$ is the average value of the magnitude of the fractional flux increase with a surface area $A_{i}$ of a small volume $\Delta V_{i}$ around grid point $i$ (M. S. Wheatland et al. 2000). For a rectangular uniform grid with cell sizes $\delta x, \delta y$, and $\delta z$ in each direction, the volume $\Delta V_{i}$ is given by $\Delta V_{i}=\delta x \delta y \delta z$, and the surface area $A_{i}$ is given by $A_{i}=2\left(\delta x \delta y+\delta y \delta z+\delta z \delta x\right)$. The quality metrics $\sigma_{J}$ and $\left\langle\left|f_{i}\right|\right\rangle$ have a value of 0 when $\boldsymbol{B}$ is perfectly force free and divergence free.
Table 1
Quantitative Comparison between Target NLFFFs and AI-generated NLFFFs

|  | $C_{\text {vec }}$ | $C_{\text {CS }}$ | $E_{\mathrm{n}}^{\prime}$ | $E_{\mathrm{m}}^{\prime}$ | $\epsilon$ | $\sigma_{J}$ | $\left\langle\left|f_{i}\right|\right\rangle$ |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Test case for semianalytical NLFFFs $(l=0.3, \Phi=0.25 \pi)$ | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.02 | $3.3 \times 10^{-5}$ |
| AI generated $\left(w_{\mathrm{ff} / \mathrm{div}}=100\right)$ | 1.00 | 0.98 | 0.91 | 0.81 | 1.01 | 0.24 | $5.8 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{ff}}=1000\right)$ | 1.00 | 0.96 | 0.89 | 0.77 | 1.01 | 0.21 | $7.4 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{ff}}=10\right)$ | 1.00 | 0.98 | 0.91 | 0.83 | 1.01 | 0.26 | $5.4 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{div}}=1000\right)$ | 1.00 | 0.96 | 0.89 | 0.78 | 1.01 | 0.25 | $4.4 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{div}}=10\right)$ | 1.00 | 0.98 | 0.91 | 0.83 | 1.02 | 0.24 | $6.2 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{ff} / \mathrm{div}}=0\right)$ | 1.00 | 0.98 | 0.92 | 0.87 | 1.01 | 0.29 | $5.8 \times 10^{-3}$ |
| AI $\left(w_{\mathrm{ff} / \mathrm{div}}=100\right)+$ optimization method | 1.00 | 0.96 | 0.87 | 0.71 | 1.00 | 0.08 | $2.4 \times 10^{-3}$ |
| Test set for numerical NLFFFs (483 NLFFFs of 30 ARs) | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 0.83 | $4.3 \times 10^{-4}$ |
| AI generated $\left(w_{\mathrm{ff} / \mathrm{div}}=100\right)$ | 0.96 | 0.94 | 0.70 | 0.58 | 0.93 | 0.85 | $5.3 \times 10^{-3}$ |
| AI generated $\left(w_{\mathrm{ff} / \mathrm{div}}=0\right)$ | 0.94 | 0.93 | 0.67 | 0.55 | 0.90 | 0.84 | $7.5 \times 10^{-3}$ |
| AI $\left(w_{\mathrm{ff} / \mathrm{div}}=100\right)+$ optimization method | 0.97 | 0.94 | 0.72 | 0.58 | 0.96 | 0.47 | $1.5 \times 10^{-3}$ |

Note. The comparison metrics $C_{\text {vec }}, C_{\mathrm{CS}}, E_{\mathrm{n}}^{\prime}, E_{\mathrm{m}}^{\prime}$, and $\epsilon$ of AI-generated NLFFFs equal to 1 when the target and AI-generated NLFFFs are identical. The quality metrics $\sigma_{J}$ and $\left\langle\left|f_{i}\right|\right\rangle$ are equal to 0 when the field is perfectly force free and divergence free. The first rows of each section show the metrics of the target NLFFFs, and the other rows show the metrics of the AI-generated NLFFFs.

## 3. Data

For the analytical test case, we use the Low and Lou semianalytical NLFFF model (B. C. Low \& Y. Q. Lou 1990). The field is determined by four parameters: $n, m, l$, and $\Phi$. We use the Low and Lou NLFFFs with $n=1$ and $m=1$. We split training, validation, and test sets based on $l$ and $\Phi$. We select the Low and Lou NLFFF with $l=0.3$ and $\Phi=0.25 \pi$ as a test field. We calculate 6000 Low and Lou NLFFFs with randomly selected $l$ and $\Phi$ parameters with the ranges $l \in[0.15,0.25) \cup[0.35,0.45)$ and $\Phi \in[0,0.2 \pi) \cup[0.3 \pi, \pi)$, ensuring that the test field is excluded from the training and validation sets. These fields are randomly divided into 5000 fields for the training set and 1000 fields for the validation set. These fields have a resolution of $N_{x} \times N_{y} \times N_{z}=$ $64 \times 64 \times 64$.

To train our PINO model for more complex NLFFFs, we use 2327 numerical NLFFFs from the ISEE database for NLFFFs of solar ARs (K. Kusano et al. 2021), which consists of 211 ARs from 2010 to 2022. These NLFFFs are not entirely independent of each other because NLFFFs extrapolated from the same ARs at different time steps are included. This database includes 3D NLFFFs extrapolated from photospheric vector magnetic fields by the MHD relaxation method (S. Inoue et al. 2013), with a resolution of $N_{x} \times N_{y} \times N_{z}=512 \times 256 \times 256$. The photospheric vector magnetic fields are obtained from the Space weather Helioseismic and Magnetic Imager (HMI; P. H. Scherrer et al. 2011) Active Region Patch (SHARP) data remapped to a Lambert cylindrical equal-area (CEA) projection (M. G. Bobra et al. 2014). The bottom boundary data for the ISEE NLFFFs are the SHARP data only resized to $N_{x} \times N_{y}=512 \times 256$ using the IDL CONGRID function, regardless of the size of the ARs, and without any further preprocessing described in T. Wiegelmann et al. (2006). The physical size of each pixel varies for NLFFFs from different ARs in the ISEE database. Since this data set uses only SHARP data, we require vector magnetic field data processed according to the SHARP pipeline for the application of our trained PINO model. The physical sizes of the ARs range from $78 \mathrm{Mm} \times 44 \mathrm{Mm}$ (AR 11736) to $943 \mathrm{Mm} \times 413 \mathrm{Mm}$ (AR 12242). The length in the $x$-direction has a mean of 346 Mm , a median of 314 Mm , and a standard deviation of 139 Mm . The length in the $y$-direction has a mean of 168 Mm , a median of 160 Mm , and a standard deviation of 60 Mm . We
split the NLFFFs of the ISEE data set into training, validation, and test sets based on ARs because the NLFFFs of the same AR are not independent. We include AR 11158 and 12673 in the test set, and the other ARs are manually selected to ensure that the training, validation, and test sets cover the similar time period. Our training set consists of 1795 NLFFFs of 151 ARs (from AR 11078 to 12975), and our validation and test sets contain 49 NLFFFs of 30 ARs (from AR 11846 to 12975) and 483 NLFFFs of other 30 ARs (from AR 11158 to 12673), respectively. The list of ARs for the data sets is provided in the Appendix. The number of NLFFFs for each AR varies from one to 390 . For more details, please refer to the ISEE database (K. Kusano et al. 2021). For data augmentation, we rotate the NLFFF by $180^{\circ}$ around the $z$-axis. To do that, we reverse the order of the NLFFF data array along the $x$ - and $y$-axis using the torch.flip function and change the sign of the $x$ - and $y$-components of the NLFFF.

## 4. Results and Discussion

Table 1 presents the quantitative comparison between the target NLFFFs and the AI-generated NLFFFs. The target NLFFFs include the test case for semianalytical NLFFFs and the test set for numerical NLFFFs. The test case for semianalytical NLFFFs is the Low and Lou semianalytical field (B. C. Low \& Y. Q. Lou 1990) with $l=0.3$ and $\Phi=0.25 \pi$, which is not used to train the PINO model. The test set for numerical NLFFFs consists of 483 numerical NLFFFs of 30 ARs (from AR 11158 to 12673) in the ISEE database (K. Kusano et al. 2021). We select the model based on the loss given by Equation (9) for the validation set, not based on the metrics given by Equations (10)-(16). The AI-generated NLFFFs are the output data of our trained PINO model. One PINO model is used for the semianalytical fields and the other one for the numerical fields. For the metrics of numerical fields, we first calculate the average metrics for each AR and then take the overall average of these individual averages for all ARs.

The semianalytical field section of Table 1 shows that the comparison metrics $C_{\text {vec }}, C_{\mathrm{CS}}, E_{\mathrm{n}}^{\prime}, E_{\mathrm{m}}^{\prime}$, and $\epsilon$ of our AI-generated field are $1.00,0.98,0.91,0.81$, and 1.01 , respectively. These values are quite close to 1 , demonstrating that our trained PINO model can generate an NLFFF that closely matches the target semianalytical NLFFF. However, the
![img-1.jpeg](img-1.jpeg)

Figure 2. EUV observations of AR 11158 and projected magnetic field lines from different extrapolation methods. The color of the field lines indicates the magnitude of the vertical current density in the photosphere. EUV observations in 171 and 94 Å are shown in (a) and (b), respectively. (c) Target magnetic field of the ISEE NLFFF database. (d) AI-generated magnetic field of our trained PINO model. (e) Magnetic field of the optimization method (T. Wiegelmann et al. 2012). (f) Magnetic field of the PINN method (λff/div = 0.1; R. Jarolim et al. 2023).

Quality metrics σ<sup>J</sup> and ⟨|f<sup>i</sup><sub>l</sub>⟩ are 0.24 and 5.8 × 10<sup>−3</sup>, respectively, which are not as good compared to the values of 0.02 and 3.3 × 10<sup>−5</sup> for the target semianalytical field. This indicates that, even with physics-informed losses, ensuring the force-freeness and divergence-freeness of NLFFFs generated by a single PINO model is challenging. We also investigate the effect of varying the weights of the physics losses, w<sub>ff</sub>, and w<sub>div</sub>. The default values are w<sub>ff</sub> = 100 and w<sub>div</sub> = 100. We vary either w<sub>ff</sub> or w<sub>div</sub> to 1000 or 10 while keeping all other weights fixed. As expected, increasing w<sub>ff</sub> or w<sub>div</sub> reduces σ<sup>J</sup> and ⟨|f<sup>i</sup><sub>l</sub>⟩, respectively, whereas decreasing w<sub>ff</sub> or w<sub>div</sub> increases the values of the quality metrics. However, higher values of w<sub>ff/div</sub> tend to worsen the comparison metrics. In comparison, lower values yield slightly better or comparable comparison metrics relative to w<sub>ff/div</sub> = 100 but give worse quality metrics, either σ<sup>J</sup> or ⟨|f<sup>i</sup><sub>l</sub>⟩. The comparison metrics without physics losses are better than those with physics losses; however, the quality metrics are worse or comparable. Therefore, the effect of the physics losses is not very significant, though there is a positive effect on σ<sup>J</sup> for the semianalytical field. Based on these, we select w<sub>ff</sub> = w<sub>div</sub> = 100, as mentioned in Section 2.

Our approach is valid for the numerical field case as well as for the semianalytical field case. In Table 1, the comparison metrics

C<sub>ves</sub>, C<sub>CS</sub>, E<sup>0</sup><sub>w</sub>, E<sup>0</sup><sub>m</sub>, and ε of our AI-generated NLFFFs are 0.96, 0.94, 0.70, 0.58, and 0.93 for the test set, respectively. Our AI-generated NLFFFs show good performance on average, especially considering that these NLFFFs are generated within 1 s using only the observed photospheric vector magnetic fields. Our results indicate that the proposed approach is an efficient alternative for rapid coronal magnetic field reconstruction, given the inherent uncertainties in the force-free assumption itself and existing NLFFF extrapolation methods. The results from the test set demonstrate how well our PINO model performs on new ARs that are not used for model training or selection. Our PINO model for numerical fields uses the SHARP vector magnetic field data at the bottom of a 3D Cartesian box, so the model can only be used with AR-sized photospheric vector magnetic field data remapped to the Lambert CEA projection following the SHARP pipeline (M. G. Bobra et al. 2014). The quality metric for divergence-freeness ⟨|f<sup>i</sup><sub>l</sub>⟩ of our AI-generated NLFFFs from the PINO model with physics losses (w<sub>ff/div</sub> = 100) is 5.3 × 10<sup>−3</sup>, which is better than 7.5 × 10<sup>−3</sup> from the PINO model without physics losses (w<sub>ff/div</sub> = 0). However, the quality metric for force-freeness σ<sup>J</sup> is 0.85 with physics losses and 0.84 without physics losses, indicating that both cases achieve a similar level of force-freeness. This may be due to the difficulty of balancing multiobjective
![img-2.jpeg](img-2.jpeg)

Figure 3. EUV observations of AR 12673 and projected magnetic field lines from different extrapolation methods. The format of the figure is the same as Figure 2.

Losses. In the numerical field case, unlike in the semianalytical case, the comparison metrics $C_{\mathrm{vec}}, C_{\mathrm{CS}}, E_{\mathrm{w}}^{\prime}, E_{\mathrm{m}}^{\prime}$, and $\epsilon$ improve when physics losses are used. Without physics losses, these metrics are $0.94,0.93,0.67,0.55$, and 0.90 for the test set, respectively. This indicates that physics losses help generate NLFFFs of ARs that are more similar to the target NLFFFs obtained from the MHD relaxation method (S. Inoue et al. 2013).

In Figures 2 and 3, we compare the EUV observations and the magnetic field lines extrapolated from several methods for ARs 11158 and 12673, respectively. The field lines are traced using streamtracer.5 Panels (a) and (b) in both figures show the EUV observations by Solar Dynamics Observatory (SDO; W. D. Pesnell et al. 2012)/Atmospheric Imaging Assembly (AIA; J. R. Lemen et al. 2012) at 171 and 94 Å, respectively, which display the structures of coronal loops in these ARs. The panels from (c) to (f) show the projected magnetic field lines from the same footpoints on the photosphere from the following: (c) the target ISEE NLFFFs; (d) our AI-generated NLFFFs; (e) the NLFFFs from the optimization method (T. Wiegelmann et al. 2012); and (f) the NLFFFs from the PINN method (R. Jarolim et al. 2023; $\lambda_{\text {ff/div }}=0.1$ ).

The AI-generated NLFFFs by our trained PINO model in panels (d) of Figures 2 and 3 show similar overall shapes to the target NLFFFs in panels (c), which confirms the ability of our trained PINO model to successfully generate NLFFFs for these ARs that are quite qualitatively similar to the target NLFFFs. This is also consistent with the quantitative results in Table 1. The coronal loop structures visible in the 171 Å images show a good match with the magnetic field lines of our AI-generated NLFFFs. The alignment of the red high current density regions in our AI-generated NLFFFs with the bright regions observed in the 94 Å EUV images further supports the ability of our trained PINO model to generate quite accurate NLFFFs for the ARs.

The overall field line structures of our AI-generated NLFFFs are also similar to those by other extrapolation methods in panels (e) and (f) of Figures 2 and 3. Our PINO model shows qualitatively comparable performance to these conventional methods. The vertically integrated current density maps in Figures 4 and 5 provide an additional representation of the magnetic topology. The spatial distribution of the current density from our AI-generated NLFFFs in panels (d) is similar to those from the target NLFFFs in panels (c) and other extrapolation methods in panels (e) and (f). While conventional

<sup>5</sup> https://github.com/sunpy/streamtracer
![img-3.jpeg](img-3.jpeg)

Figure 4. EUV observations of AR 11158 and vertically integrated current density maps from different extrapolation methods. The format of the figure is the same as Figure 2.

Methods require several hours to compute a single high-resolution (512 × 256 × 256) NLFFF for an AR, our AI-generated NLFFFs can be obtained within 1 s on a single consumer GPU (NVIDIA RTX 4070 Ti) and within 2 s on a single consumer CPU (Intel i5-13600K). This significant speedup is achieved because our PINO model does not require retraining for new ARs.

Figure 6 shows the time series of total and free magnetic energy calculated from NLFFFs of AR 11158. Although the magnetic energy of our AI-generated NLFFFs is about 5% lower than that of the target fields, it follows a similar trend and aligns well with the magnetic energy obtained from the optimization method. Since our trained PINO model generates an NLFFF within 1 s, the total computation time for 601 SHARP data with the full cadence of 12 minutes for the five-day observation is less than 10 minutes. This shows the sufficient potential of our PINO model as a method for real-time solar coronal magnetic field modeling. The dashed black vertical lines indicate flares ≥M1 from 00:00 UTC on 2011 February 12 to 00:00 UTC on 2011 February 17. The free magnetic energy also shows trends similar to those obtained with other methods. While the drop in free magnetic energy at the X2.2 flare is captured by our AI-generated NLFFFs, the M6.6 flare is not well captured. This discrepancy may be due to the high divergence freeness of our AI-generated NLFFFs (G. Valori et al. 2013). This shows a limitation of our method: a single PINO model may not be able to provide accurate NLFFFs for all boundary condition data.

There may be concerns about the force freeness and divergence freeness of our AI-generated NLFFFs, as indicated by their quality metrics compared to the target NLFFFs in Table 1. Attempts to resolve this by increasing the loss weights w<sub>ff</sub> and w<sub>div</sub>, or by using a vector potential, result in unstable training without significant improvements. Using preprocessed boundary conditions (T. Wiegelmann et al. 2006) as an input for our trained PINO model also does not lead to significant improvement. This may be due to insufficient boundary conditions of NLFFF problems from using only the bottom boundary condition as an input. Another possible reason is that our PINO model does not accept coordinate inputs, unlike PINN, which can achieve excellent force freeness and divergence freeness through automatic differentiation. Also, without coordinate inputs, it is necessary to hold the entire volume data in GPU memory during training. This requires about 11 GB for numerical NLFFFs from the ISEE database. This is why we use a batch size of one, as we trained the model on an NVIDIA RTX 4070Ti with 12 GB of GPU memory. Using a subset of coordinate inputs for each batch, as in PINNs (R. Jarolim et al. 2023), could mitigate memory limitations for larger ARs or higher altitudes. However, our examinations with DeepONet, which uses coordinate inputs in operator learning, show several challenges: overly smooth bottom boundary fields; worsening comparison metrics; and increasing
![img-4.jpeg](img-4.jpeg)

Figure 5. EUV observations of AR 12673 and vertically integrated current density maps from different extrapolation methods. The format of the figure is the same as Figure 2.

Inference time due to the need for point sampling in the computational domain. Thus, we use the U-shaped neural operator architecture, which does not rely on coordinate inputs. Improving PINO using coordinate inputs is a future research direction.

As another option to solve the above problem, our PINO model can provide the input for the optimization method (T. Wiegelmann et al. 2012). We find that using an AI-generated NLFFF as the initial field for the optimization method leads to a refined AI-generated NLFFF that has better-quality metrics, with minimal changes or slight improvements in the comparison metrics, as shown in Table 1 and indicated by the ""AI + optimization method."" For σ<sup>f</sup>, the values decrease from 0.24 to 0.08 for the semianalytical field and from 0.85 to 0.47 for the numerical fields. For |*f*<sub>*i*</sub>|>, the values decrease from 5.8 × 10<sup>−3</sup> to 2.4 × 10<sup>−3</sup> for the semianalytical field and from 5.3 × 10<sup>−3</sup> to 1.5 × 10<sup>−3</sup> for the numerical fields. Our GPU-accelerated optimization method code converges within 5 minutes for a resolution of 512 × 256 × 256. The total computation time remains below the 12 minutes cadence of SHARP data for observed photospheric vector magnetic fields. However, this is just one approach we have found to address the issue of quality metrics in AI-generated NLFFFs, and further verification of this method is needed.

### 5. Summary and Conclusion

In this study, we have developed a PINO model for the real-time extrapolation of NLFFFs in the solar corona. After training with physics loss based on force-free and divergence-free conditions and data loss, our PINO model can generate 3D NLFFFs in less than 1 s on a GPU and 2 s on a CPU for new ARs using only observed photospheric vector magnetic fields. The AI-generated NLFFFs by our trained PINO model closely resemble the target NLFFFs both qualitatively and quantitatively, including the Low and Lou (B. C. Low & Y. Q. Lou 1990) semianalytical and ISEE numerical NLFFFs (K. Kusano et al. 2021). The qualitative similarity is illustrated by the magnetic field line structures (Figures 2 and 3) and the spatial distribution of the current density (Figures 4 and 5) for AR 11158 and 12673 in the test set. The quantitative comparison metrics—*C*<sub>vec</sub>, *C*<sub>CS</sub>, *E*<sub>n</sub><sup>′</sup>, *E*<sub>m</sub><sup>′</sup>, and ε—are 1.00, 0.98, 0.91, 0.81, and 1.01 for the semianalytical NLFFF test case, and 0.96, 0.94, 0.70, 0.58, and 0.93 for 30 ARs in the numerical NLFFF test set (Table 1). The trend of 3D magnetic energy from AI-generated NLFFFs (Figure 6) is similar to the target NLFFFs. The AI-generated NLFFFs also exhibit similarities to other NLFFF extrapolation methods.
![img-5.jpeg](img-5.jpeg)

Figure 6. Time series of (top) total magnetic energy and (bottom) free magnetic energy calculated from NLFFFs of AR 11158. The dashed black vertical lines indicate flares ≥M1. We use SHARP data as the input data of the PINO model. The solid red line shows magnetic energy from the target fields of the ISEE NLFFF database, which covers the period from 12:00 UTC on 2011 February 13 to 12:00 UTC on 2011 February 15. The solid blue line is for our trained PINO, the dotted black line is for the optimization method (T. Wiegelmann et al. 2012), and the dashed green line is for the PINN method (R. Jarolim et al. 2023). The three lines correspond to the period from 00:00 UTC on 2011 February 12 to 00:00 UTC on 2011 February 17. The data for the black and green lines are obtained from R. Jarolim et al. (2023).

In terms of field line structures (Figures 2 and 3), current density distribution (Figures 4 and 5), and magnetic energy trend (Figure 6).

The ability to generate high-resolution NLFFFs in a fraction of the time required by conventional methods is a major contribution since conventional NLFFF extrapolation techniques often take several hours to compute a single high-resolution NLFFF (at least for the first one). This reduction in computation time by our trained PINO model shows its potential as a method for real-time solar coronal magnetic field modeling. For example, it could be useful for a physics-based flare prediction method based on a series of NLFFFs (K. Kusano et al. 2020). We also propose a solution for unsatisfactory quality metrics of our AI-generated NLFFFs, which is one limitation of our method. When we use the AI-generated field as the initial field for the optimization method, it seems to produce a refined NLFFF with better-quality metrics. The quality metrics for force freeness σ<sup>f</sup> and divergence freeness <|f<sub>i</sub><sup>i</sup>|> both decrease for the semianalytical NLFFF test case and for 30 ARs in the numerical NLFFF test set (Table 1). For the semianalytical field, σ<sup>f</sup> decreases from 0.24 to 0.08, and <|f<sub>i</sub><sup>i</sup>|> decreases from 5.8 × 10<sup>−3</sup> to 2.4 × 10<sup>−3</sup>. For the numerical fields, σ<sup>f</sup> decreases from 0.85 to 0.47, and <|f<sub>i</sub><sup>i</sup>|> decreases from 5.3 × 10<sup>−3</sup> to 1.5 × 10<sup>−3</sup>. However, further verification is needed, as the optimization method usually struggles to converge to new solutions.

We plan to focus on improving AI-generated NLFFFs by using a larger and more diverse data set of NLFFFs from various ARs for training, leading to more accurate real-time extrapolation of NLFFFs. Moreover, integrating traditional PDE solvers into the training process, a concept known as differentiable physics (K. Um et al. 2020), may further enhance the accuracy of the AI-generated NLFFFs. Although this study addresses parametric NLFFF PDEs, we expect that our approach could be extended to full MHD equations for more comprehensive real-time reconstructions of the solar corona.
## Acknowledgments

We acknowledge the use of the ISEE database from Nagoya University. We appreciate the referee's constructive comments. We also appreciate the community's efforts in developing the open-source packages used in this work. The SDO data are courtesy of NASA/SDO and the AIA and HMI science teams. We acknowledge the support from JSPS KAKENHI (No. JP21H04492) for maintaining the publicly available ISEE NLFFF database from Nagoya University. This research was supported by the BK21 FOUR program through the National Research Foundation of Korea (NRF) under the Ministry of Education (MoE) (Kyung Hee University, Human Education Team for the Next Generation of Space Exploration), the BK21 FOUR program of Graduate School, Kyung Hee University (No. GS-1-JO-NON-20242364), the Basic Science Research Program through the NRF funded by the MoE (No. RS-202300248916), the Korea Astronomy and Space Science Institute under the R\&D program (No. 2024-1-850-02) supervised by the Ministry of Science and ICT (MSIT), and the Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korean government (MSIT) (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning).

Software: PyTorch (A. Paszke et al. 2019), TorchMetrics (N. S. Detlefsen et al. 2022), Matplotlib (J. D. Hunter 2007), NumPy (C. R. Harris et al. 2020), SciPy (P. Virtanen et al. 2020), Astropy (Astropy Collaboration et al. 2013, 2018, 2022), SunPy (The SunPy Community et al. 2020).

## Appendix <br> Numerical Field Data Set Composition

The training set consists of 1795 NLFFFs from 151 ARs: 11078, 11089, 11092, 11093, 11108, 11109, 11117, 11130, 11131, 11133, 11140, 11163, 11164, 11165, 11166, 11169, 11176, 11183, 11190, 11195, 11196, 11199, 11236, 11257, 11260, 11261, 11263, 11271, 11283, 11289, 11301, 11302, 11305, 11312, 11316, 11325, 11327, 11330, 11339, 11354, 11358, 11362, 11363, 11374, 11384, 11386, 11387, 11389, 11390, 11391, 11393, 11402, 11416, 11422, 11428, 11429, 11455, 11459, 11460, 11465, 11471, 11476, 11484, 11486, 11492, 11497, 11512, 11515, 11520, 11543, 11555, 11560, 11562, 11564, 11579, 11582, 11585, 11589, 11591, 11596, 11598, 11613, 11618, 11620, 11635, 11640, 11652, 11654, 11660, 11665, 11682, 11698, 11711, 11718, 11719, 11723, 11726, 11730, 11731, 11736, 11745, 11748, 11755, 11765, 11776, 11777, 11793, 11818, 11827, 11835, 11877, 11884, 11890, 11936, 11944, 11967, 12089, 12109, 12121, 12135, 12144, 12146, 12149, 12152, 12158, 12173, 12175, 12177, 12186, 12192, 12203, 12205, 12209, 12216, 12217, 12219, 12221, 12222, 12232, 12242, 12297, 12371, 12403, 12422, 12489, 12494, 12544, 12738, 12816, 12936, and 12975. The length in the $x$-direction ranges from a minimum of 78 Mm (AR 11736) to a maximum of 943 Mm (AR 12242), with a mean of 348 Mm , a median of 310 Mm , and a standard deviation of 152 Mm . The length in the $y$-direction ranges from a minimum of 44 Mm (AR 11736) to a maximum of 413 Mm (AR 12242), with a mean of 167 Mm , a median of 157 Mm , and a standard deviation of 62 Mm .

The validation set consists of 49 NLFFFs from 30 ARs: 11846, 11875, 11899, 11909, 11917, 11946, 11974, 11991, 12002, 12014, 12017, 12026, 12036, 12047, 12080, 12236,

12249, 12253, 12259, 12271, 12277, 12303, 12325, 12360, 12381, 12415, 12420, 12436, 12473, and 12781. The length in the $x$-direction ranges from a minimum of 234 Mm (AR 12017) to a maximum of 649 Mm (AR 12080), with a mean of 366 Mm , a median of 340 Mm , and a standard deviation of 103 Mm . The length in the $y$-direction ranges from a minimum of 95 Mm (AR 12271) to a maximum of 368 Mm (AR 12259), with a mean of 182 Mm , a median of 169 Mm , and a standard deviation of 58 Mm .

The test set consists of 483 NLFFFs from 30 ARs: 11158, 11861, 11882, 11908, 11916, 11931, 11968, 11977, 11996, 12011, 12021, 12034, 12042, 12049, 12082, 12241, 12251, 12257, 12268, 12275, 12280, 12305, 12335, 12367, 12396, 12418, 12434, 12443, 12488, and 12673. The length in the $x$ direction ranges from a minimum of 195 Mm (AR 12275) to a maximum of 535 Mm (AR 12305), with a mean of 313 Mm , a median of 287 Mm , and a standard deviation of 85 Mm . The length in the $y$-direction ranges from a minimum of 86 Mm (AR 12275) to a maximum of 267 Mm (AR 12082), with a mean of 159 Mm , a median of 155 Mm , and a standard deviation of 48 Mm .

## ORCID iDs

Mingyu Jeon (2) https://orcid.org/0009-0004-7798-5052
Hyun-Jin Jeong (2) https://orcid.org/0000-0003-4616-947X
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Jihye Kang (2) https://orcid.org/0000-0001-6213-4088
Kanya Kusano (2) https://orcid.org/0000-0002-6814-6810

## References

Asensio Ramos, A., Cheung, M. C. M., Chifu, I., \& Gafeira, R. 2023, LRSP, 20, 4
Astropy Collaboration, Price-Whelan, A. M., Lim, P. L., et al. 2022, ApJ, 935, 167
Astropy Collaboration, Price-Whelan, A. M., Sipőcz, B. M., et al. 2018, AJ, 156, 123
Astropy Collaboration, Robitaille, T. P., Tollerud, E. J., et al. 2013, A\&A, 558, A33
Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SoPh, 289, 3549
Chen, P. F. 2011, LRSP, 8, 1
Detlefsen, N. S., Borovec, J., Schock, J., et al. 2022, JOSS, 7, 4101
Faroughi, S. A., Pawar, N. M., Fernandes, C., et al. 2024, J. Comput. Inform. Sci. Eng., 24, 040802
Goswami, S., Bora, A., Yu, Y., \& Karniadakis, G. E. 2023, Machine Learning in Modeling and Simulation: Methods and Applications (Berlin: Springer), 219
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585, 357
Hunter, J. D. 2007, CSE, 9, 90
Inoue, S., Magara, T., Pandey, V. S., et al. 2013, ApJ, 780, 101
Jarolim, R., Thalmann, J. K., Veronig, A. M., \& Podladchikova, T. 2023, NatAe, 7, 1171
Jarolim, R., Tremblay, B., Rempel, M., et al. 2024, ApJL, 963, L21
Jeong, H.-J., Moon, Y.-J., Park, E., Lee, H., \& Baek, J.-H. 2022, ApJS, 262, 50
Karniadakis, G. E., Kevrekidis, I. G., Lu, L., et al. 2021, NatRP, 3, 422
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kovachki, N. B., Li, Z., Liu, B., et al. 2021, arXiv:2108.08481
Kusano, K., Iijima, H., Kaneko, T., et al. 2021, ISEE Database for Nonlinear Force-Free Field of Solar Active Regions, v1.0, Nagoya University, doi:10. 34515/DATA.HSC-00000
Kusano, K., Iju, T., Bamba, Y., \& Inoue, S. 2020, Sci, 369, 587
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Li, Z., Kovachki, N., Azizzademesheli, K., et al. 2020, arXiv:2010.08895
Li, Z., Zheng, H., Kovachki, N., et al. 2024, ACM/IMS J. Data Sci., 1, 1
Linker, J. A., Mikić, Z., Biesecker, D. A., et al. 1999, JGR, 104, 9809
Low, B. C., \& Lou, Y. Q. 1990, ApJ, 352, 343
Lu, L., Jin, P., Pang, G., Zhang, Z., \& Karniadakis, G. E. 2021, NatMI, 3, 218
Lu, L., Meng, X., Cai, S., et al. 2022, CMAME, 393, 114778
Mikić, Z., Linker, J. A., Schnack, D. D., Lionello, R., \& Tarditi, A. 1999, PhPI, 6, 2217
Mikić, Z., \& McClymont, A. N. 1994, in ASP Conf Ser. 68, Solar Active Region Evolution: Comparing Models with Observations, ed. K. S. Balasubramaniam \& G. W. Simon (San Francisco, CA: ASP), 225

Paszke, A., Gross, S., Massa, F., et al. 2019, arXiv:1912.01703
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Rahman, M. A., Ross, Z. E., \& Azizzadenesheli, K. 2022, arXiv:2204.11127
Rahman, S., Jeong, H.-J., Siddique, A., Moon, Y.-J., \& Lawrance, B. 2024, ApJS, 271, 14
Raissi, M., Perdikaris, P., \& Karniadakis, G. E. 2019, JCoPh, 378, 686
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schrijver, C. J., De Rosa, M. L., Metcalf, T. R., et al. 2006, SoPh, 235, 161

Shibata, K., \& Magara, T. 2011, LRSP, 8, 6
Temmer, M. 2021, LRSP, 18, 4
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Um, K., Brand, R., Fei, Y., Holl, P., \& Thürey, N. 2020, arXiv:2007.00016
Valori, G., Démoulin, P., Pariat, E., \& Masson, S. 2013, A\&A, 553, A38
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Wang, S., Wang, H., \& Perdikaris, P. 2021, SciA, 7, eabi8605
Wheatland, M. S., Sturrock, P. A., \& Roumeliotis, G. 2000, ApJ, 540, 1150
Wiegelmann, T., Inhester, B., \& Sakurai, T. 2006, SoPh, 233, 215
Wiegelmann, T., Petrie, G. J. D., \& Riley, P. 2017, SSRv, 210, 249
Wiegelmann, T., \& Sakurai, T. 2021, LRSP, 18, 1
Wiegelmann, T., Thalmann, J. K., Inhester, B., et al. 2012, SoPh, 281, 37
Zhang, J., Temmer, M., Gopalswamy, N., et al. 2021, PEPS, 8, 56"
Hyun-Jin Jeong et al 2025 - Prediction of the Next Solar Rotation Synoptic Maps Using an Artificial Intelligence– based Surface Flux Transport Model.pdf,"# Prediction of the Next Solar Rotation Synoptic Maps Using an Artificial Intelligencebased Surface Flux Transport Model 

Hyun-Jin Jeong ${ }^{1,2}$ (D) Mingyu Jeon ${ }^{2}$ (D), Daeil Kim ${ }^{2}$ (D), Youngjae Kim ${ }^{2}$ (D), Ji-Hye Baek ${ }^{3,4}$ (D), Yong-Jae Moon ${ }^{2,3}$ (D), and Seonghwan Choi ${ }^{3,4}$ (D)<br>${ }^{1}$ Centre for mathematical Plasma Astrophysics, Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium; jeong_hj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{3}$ Technology Center for Astronomy and Space Science, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{4}$ Space Science Division, Korea Astronomy and Space Science Institute, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{5}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2024 December 6; revised 2025 March 12; accepted 2025 March 21; published 2025 April 16


#### Abstract

In this study, we develop an artificial intelligence (AI)-based solar surface flux transport (SFT) model. We predict synoptic maps for the next solar rotation ( 27.2753 days) using deep learning. Our model takes the latest synoptic maps and their sine-latitude grid data as inputs. Synoptic maps, which represent global magnetic field distributions on the solar surface, have been widely used as initial boundary conditions in the Sun and space-weather prediction models. Here we train and evaluate our deep-learning model, based on the Pix2PixCC architecture, using data sets of Solar Dynamics Observatory/Helioseismic and Magnetic Imager, Solar and Heliospheric Observatory/ Michelson Doppler Imager, and National Solar Observatory/Global Oscillation Network Group synoptic maps with a resolution of 360 by 180 (longitude and sine latitude) from 1996 to 2023. We present results of our model and compare them with those from the persistent model and the conventional SFT model, including the effects of differential rotation, meridional flow, and diffusion on the solar surface. The average pixel-to-pixel correlation coefficient between the target and our AI-generated data, after 10 by 10 binning with a $10^{6}$ resolution in longitude, is 0.71 . This result is qualitatively similar to the results of the conventional SFT model $(0.65-0.68)$ and better than the results of the persistent model ( 0.56 ). Our model successfully generates magnetic features, such as the diffusion of solar active regions and the motions of supergranules. Using synthetic input data with bipolar structures, we confirm that our model successfully reproduces differential rotation and meridional flow. Finally, we discuss the advantages and limitations of our model in view of magnetic field evolution and its potential applications.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); The Sun (1693); Astronomy data analysis (1858); Convolutional neural networks (1938)

## 1. Introduction

Magnetic fields on the solar surface play a crucial role in shaping the large-scale structure of the Sun's atmosphere and driving space-weather disturbances that affect Earth's environment (D. H. Mackay \& A. R. Yeates 2012; D. Nandy et al. 2023). Various numerical schemes for predicting the solar corona and space weather have used global solar photospheric magnetic field data as model inputs (M. J. Owens et al. 2014). The global photospheric field data are routinely provided as synoptic maps, produced by merging a series of line-of-sight full-disk magnetograms over a solar synodic rotation period of 27.2753 days (referred to as a Carrington rotation or CR). For each magnetogram, the data within a certain longitudinal range of the central meridian are used to construct the synoptic maps (R. Howard et al. 1969; J. T. Hoeksema \& P. H. Scherrer 1986). The synoptic maps, available over several decades, have facilitated understanding of the evolution and transport of solar magnetic flux on timescales ranging from a solar rotation to solar cycles (L. E. A. Vieira \& S. K. Solanki 2010; S. Gosain et al. 2013).

The surface flux transport (SFT) model has been used to predict the distribution of the Sun's magnetic fields and has

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
been shown to provide a good description of the global evolution after magnetic field emergence (R. H. Cameron et al. 2012). The conventional SFT model is based on the idea that radial magnetic flux on the solar surface is transported by horizontal plasma flow but without any back-reaction on these flows (R. B. Leighton 1964). Accurate modeling of SFT requires observationally constrained descriptions of large-scale flow parameters, such as differential rotation, meridional circulation, and supergranular diffusion. On the solar surface, equatorial regions rotate faster than polar regions. Poleward meridional circulation drives the polarity reversal of the solar magnetic fields. Magnetic field diffusion is generated by the random motions of supergranules on the solar surface. Each SFT parameter has been empirically determined based on longterm photospheric observations, and the model results are highly dependent on their values (T. Whitbread et al. 2017).

Deep learning, also broadly known as artificial intelligence (AI), has been used to predict magnetic fields on the solar surface. L. Bai et al. (2021) predicted the evolution of a solar active region's magnetic fields over the next 6 hr using a 12 hr sequence of Spaceweather HMI Active Region Patch data sets with a spatiotemporal long short-term memory network. F. P. Ramunno et al. (2024) predicted the next 24 hr of solar full-disk magnetograms using Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI) data sets with a denoising diffusion probabilistic model. However, it has been challenging to compare global magnetic flux transport
![img-0.jpeg](img-0.jpeg)

Figure 1. Overview of the conventional synoptic map and SFT model parameters, and our proposed approaches to predict the next solar rotation magnetic fields. (a) A synoptic map is constructed from magnetic field data near the Sun's central meridian over a solar rotation. (b) The conventional surface flux transport model, describing the long-term evolution of the photospheric magnetic fields, includes differential rotation, meridional flow, and supergranular diffusion. (c) Our deep neural network generates the next solar rotation photospheric field data from the latest synoptic map and its sine-latitude grid data.
between the results of the conventional SFT model and AIbased prediction models when using local or full-disk magnetic field predictions with several-hour-ahead results. J. J. Athalathil et al. (2024) used a physics-informed neural network-based model to study the evolution of bipolar magnetic regions, aiming to enhance computational accuracy in simulating SFT equations on the solar surface by overcoming the challenges of conventional grid-based methods. However, their model did not incorporate observed data for training, and they planed to use real magnetic field data in future studies.

In the present study, we propose a data-driven AI-based SFT model to predict the evolution of global magnetic fields on the solar surface during a solar rotation. We use synoptic maps and their sine-latitude grid data as the model input, and the next solar rotation synoptic maps as the model output. Our AI-based SFT model is based on the Pix2PixCC architecture, which can generate realistic magnetic field data without any artificial saturation limits from the model inputs (H.-J. Jeong et al. 2022). Figure 1 shows the construction of a synoptic map, three main parameters of the conventional SFT model, and our AIbased SFT model's input and output data configurations. We describe our data configurations in Section 2 and model structures in Section 3. We show the evaluation metric results of our AI-based SFT model and compare our model results with those from the persistent model and the conventional SFT models in Section 4. We summarize and conclude our study in Section 5.

## 2. Data

Here we use Solar and Heliospheric Observatory (SOHO)/ Michelson Doppler Imager (MDI) synoptic maps from 1996 June to 2010 November, SDO/HMI synoptic maps from 2010

May to 2023 December, and National Solar Observatory (NSO)/Global Oscillation Network Group (GONG) synoptic maps from 2006 August to 2023 December to train and evaluate our deep-learning model. Polar fields that are missing or of poor quality from HMI and MDI synoptic maps are filled using a two-dimensional spatial-temporal interpolation correction scheme from X. Sun et al. (2011) and those from GONG synoptic maps are filled using a cubic polynomial surface fit to the currently observed magnetic fields at neighboring latitudes. The full CR HMI, MDI, and GONG synoptic maps have resolutions of $3600 \times 1440,3600 \times 1080$, and $360 \times 180$, equally spaced in longitude and sine latitude, respectively. We interpolate all maps to a uniform $360 \times 180$ grid in longitude and sine latitude without applying cross calibrations. We make them $512 \times 256$ data for computation by adding proper padding (same data for the left and right sides, and reflected data for the upper and lower sides), considering the spherical surface. For the model evaluation, we use $360 \times 180$ data extracted from the $512 \times 256$ model input and output corresponding to the synoptic maps. The resolution of our model input and output is approximately $11^{\prime \prime}$ per pixel.

For this study, we collected 592 synoptic maps from CR 1910 to CR 2279, including data from solar cycles 23 and 24 and data from the ascending phase of solar cycle 25 , to train, validate, and test our model. Since the number of synoptic maps is insufficient, we augment the training data set approximately 360 times by shifting $1^{\prime \prime}$ in longitude to provide a more diverse data set during training (C. Shorten \& T. M. Khoshgoftaar 2019; S. Rahman et al. 2023). Data augmentation is not applied to the validation and test data sets. We use pairs of synoptic maps and their sine-latitude grid data as the input and the next solar rotation synoptic maps as the
output of our model, ensuring no duplication between the training, validation, and test data sets. We consider two consecutive years for training, six consecutive months for validation, and six consecutive months for testing from 1996 June to 2023 December. To ensure that our model's performance is generalized and not limited to specific data sets, we employ K-fold cross validation with six folds $(K=6)$, enabling the entire set of synoptic maps to be used for testing. We train our model six times with different data splits and report the averaged metric results from the six trained models. For each fold iteration, we take approximately 90,000 (multiplied by 360), 100, and 100 pairs for the training, validation, and test data sets, respectively.

## 3. Methods

### 3.1. Conventional SFT Model

The conventional SFT model describes the passive transport of radial magnetic fields $\left(B_{r}\right)$ on the solar surface under the effects of differential rotation, meridional flow, and supergranular diffusion as shown in Figure 1(b). In this study, we use empirical parameters and the equation of the conventional SFT model (A. R. Yeates et al. 2023):

$$
\begin{aligned}
\frac{\partial B_{r}}{\partial t}= & -\Omega(\theta) \frac{\partial B_{r}}{\partial \phi}-\frac{1}{R_{\odot} \sin \theta} \frac{\partial}{\partial \theta}\left[u(\theta) B_{r} \sin \theta\right]+\eta_{\mathrm{H}} \\
& \times\left[\frac{1}{R_{\odot}^{2} \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \frac{\partial B_{r}}{\partial \theta}\right)+\frac{1}{R_{\odot}^{2} \sin ^{2} \theta} \frac{\partial^{2} B_{r}}{\partial \phi^{2}}\right]
\end{aligned}
$$

where $\phi, \theta, R_{\odot}, \Omega(\theta), u(\theta)$, and $\eta_{\mathrm{H}}$ represent the longitude, latitude, solar radius, differential rotation, meridional circulation, and horizontal diffusion, respectively. The equation is derived from the radial component of the mean-field magnetohydrodynamic induction equation at the solar surface, assuming that the field at the surface is purely vertical (J. Jiang et al. 2014). We use an empirically derived velocity profile for the differential rotation as determined by H. B. Snodgrass \& R. K. Ulrich (1990): $\Omega(\theta)=0.18-2.396 \cos ^{2} \theta-$ $1.787 \cos ^{4} \theta$, where $\Omega(\theta)$ is expressed in units of degrees per day. The constant term in the differential rotation profile is provided in the Carrington frame, which is commonly used in conventional SFT models. For the meridional flow, we use the profile $u(\theta)=-R_{\odot} \Delta_{u} \cos \theta \sin ^{\rho} \theta$, where $\Delta_{u}$ represents the flow divergence at the solar equator. For this study, we adopt $\Delta_{u}=6.9 \times 10^{-8} \mathrm{~s}^{-1}, p=3.87$, and $\eta_{\mathrm{H}}=425 \mathrm{~km}^{2} \mathrm{~s}^{-1}$. For more details, refer to A. R. Yeates et al. (2023); its baseline code is available at https://github.com/antyeates1983/sft_data.

### 3.2. AI-based SFT Model

We employ a deep-learning model based on the Pix2PixCC, as illustrated in Figure 2. Our model consists of three major components: a generator, a discriminator, and an inspector. The generator aims to produce target-like outputs from inputs, guided by objective functions from the discriminator and the inspector during the training. In this study, we replace the generator of the Pix2PixCC proposed by H.-J. Jeong et al. (2022) with the U-Net. The U-Net features a symmetric U-shaped architecture, consisting of encoder convolutional layers and decoder transposed convolutional layers, connected by skip connections (O. Ronneberger et al. 2015). Skip connections provide detailed features from previous encoder
layers to decoder layers, enabling the generation of more precise and representative outputs. The U-Net with skip connections is widely used in video frame prediction tasks (Y. Qiang et al. 2020; B. Q. Bastos et al. 2021), and has been applied to predict the global total electron content of the ionosphere for one-day space-weather forecasting (S. Lee et al. 2021). The discriminator distinguishes between real pairs (input and target data) and generated pairs (input and the generator output), and the inspector calculates correlation coefficients (CCs) between targets and the generator outputs to improve the consistency between target and output.

The discriminator extracts features via convolutional layers, enabling the calculation of a feature-matching loss $\left(\mathcal{L}_{\mathrm{FM}}\right)$, which optimizes the generator. Unlike direct data differences, $\mathcal{L}_{\mathrm{FM}}$ minimizes the discrepancy between feature maps of real and generated pairs. This approach is particularly effective for handling data with a broad dynamic range (A. Rana et al. 2019; D. Marnerides et al. 2021):

$$
\mathcal{L}_{\mathrm{FM}}(G, D)=\sum_{i=1}^{T} \frac{1}{N_{i}}\left\|D^{(i)}(x, y)-D^{(i)}(x, G(x))\right\|
$$

where $G, D, T, i, N_{i}, x$, and $y$ denote the generator, the discriminator, the total number of layers, the serial number of the layers, the number of elements in output feature maps of each layer, the input data, and the target data, respectively. $D^{(i)}$ represents the $i$ th layer of $D$, and $G(x)$ is the generator's output.

For the adversarial process to produce realistic outputs, we adopt the least-squares generative adversarial network (LSGAN) losses (X. Mao et al. 2017), defined as

$$
\begin{aligned}
& \mathcal{L}_{\mathrm{LSGAN}}^{G}(G, D)=\frac{1}{2}(D(x, G(x))-1)^{2} \\
& \mathcal{L}_{\mathrm{LSGAN}}^{D}(G, D)=\frac{1}{2}(D(x, y)-1)^{2}+\frac{1}{2}(D(x, G(x)))^{2}
\end{aligned}
$$

where $D(x, G(x))$ and $D(x, y)$ represent probabilities ranging from 0 (generated) to 1 (real), computed by the discriminator for generated and real pairs, respectively. The generator minimizes the $\mathcal{L}_{\mathrm{LSGAN}}^{G}$, while the discriminator minimizes $\mathcal{L}_{\mathrm{LSGAN}}^{D}$. The competition between the generator and the discriminator contributes to generating realistic outputs. The performance of adversarial objectives has been well demonstrated in image-to-image translation tasks for solar data (E. Park et al. 2019; H.-J. Jeong et al. 2020, 2022).

To stabilize training, we adopt a CC loss $\left(\mathcal{L}_{\mathrm{CC}}\right)$ based on Lin's concordance CC (L. I.-K. Lin 1989), which balances positive and negative polarity fields better than error-based losses (R. Vallejos et al. 2020; B. T. Atmaja \& M. Akagi 2021). The $\mathcal{L}_{\mathrm{CC}}$, computed at multiple scales, is expressed as

$$
\mathcal{L}_{\mathrm{CC}}(G)=\sum_{i=0}^{T} \frac{1}{T+1}\left(1-\mathrm{CC}_{i}(y, G(x))\right)
$$

where $T$ and $i$ denote the total number of downsampling by a factor of 2 and the serial number of the downsampling, respectively. $\mathrm{CC}_{i}$ indicates the CC value between the $2^{i}$ times downsampled target and generated data. The average of the CC values from multiscale targets and AI-generated results guides the model in optimizing its parameters. In addition, with the
![img-1.jpeg](img-1.jpeg)

Figure 2. Flowchart and structures of our deep-learning model based on the Pix2PixCC. The generator produces target-like data from input data. The discriminator trains to distinguish between the real pair (input and target data) and the generated pair (input and AI-generated data). The inspector computes concordance CCs between the target data and generated data. The generator's parameters are updated from the losses calculated by the inspector and discriminator during the model-training process.

help of LCC, we avoid enforcing artificial saturation limits on our model.

The final objective functions are

$$
\begin{aligned}
& \min_{G}\left(\lambda_1 \mathcal{L}_{\text{LSGAN}}^G(G, D) + \lambda_2 \mathcal{L}_{\text{FM}}(G, D) + \lambda_3 \mathcal{L}_{\text{CC}}(G)\right) \\
& \min_{D} \left(\mathcal{L}_{\text{LSGAN}}^D(G, D)\right),
\end{aligned}
$$

where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are hyperparameters that control the importance of $\mathcal{L}_{\text{LSGAN}}^G$, $\mathcal{L}_{\text{FM}}$, and $\mathcal{L}_{\text{CC}}$, respectively. We set $\lambda_1$, $\lambda_2$, and $\lambda_3$ to 2, 10, and 5, respectively, following H.-J. Jeong et al. (2022). We optimize the objective functions using the adaptive moment estimation (Adam; D. P. Kingma & J. Ba 2014) optimizer with a learning rate of 0.0002 over 1,000,000 iterations, and select the best-performing model using validation data sets based on evaluation metrics.

## 4. Results and Discussion

### 4.1. Quantitative Comparison

We evaluate the performance of our deep-learning model using three metrics: rms error (RMSE), feature similarity index measure (FSIM), and pixel-to-pixel Pearson CC between the target data and the model-predicted data in the test data sets. The RMSE measures the average differences between each pair of corresponding pixels in the output and target data. The FSIM quantifies low-level feature similarity using phase congruency and gradient magnitude to identify the local quality of the data (L. Zhang et al. 2011). The phase congruency computes the alignment of phase and amplitude across individual frequency components in the frequency domain, and the gradient magnitude measures contrast variation. The FSIM has been widely used to evaluate how blurry, noisy, or distorted the output is compared to the target data (U. Sara et al. 2019). We compute the pixel-to-pixel CC after 10 × 10 binning, which results in a 10° resolution in longitude, to compare the large-scale structure of magnetic fields on the solar surface (T. Getachew et al. 2019). The FSIM and the pixel-to-pixel CC range from 0 to 1, where 1 represents perfect agreement.

Table 1 shows that the quantitative evaluation results of our AI-based SFT model are similar to those of the conventional SFT model and better than those of the persistent model. The persistent model is based on the assumption that the output data will be the same as the input data. The average RMSE of our model results (17.8 G) is lower than that of the conventional
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison of pixel-to-pixel magnetic fields between target data and model results. Panels (a), (b), and (c) show scatter plots of magnetic fields from target data and results from our AI-based SFT model, the conventional SFT model, and the persistent model, respectively, in a 10° resolution. Panels (d), (e), and (f) show scatter plots of the same data in a 1° resolution. Gray dashed diagonal lines indicate positive correlations between the target's and the model's magnetic fields.

Table 1

|   | RMSE (G) | FSIM | Pixel-to-pixel CC (10 × 10 Binning)  |
| --- | --- | --- | --- |
|  Our AI-based SFT model results | 17.8 | 0.46 | 0.71  |
|  Conventional SFT model results | 18.0–20.2 | 0.39–0.43 | 0.65–0.68  |
|  Persistent model results | 25.1 | 0.42 | 0.56  |

**Note.** Lower RMSE values and higher FSIM and pixel-to-pixel CC values indicate better results.

SFT model results (18.0 G) and the persistent model results (25.1 G). The average FSIM and pixel-to-pixel CC of our model results (0.46 and 0.71, respectively) are higher than those of the conventional SFT model results (0.39 and 0.68, respectively) and the persistent model results (0.42 and 0.56, respectively). The average FSIM of the conventional SFT model results is lower than that of the persistent model results due to the smoothed magnetic field distributions in the conventional SFT model results. When ηH is reduced from 425 to 250 km² s⁻¹ as suggested by R. Cameron et al. (2010), the conventional SFT model results yield the average RMSE of 19.8 G, FSIM 0.40, and pixel-to-pixel CC of 0.67. When the flux-dependent ηH is implemented, as described in J. Worden & J. Harvey (2000), the conventional SFT model results show the average RMSE of 20.2 G, FSIM of 0.43, and pixel-to-pixel CC of 0.65. As the FSIM of the conventional SFT model results increases through adjustments to the ηH parameter, its RMSE increases and pixel-to-pixel CC decreases. Our model results show higher FSIM and pixel-to-pixel CC and lower RMSE compared to the results of three different ηH configurations for the conventional SFT model.

Figure 3 shows scatter plots of pixel-to-pixel magnetic field strengths between the target data and model results for the same data sets when calculating the average pixel-to-pixel CCs in Table 1. The results of our AI-based model and the conventional model show similar pixel-to-pixel magnetic field distributions in a 10° resolution, as shown in Figures 3(a) and (b). There are several pixels for which all three models cannot produce strong magnetic fields relative to the target data, and we expect that the reason is that these models cannot predict newly emerging magnetic fields during a solar rotation. In the case of magnetic fields generated by our model that are stronger than 200 G, the pixel-to-pixel CC shows a high value of 0.90 with the target data in a 1° resolution (Figure 3(d)). The pixel-to-pixel CCs for strong magnetic field predictions from the conventional model (Figure 3(e)) and the persistent model (Figure 3(f)) in a 1° resolution are 0.48 and 0.18, respectively.

We also train and evaluate our model separately using data sets from HMI, MDI, and GONG, and the average results from
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparison between MDI synoptic maps for CR 2026 (the model input) and CR 2027 (target), a conventional SFT model result, and our AI-generated data for the next solar rotation. (a)–(b) The MDI synoptic maps are input (a) and target data (b) for our deep-learning model. (c) Conventional SFT model result for the next solar rotation from the input data. (d) Our AI-generated data from the input data. Green arrows A1, A2, and A3 in (a)–(d) indicate the transported magnetic fluxes during a solar rotation, respectively. Red boxes in (a)–(d) show the location of a newly emerging active region.

![img-4.jpeg](img-4.jpeg)

Figure 5. Comparison between GONG synoptic maps for CR 2238 (the model input) and CR 2239 (target), a conventional SFT model result, and our AI-generated data for the next solar rotation. Each column, green arrows and red boxes are the same as in Figure 4.
![img-5.jpeg](img-5.jpeg)

Figure 6. A series of synoptic maps, our AI-generated data, and conventional SFT model results for three solar rotations. (a)–(d) HMI synoptic maps from CR 2195 (the model input) to CR 2198 (target). (e) HMI synoptic map for CR 2195, which is input data for our deep-learning model. (f)–(h) Iteratively generated data by our deep-learning model from the input data. (i) HMI synoptic map for CR 2195, which is an input data of the conventional SFT model. (j)–(l) A series of the SFT model results for three solar rotations. Green arrows and red boxes are the same as in Figure 4.

These data sets are mostly consistent with those in Table 1. In addition, we attempt to predict the next solar rotation synoptic maps by including synoptic maps from the last one and two solar rotations as additional model inputs, but this does not lead to significant improvements. The evaluation results of the conventional SFT model might be improved by incorporating the Air Force Data Assimilative Photospheric Flux Transport (K. S. Hickmann et al. 2015) model, which employs stochastic diffusion and data assimilation methods to reproduce more realistic magnetic features, and the Advective Flux Transport (L. Upton & D. H. Hathaway 2013) model, which utilizes vector spherical harmonics to replicate the characteristics of convective flows observed on the Sun.

### 4.2. Qualitative Comparison

We compare the results of our AI-based SFT model with the input and target data and the results of the conventional SFT model. Figure 4 shows the data and model results for MDI synoptic maps in early 2005. The input data corresponds to the MDI synoptic map for CR 2026, and the target data is the MDI synoptic map for the next solar rotation (CR 2027). Our AI-generated data produces the differential rotation and poleward flow of magnetic fields during a solar rotation well, as indicated by the green arrows (A1, A2, and A3) in Figure 4. Compared to the conventional SFT model result, our model result shows that magnetic field distributions and features are much closer to the target data. Our model, the conventional SFT model, and the persistent model cannot predict the appearance of a new active region (red box in Figure 4(b)) during the solar rotation.

Figure 5 shows the results when the model input is a GONG synoptic map for CR 2238 at the end of 2020. However, as in the previous case, none of the models—including ours, the conventional SFT model, and the persistent model—can predict the newly emerging active region (red box in Figure 5). While both our model and the conventional SFT model produce similar large-scale magnetic field distributions, our model generates small-scale magnetic features that are closer to the target data.

We can predict magnetic field data for several solar rotations ahead by running our model iteratively. Figure 6 shows HMI synoptic maps from CR 2195 to CR 2198 and the corresponding model results based on the input data for CR 2195.
![img-6.jpeg](img-6.jpeg)

Figure 7. AI-generated data from synthetic magnetic field data and comparison with the conventional SFT model results. (a) Examples of synthetic 10 bipoles' data from −0.5 to 0.5 sine latitude and from 60° to 300° longitude at regular intervals. (b) AI-generated data from the synthetic input data. (c) Conventional SFT model result for the next solar rotation from the synthetic input data. (d)–(e) Differential rotation and meridional flow speed estimated by our AI-generated data. Blue dashed lines in (b), (d), and (e) show profiles of the SFT by our deep-learning model. Brown lines in (c)–(e) show profiles of the SFT using the conventional SFT model. Black dots and horizontal gray bars in (d)–(e) represent the mean and standard deviation, respectively, of the estimated differential rotation speeds and meridional flow speeds, calculated within 1° latitude bins.

Using our AI-based SFT model, we first generate data for one solar rotation ahead from the model input, as shown in Figure 6(f). Next, we generate data for two solar rotations ahead using the AI-generated data, as shown in Figure 6(g), and then generate data for three solar rotations ahead in a similar manner, as shown in Figure 6(h). The sequence of results from our model shows reasonable agreement with the target data. The magnetic fields of the generated data diffuse and transport toward higher latitudes, similar to those of the target data, as indicated by the green arrows (A1 and A2) in Figure 6. Although neither our model nor the conventional SFT model can predict the appearance of the new active region for CR 2198 (red boxes in Figure 6), our model generates small-sale magnetic features that are closer to the target HMI synoptic data over the three solar rotations.

### 4.3. Comparison Using Synthetic Data

Here we use synthetic input data to confirm that our model represents solar SFT. The synthetic data include bipolar structures with magnetic field strengths ranging from −150 to +150 G. We make 3000 synthetic data at 10° longitude intervals from 60° to 300° and at 1° latitude intervals from −0.5 to +0.5, with two different leading magnetic field polarities for the bipolar structures. Using our AI-based SFT model, we generate data for the next solar rotation from the synthetic input data. We then calculate the longitudinal and latitudinal differences of the magnetic flux-weighted centers between the input and output data. The longitudinal differences represent differential rotation, and the latitudinal differences indicate meridional flow.

Figure 7 shows examples of 10 bipoles as input data, spaced at 60° longitude intervals and 0.1 sine-latitude intervals, and the corresponding model results. We confirm that the differential rotation and meridional flow profiles derived from our AI-generated data exhibit reasonable agreement with the conventional SFT profiles reported in several studies. The blue dashed lines in Figure 7 are SFT profiles computed from our model outputs using all 3000 synthetic input data. The brown solid lines represent profiles from the conventional SFT model parameters described in Section 3.1, providing a comparison with our AI-based SFT model results. Our results show patterns of differential rotation and meridional flow that are similar to those produced by the conventional SFT model, as shown in Figures 7(d) and (e), respectively. The brown dashed line in Figure 7(d) represents the empirical differential rotation profile measured from ephemeral regions observed during solar cycle 24 (A. S. Kutsenko 2021). The brown dashed line in Figure 7(e) shows the meridional flow profile implemented by the SFT model of C. J. Schrijver & A. M. Title (2001) and K. S. Hickmann et al. (2015).

We examine our AI-based SFT model using synthetic input data that include anomalous bipolar structures, which do not obey Hale's polarity law (G. E. Hale & S. B. Nicholson 1925) or Joy's tilt law (G. E. Hale et al. 1919). As shown in Figure 8, our model predicts the overall magnetic field distribution one solar rotation later from the anomalous bipole synthetic data, similar to the results of the conventional SFT model.
![img-7.jpeg](img-7.jpeg)

Figure 8. Comparison of our AI-generated data and the conventional SFT model results from synthetic input data, including bipolar structures with different orientations and polarities in the four configurations (Hale, anti-Hale, Joy, and anti-Joy regions) in each hemisphere. Panels (a), (b), and (c) show the synthetic input data, the results of our AI-based SFT model, and the results of the conventional SFT model, respectively.

Anomalous active regions cause significant changes in large-scale solar polar field buildup and open magnetic flux dynamics during a solar cycle (J. Jiang et al. 2014; S. Pal et al. 2023). We expect that our AI-based SFT model can handle the diversity of observed solar surface field evolution dynamics and is useful for predicting the evolution of magnetic fields in both standard and anomalous bipolar regions.

### 5. Summary and Conclusion

In this study, we have developed an AI-based SFT model to predict the evolution of solar magnetic fields during a CR. We trained and evaluated a deep-learning model based on the Pix2PixCC architecture using SDO/HMI, SOHO/MDI, and NSO/GONG synoptic maps spanning 1996–2023. Our AI-based SFT model generates magnetic field distributions for the next solar rotation, similar to the conventional SFT model and better than the persistent model in quantitative metrics such as RMSE, FSIM, and pixel-to-pixel CC. Our AI-based model offers a useful method for quick predictive assessment. Using test and synthetic data sets, we demonstrated that our model successfully implements key SFT motions, such as differential rotation and meridional flow on the solar surface. Our model also generates small-scale magnetic features better than the conventional SFT models. Our model enables iterative forecasting over three solar rotations, showing reasonable agreement with observed data. However, like the conventional SFT model and the persistent model, predicting the magnetic fields of newly emerging active regions remains challenging for our AI-based SFT model. The predictive capability of the conventional SFT model comes from the memory inherent in the slow solar surface flow processes, assuming no significant new emergences (D. Nandy et al. 2018; S. Dash et al. 2020). We expect that this inherent memory allows our AI-based model, which is trained using more than 25 yr of synoptic maps, to predict the magnetic fields of the next solar rotation similar to the results of the conventional SFT model.

Our results can be used as initial boundary conditions for solar coronal and heliospheric numerical models, potentially enhancing space-weather forecasting from several days to months in advance. Conventional SFT model results have been utilized to predict global coronal structures (D. Nandy et al. 2018), solar extreme ultraviolet irradiance (C. Henney et al. 2015), solar wind speeds (G. Barnes et al. 2023), upcoming solar activity cycles (P. Bhowmik & D. Nandy 2018), etc. We expect that our AI-generated data would provide improved predictive outcomes for these applications. However, we acknowledge the limitations of our method in predicting emerging magnetic fluxes of active regions, which makes it challenging to apply our AI-generated data to solar flare forecasts and extreme space-weather predictions during solar maximum. We expect that using input data with shorter time intervals (L. Bai et al. 2021; F. P. Ramunno et al. 2024) or additional data on convective motion below the solar surface (R. H. Cameron et al. 2012) may enhance our model's predictions of emerging magnetic fluxes. We leave this to future work. Our study shows a possibility that the AI-based model may show better results than existing empirical models. Furthermore, our new methodology, which uses a well-trained AI-based model and synthetic input data, can be applied to estimate empirical profiles from two-dimensional observations.

### Acknowledgments

We acknowledge support from the SpaceAI program (https://spaceai.kasi.re.kr), led by the Korea Astronomy and Space Science Institute (KASI) in partnership with Kyung Hee University, Korea Advanced Institute of Science & Technology (KAIST), and private-sector companies. SpaceAI provides a collaborative framework in which scientists, software engineers, industry experts as well as students/citizens all participate in as various project teams to solve multidisciplinary, community-wide questions in space science and technology with artificial intelligence. This research was supported by a Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2023-00248916), an Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (RS-2023-00234488, Development of solar synoptic magnetograms using deep-learning, 15%), the KASI under the R&D program (Project No. 2024-1-850-02) supervised by the MSIT, and the BK21 FOUR program of Graduate School, Kyung Hee University (GS-1-JO-NON-20242364). We thank the numerous researchers who have contributed to the development of SFT models, with special appreciation to A. R. Yeates et al. (2023). We also acknowledge the community efforts dedicated to developing the open-source packages used in this work.

**Software:** PyTorch (A. Paszke et al. 2019), SunPy (The SunPy Community et al. 2020), Astropy (T. P. Robitaille et al. 2013; A. M. Price-Whelan et al. 2018), SciPy (P. Virtanen et al. 2020), NumPy (C. R. Harris et al. 2020), Matplotlib (J. D. Hunter 2007), Scikit-image (S. Van der Walt et al. 2014).
## ORCID IDs

Hyun-Jin Jeong (1) https://orcid.org/0000-0003-4616-947X Mingyu Jeon (1) https://orcid.org/0009-0004-7798-5052

Daeil Kim (1) https://orcid.org/0009-0008-5566-6084
Youngjae Kim (1) https://orcid.org/0009-0009-2316-3658
Ji-Hye Baek (1) https://orcid.org/0000-0002-0230-4417
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Seonghwan Choi (1) https://orcid.org/0000-0002-1946-7327

## References

Athalathil, J. J., Vaidya, B., Kundu, S., Upendran, V., \& Cheung, M. C. 2024, ApJ, 975, 258
Atmaja, B. T., \& Akagi, M. 2021, JPhCS, 1896, 012004
Bai, L., Bi, Y., Yang, B., et al. 2021, RAA, 21, 113
Barnes, G., DeRosa, M. L., Jones, S. I., et al. 2023, ApJ, 946, 105
Bastos, B. Q., Oliveira, F. L. C., \& Milidiu, R. L. 2021, Int. J. Forecast., 37, 949
Bhowmik, P., \& Nandy, D. 2018, NatCo, 9, 5209
Cameron, R., Jiang, J., Schmitt, D., \& Schüssler, M. 2010, ApJ, 719, 264
Cameron, R. H., Schmitt, D., Jiang, J., \& Işık, E. 2012, A\&A, 542, A127
Dash, S., Bhowmik, P., Athira, B., Ghosh, N., \& Nandy, D. 2020, ApJ, 890, 37
Getachew, T., Virtanen, I., \& Mursula, K. 2019, ApJ, 874, 116
Gosain, S., Pevtsov, A., Rudenko, G., \& Anfinogentov, S. 2013, ApJ, 772, 52
Hale, G. E., Ellerman, F., Nicholson, S. B., \& Joy, A. H. 1919, ApJ, 49, 153
Hale, G. E., \& Nicholson, S. B. 1925, ApJ, 62, 270
Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, Natur, 585, 357
Henney, C., Hock, R., Schooley, A., et al. 2015, SpWea, 13, 141
Hickmann, K. S., Godinez, H. C., Henney, C. J., \& Arge, C. N. 2015, SoPh, 290, 1105
Hoeksema, J. T., \& Scherrer, P. H. 1986, SoPh, 105, 205
Howard, R., Bumba, V., \& Smith, S. F. 1969, Atlas of Solar Magnetic Fields (Washington, DC: Carnegie Institution)
Hunter, J. D. 2007, CSE, 9, 90
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Jeong, H.-J., Moon, Y.-J., Park, E., Lee, H., \& Baek, J.-H. 2022, ApJS, 262, 50
Jiang, J., Hathaway, D., Cameron, R., et al. 2014, SSRv, 186, 491
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kutsenko, A. S. 2021, MNRAS, 500, 5159

Lin, L. I.-K. 1989, Biometrics, 45, 255
Lee, S., Ji, E.-Y., Moon, Y.-J., \& Park, E. 2021, SpWea, 19, e2020SW002600
Leighton, R. B. 1964, ApJ, 140, 1547
Mackay, D. H., \& Yeates, A. R. 2012, LRSP, 9, 1
Mao, X., Li, Q., Xie, H., et al. 2017, in 2017 IEEE Int. Conf. on Computer Vision (Piscataway, NJ: IEEE), 2813
Marnerides, D., Bashford-Rogers, T., \& Debattista, K. 2021, Senso, 21, 4032
Nandy, D., Baruah, Y., Bhowmik, P., et al. 2023, JASTP, 248, 106081
Nandy, D., Bhowmik, P., Yeates, A. R., et al. 2018, ApJ, 853, 72
Owens, M. J., Horbury, T., Wicks, R., et al. 2014, SpWea, 12, 395
Pal, S., Bhowmik, P., Mahajan, S. S., \& Nandy, D. 2023, ApJ, 953, 51
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Paszke, A., Gross, S., Massa, F., et al. 2019, in Advances in Neural Information Processing Systems 32, ed. H. Wallach et al. (NeurIPS), https://papers. neurips.cc/paper_files/paper/2019/faah/bdbca288fee7f92f2bfa9f7012727740Abstract.html
Price-Whelan, A. M., Sipőcz, B., Günther, H., et al. 2018, AJ, 156, 123
Qiang, Y., Fei, S., Jiao, Y., \& Li, L. 2020, JPhCS, 1627, 012014
Rahman, S., Shin, S., Jeong, H.-J., et al. 2023, ApJ, 948, 21
Ramunno, F. P., Jeong, H.-J., Hackstein, S., et al. 2024, arXiv:2407.11659
Rana, A., Singh, P., Valenzise, G., et al. 2019, ITIP, 29, 1285
Robitaille, T. P., Tollerud, E. J., Greenfield, P., et al. 2013, A\&A, 558, A33
Ronneberger, O., Fischer, P., \& Brox, T. 2015, Medical Image Computing and Computer-assisted Intervention-MICCAI 2015 (Cham: Springer), 234
Sara, U., Akter, M., \& Uddin, M. S. 2019, J. Comput. Commun., 7, 8
Schrijver, C. J., \& Title, A. M. 2001, ApJ, 551, 1099
Shorten, C., \& Khoshgoftaar, T. M. 2019, J. Big Data, 6, 1
Snodgrass, H. B., \& Ulrich, R. K. 1990, ApJ, 351, 309
Sun, X., Liu, Y., Hoeksema, J., Hayashi, K., \& Zhao, X. 2011, SoPh, 270, 9
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Upton, L., \& Hathaway, D. H. 2013, ApJ, 780, 5
Vallejos, R., Pérez, J., Ellison, A. M., \& Richardson, A. D. 2020, SpaSt, 40, 100405
Van der Walt, S., Schönberger, J. L., Nunez-Iglesias, J., et al. 2014, PeerJ, 2, e453
Vieira, L. E. A., \& Solanki, S. K. 2010, A\&A, 509, A100
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Whitbread, T., Yeates, A., Muñoz-Jaramillo, A., \& Petrie, G. 2017, A\&A, 607, A76
Worden, J., \& Harvey, J. 2000, SoPh, 195, 247
Yeates, A. R., Cheung, M. C., Jiang, J., Petrovay, K., \& Wang, Y.-M. 2023, SSRv, 219, 31
Zhang, L., Zhang, L., Mou, X., \& Zhang, D. 2011, ITIP, 20, 2378"
Jaewon Lee et al 2024 - Can Solar Limb Flare Prediction Be Properly Made by Extreme-ultraviolet Intensities.pdf,"# Can Solar Limb Flare Prediction Be Properly Made by Extreme-ultraviolet Intensities? 

Jaewon Lee ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), Hyun-Jin Jeong ${ }^{2}$ (D), Kangwoo $\mathrm{Yi}^{2}$ (D), and Harim Lee ${ }^{3}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2024 April 13; revised 2024 August 4; accepted 2024 August 5; published 2024 August 19


#### Abstract

We address the question of whether the solar limb flare prediction can be properly made by EUV intensity, which has less projection effects than solar white light and magnetogram data. We develop empirical and multilayer perceptron (MLP) models to forecast the probability of a major solar limb flare within a day. We use Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) 94 and $131 \AA$ that have high correlations and large slopes with X-ray flare fluxes from 2010 to 2022. We select 240 flares stronger than or equal to the M1.0 class and located near the limb region ( $60^{\circ}$ or more in heliographic longitude). For input data, we use the limb intensity as the sum of SDO/AIA intensities in the limb region and the total intensity of the whole image. We compare the model performances using metrics such as the receiver operating characteristic-area under the curve. Our major results are as follows. First, we can forecast major solar limb flare occurrences with only SDO/AIA 94 and/or $131 \AA$ intensities. Second, our models show better probability prediction than the climatological model. Third, both empirical $(\mathrm{AUC}=0.85)$ and $\mathrm{MLP}(\mathrm{AUC}=0.84)$ models have similar performances, which are much better than a random forecast $(\mathrm{AUC}=0.50)$. Finally, it is interesting to note that our model can forecast the flaring probability of all 52 events during the test period, while the models in the NASA/CCMC flare scoreboard can forecast only 22 events. From the above results, we can answer that the solar limb flare prediction using EUV intensity can be properly made.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar extreme ultraviolet emission (1493); Solar flares (1496); Neural networks (1933)

## 1. Introduction

Solar flares are one of the most energetically significant eruptive phenomena in the solar atmosphere (Shibata \& Magara 2011). Solar flares are probable causes of solar energetic particles (SEPs), along with coronal mass ejections (CMEs; Papaioannou et al. 2016). Solar flares can occur not only on the front side but also near the limbs and backside of the Sun. The source location of SEP-associated flares tends to be close to the western limb (Dierckxsens et al. 2015). On the other hand, flares occurring near the eastern limb are difficult to predict because the evolution of active regions (ARs) on the farside is unseen from the Earth (Pangestu et al. 2023). Hence, eastern limb flare may lead to sudden radio blackouts (Yasyukevich et al. 2018). In these senses, the effort to forecast the solar limb flare is crucial from a space weather perspective. Historically, one of the most commonly used data for solar flare prediction is McIntosh sunspot group classification (McIntosh 1990). This sunspot group classification is based on solar white light and magnetogram data. The National Oceanic and Atmospheric Administration (NOAA) currently provides the McIntosh classification through the Solar Region Summary (SRS). Most flare prediction models from the Community Coordinated Modeling Center (CCMC; Hesse et al. 2001) of the National Aeronautics and Space Administration (NASA) use McIntosh classification from SRS and magnetograms from the Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) of Solar Dynamics Observatory (SDO; Pesnell et al. 2012; Leka et al. 2019, A-EFFORT;

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.

Georgoulis \& Rust 2007, AMOS; Lee et al. 2012, ASAP; Colak \& Qahwaji 2009, ASSA; Hong et al. 2014, BOM; Steward et al. 2017, DAFFS, DAFFS-G; Leka et al. 2018b, MAG4; Falconer et al. 2011, MCSTAT, MCEVOL; Gallagher et al. 2002; Bloomfield et al. 2012; McCloskey et al. 2018, MOSWOC; Murray et al. 2017, NICT; Kubo et al. 2017, NJIT; Park et al. 2010, NOAA; Crown 2012, SIDC; Berghmans et al. 2005; Devos et al. 2014). However, near the solar limb, solar magnetograms and white light data are strongly affected by projection effects adding distortion. Despite attempts to correct these projection effects through coordinate transformations, the process introduces a significant amount of error (Wilkinson et al. 1989). The inaccuracy in predicting flare occurrences near the solar limb region is attributed to the limitations of existing models (Park et al. 2020) and much of the previous research is limited to ARs in the restricted longitudes: $\leqslant 30^{\circ}$ in Cui et al. (2006) and Huang et al. (2018); $\leqslant 45^{\circ}$ in Ahmed et al. (2013), Bobra et al. (2014), Zheng et al. (2019), and Deng et al. (2021); and $\leqslant 68^{\circ}$ in Wang et al. (2020).

Here we present solar limb flare models within the next 24 hr using solar ultraviolet (EUV) data to find answers to the following question: can solar limb flare prediction be properly made by EUV intensities? The difference in optical depth between traditional input data (magnetograms and white light data) of the flare prediction models and the EUV data may provide a breakthrough in predictions of solar limb flare. Magnetogram and white light data, which reflect solar photosphere (Boyer et al. 1985) and chromosphere (Hudson 1972), are optically thick, while solar EUV data, reflecting the solar corona (Battaglia \& Kontar 2011), are optically thin. Thus, EUV data are expected to be less affected by projection effects near the solar limb compared to solar magnetograms and white light data, which results in fewer distortions in the information.
In this Letter, we present empirical and multilayer perceptron (MLP; Gardner \& Dorling 1998) models using solar EUV data to forecast the occurrence of major solar limb flares within the next 24 hr . This study is organized as follows. Specific data used in our study are covered in Section 2. The structures of the empirical and MLP models used in this study are presented in Section 3. Section 4 describes the prediction performance of each model and its comparison. Finally, in Section 5, we provide a brief summary and conclusion of our study.

## 2. Data

### 2.1. Data Selection

Our study focuses on major limb flare prediction. Thus, we need to define ""major"" and ""limb"" for the data selection. Solar flares are classified as $\mathrm{A}, \mathrm{B}, \mathrm{C}, \mathrm{M}$, and X based on the soft X-ray $(0.1-0.8 \mathrm{~nm})$ flux and X -class flares having the highest flux. In this study, we define major flares as those with a class of M1.0 or higher. A limb flare is defined when its source location is at a heliographic longitude of $60^{\circ}$ or more. We have a couple of reasons for this longitudinal criterion. First, there are serious projection effects near the limb: misidentification of sunspot classification and magnetic field vectors (Venkatakrishnan \& Gary 1989). Second, many flare studies and prediction models are based on the data within a heliographic longitude of $60^{\circ}$ to minimize the projection effect near the solar limb region (Park et al. 2010; Falconer et al. 2011; Liu et al. 2012; Lim et al. 2019). Following these criteria, we collect flare data from the Heliophysics Event Knowledgebase (HEK; Hurlburt et al. 2012) provided by Lockheed Martin Solar and Astrophysics Laboratory (LMSAL). We use solar EUV data from the Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) instrument on SDO. Among the seven EUV wavelengths ( $94,131,171,193,211,304$, and $335 \AA$ ) of SDO/AIA, we select 94 and $131 \AA$ since they represent high-temperature coronal regions, and their peak intensities have stronger correlations having larger slopes with Geostationary Operational Environmental Satellite (GOES; Menzel \& Purdom 1994) X-ray fluxes than other wavelengths (Van der Sande et al. 2022).

### 2.2. Data Collection and Preprocessing

The EUV data are collected from 2010 May 13 to 2022 December 31 at UT 00:00 each day, and if a major limb flare occurs within the subsequent 24 hr from the observation time, they are labeled as flaring samples. We remove the data whose data are partially saturated due to solar activity such as strong flares using the information given in their FITS header. Our models consider two types of solar EUV data: (1) total intensity, which is the sum of intensity for all pixels, not only in the solar disk, and (2) limb intensity, which is the sum of intensity for pixels in regions with heliographic longitude of $60^{\circ}$ or more. The limb intensity also includes the outer region of the solar disk. We anticipate that our models can predict flare occurrences in the limb region by considering variations in limb intensity relative to total intensity. As a result, we can get a total of 4222 nonflaring data and 240 flaring data. We divide the data into train and test data sets to construct the empirical and MLP models, considering both flare occurrences and the solar cycle. For each year, April, August, and December are used for testing. February and June serve as the validation set for MLP models. The remaining months are used for training.

To accommodate computing resources, we downsample the original FITS data from $4096 \times 4096$ pixels to $1024 \times 1024$ pixels while fixing the solar radius to 392 pixels (Park et al. 2019). Then we divide all SDO/AIA pixel intensities by the exposure time to ensure consistent exposure conditions. We also perform the north-up and recentering processes during the resizing. To mitigate bias toward large values, we apply a logarithm transformation to both summed intensities.

Optical instruments like SDO/AIA suffer degradation in their sensitivity over time (Boerner et al. 2012). To consider long-term data, we should correct the degradation effect. Conventionally, degradation can be corrected by cross calibration between SDO/AIA and the Extreme Ultraviolet Variability Experiment (EVE; Woods et al. 2012) of SDO (Boerner et al. 2014). SDO/EVE is also calibrated with observations of NASA's sounding rockets (Wieman et al. 2016). This approach is not problematic when using data within the solar disk. However, utilizing data from outside the solar disk for multiple channels, there are considerable discrepancies.

Figure 1 shows the distribution of summed intensities at 94 ( $y$-axis) and $131 \AA(x$-axis). After applying traditional degradation correction using the AIA response table (Boerner et al. 2014) version 10 from the Joint Science Operations Center (JSOC), the ratios of summed intensities inside the solar disk show a relatively constant at the two wavelengths across years (see panel (a) of Figure 1). However, when adding the intensities outside the disk, the ratios of the limb intensities at the two wavelengths for each year are divided into two groups, as shown in panel (b) of Figure 1. This implies that the ratio varies over time, which can potentially impact the performance of models using multichannel inputs. To address this issue, we cross-calibrate all data using the nonflaring data of 2011, which is close to the starting time of SDO operation. Fitting the data to the 2011 nonflaring data set shows a quite good coherent relationship, as shown in the right panel (c) of Figure 1, demonstrating that the combined use of multichannel data has been successfully made.

## 3. Method

### 3.1. Empirical and MLP Models

We develop two kinds of solar limb flare models: an empirical model and an MLP one. To construct the empirical model, we examine the total intensity and limb intensity of 94 and $131 \AA$ for the training data set and divide them into several bins. Then we investigate the probability of flare occurrence in each bin, which is carried out by dividing the number of flaring data by the number of all samples for each bin. To provide meaningful probabilistic predictions, each bin should have the counts of sufficient samples. In this study, each bin contains a minimum of 24 samples, corresponding to $10 \%$ of the total flaring data. To satisfy this, the size of all bins is fixed at 0.5 $[\log (\mathrm{DN} / \mathrm{s})]$. We use the same bins of the training data set for test data. Out of a total of 4222 nonflaring data and 240 flaring data, 1077 and 52 of each were used for testing our empirical model performance.

Our MLP model takes the limb intensity and total intensity of 94 and $131 \AA$ as input. It is developed using supervised learning, where the flare occurrence status serves as the label. We train MLP models using 2472 nonflare samples and 161 flare samples and validate them using 673 nonflare samples and 27 flare samples. We use the same test data as the empirical
![img-0.jpeg](img-0.jpeg)

Figure 1. The distribution of summed intensities at 94 (y-axis) and 131 Å (x-axis). The data of each year are represented by a different color. Cross and circle symbols represent flaring and nonflaring samples, respectively. Panel (a) is the summation of pixels inside the solar disk. Panel (b) is the defined limb intensity and panel (c) is the cross-calibrated one by the 2011 nonflaring data set. All data are corrected by AIA response table version 10 and presented on a logarithmic scale.

model. Our MLP model consists of two layers with 128 nodes, batch normalization (Ioffe & Szegedy 2015), activation function, and dropout. Batch normalization and dropout prevent the model from biasing specific values or nodes. Nonlinear rectified linear unit (Nair & Hinton 2010) activation functions are used to deal with nonlinearity. The input data processed through these layers undergo a sigmoid function to generate a value between 0 and 1, representing the probability of a flare occurrence. If the model's output probability is equal to or greater than the threshold, it is classified as a flare occurrence; otherwise, it is classified as nonflaring in a binary classification. The loss is calculated using the binary cross entropy (BCE; Cox 1958) loss function, which considers the model's predicted results against the actual flare occurrence status. Through multiple epochs, the model updates the weights associated with connections between each node to reduce the BCE loss.

When the data set is imbalanced, the BCE loss tends to be biased toward the dominant set of data. In such cases, the probabilities predicted by the model are limited to a narrower range (e.g., [0, 0.3]) instead of [0, 1]. Additionally, since the metric results used to evaluate the model's performance vary depending on the threshold, there is also a need to find an appropriate threshold. Meanwhile, the Score-Oriented Loss (SOL; Guastavino et al. 2022a; Marchetti et al. 2022) function, which simultaneously performs score maximization and minimization of loss during the training process, has no need to find a threshold where the metric is maximized. In the process of optimizing a metric that is effective for imbalanced data sets, the model's probabilities would be distributed between 0 and 1. However, the probabilistic predictions made by a model trained with SOL are mainly clustered around 0 or 1, which would be like deterministic classification rather than probabilistic prediction. The BCE loss allows for a relatively continuous distribution of predicted probabilities. Since each approach has its own advantages and disadvantages, and the probabilistic prediction is just as important as deterministic prediction for risk assessment in operations, we present both results: BCE loss and SOL. Here the optimal thresholds are obtained from the train data set for each metric, which will be described in the next section (3.2). We also have tried four types of models: (1) simple convolutional neural networks (Li et al. 2021) models using EUV images, (2) convolutional block attention module (Woo et al. 2018) architectures that incorporate attention mechanisms (Yohanandan et al. 2018), (3) Deep Flare Net (DeFN. Nishizuka et al. 2018), which is an MLP-based residual network (He et al. 2016), and (4) DeFN-R (Nishizuka et al. 2020) architectures. Since our simple MLP model is better than the other model architectures from the perspective of the Brier skill score (BSS; Brier 1950; Nishizuka et al. 2020, Section 3.2), we will mainly present the MLP model.

### 3.2. Metrics for Evaluation

The evaluation of our models uses various performance metrics commonly employed in many flare prediction studies. These include deterministic prediction metrics such as true skill statistics (TSS; Allouche et al. 2006), the Heidke skill score (HSS; Heidke 1926), and the receiver operating characteristic —area under the curve (ROC-AUC; Fawcett 2006). The probabilistic prediction metric such as BSS is also involved. TSS is given by

$$
\text{TSS} = \frac{\text{TP}}{\text{TP} + \text{FN}} - \frac{\text{FP}}{\text{FP} + \text{TN}} = \text{POD} - \text{POFD}
$$

where TP, FN, FP, and TN are true positive (hit), false negative (miss), false positive (false alarm), and true negative (correct rejection). Here, POD and POFD represent the probability of detection, and the probability of false detection, respectively. TSS ranges from −1 to 1, where 1 represents a perfect prediction. TSS is a comprehensive metric that evaluates prediction performance by considering both the POD and POFD. It is useful in binary
![img-1.jpeg](img-1.jpeg)

Figure 2. The flare occurrence rates of the test data set as a function of 94 Å limb intensity (a) and 131 Å total intensity (b). The ratios above each bin indicate the number of flaring events divided by that of the total ones.

Classification tasks and exhibits robustness against imbalanced data sets (i.e., flare class) compared to other metrics (Guastavino et al. 2022b). On the other hand, HSS is defined by

$$
\text{HSS} = \frac{2[(\text{TP} \times \text{TN}) - (\text{FP} \times \text{FN})]}{(\text{TP} + \text{FN})(\text{FN} + \text{TN}) + (\text{TP} + \text{FP})(\text{FP} + \text{TN})}
$$

(2)

HSS ranges from $-\infty$ to 1, and it represents a perfect prediction when it equals 1. HSS is useful as it measures the relative improvement of predictions compared to random guessing across various data sets (Hyvärinen 2014). ROCAUC represents the area under the curve when plotted as the ROC curve with POFD on the $x$-axis and POD on the $y$-axis. It ranges from 0 to 1, the closer the value is to 1, the better the model is at classification. For a random classifier, ROC-AUC is 0.5. BSS is given by

$$
\text{BSS} = \frac{\text{BS} - \text{BS}_C}{0 - \text{BS}_C} = 1 - \frac{\text{BS}}{\text{BS}_C}
$$

(3)

where BS is the Brier score of our model and BS is the BS of the climatological model. In this study, we use the climatological model as a reference model of BSS calculation, where the climatological model is a model that predicts the probability of event occurrence for all samples as equal to the event rate of the test data set. The BS and BS are expressed by

$$
\text{BS} = \sum_{n=1}^{N} (p(y_n) - y_n^\alpha)^{2}
$$

$$
\text{BS}_C = \sum_{n=1}^{N} (S - y_n^\alpha)^{2},
$$

(5)

where $N$, $p(y_n)$, and $y_n^\alpha$ correspond to the total number of samples, model's prediction, and observed value, respectively. Here S indicates the actual event rate of the data. S is given by

$$
S = \frac{\text{TP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}},
$$

BSS ranges from -1 to 1, and the unity is only attainable in a perfect deterministic prediction. To compare with the probabilistic prediction of flare occurrences from models in the NASA/CCMC flare scoreboard, we used the BSS as the criterion for selecting the best model.

## 4. Result and Discussion

### 4.1. Comparison between Empirical and MLP Models

We combine two wavelengths (94 and 131 Å) and two types of intensities (total intensity and limb intensity) to construct various empirical and MLP models, comparing them using BSS to find the best model. The best empirical model utilizes only the limb intensity of 94 Å, yielding a BSS = 0.11.

Figure 2 shows the flare occurrence rates of the test data set as a function of 94 Å limb intensity and 131 Å total intensity. As shown in Figure 2, the probability distribution of 94 Å limb intensity is more contrasted than that of 131 Å total intensity. In the case of 94 Å limb intensity, the Pearson correlation coefficient (CC; Pearson 1895) with the flare occurrence ratio is 0.91 for the training data set and 0.94 for the test data set, respectively. Additionally, the CC of the flare occurrence probabilities between the train data set and test data set is approximately 0.99, demonstrating that the test data sets have a similar trend with the training ones.

The best MLP model (described in Section 3 and using BCE loss) is utilizing the limb intensity and total intensity of 94 and 131 Å, with a BSS = 0.10. It is interesting to note that the simplest MLP model shows the best performance. We think of a possible reason as follows. As observed in the empirical
Table 1
The Contingency Tables of Our Empirical and MLP Models with BCE or SOL, along with the Thresholds (@th) Optimized to TSS and HSS

| TSS optimized | Empirical <br> (@th $=5.65)$ | MLP (BCE) <br> (@th $=0.07)$ | MLP (SOL) <br> (@th $=0.42)$ |
| :-- | :--: | :--: | :--: |
| TSS | 0.53 | 0.53 | 0.54 |
| HSS | 0.14 | 0.15 | 0.18 |
| TP | 43 | 42 | 39 |
| FP | 322 | 296 | 231 |
| FN | 9 | 10 | 13 |
| TN | 755 | 781 | 846 |
| HSS optimized | Empirical <br> (@th $=5.77)$ | MLP (BCE) <br> (@th $=0.18)$ | MLP (SOL) <br> (@th $=0.73)$ |
| TSS | 0.32 | 0.36 | 0.45 |
| HSS | 0.27 | 0.27 | 0.26 |
| TP | 19 | 22 | 28 |
| FP | 54 | 68 | 99 |
| FN | 33 | 30 | 24 |
| TN | 1023 | 1009 | 978 |

model, intensities at 94 and $131 \AA$ of EUV wavelengths exhibit high correlations with flare occurrence probability. Hence, they may not require complex nonlinearity, and a simpler model structure can perform well. On the other hand, unlike the empirical model, the MLP model achieves the best BSS when both intensities of the two EUV wavelengths are used. It may be attributed to the ability of the MLP model's architecture to effectively extract features from each wavelength and combine them better, rather than simply combining the data into 2D data as in the empirical model. In Equation (3), the BSS is ultimately related to the ratio of mean square error (MSE) between our model and the reference climatological model, which forecasts all the instances as the flaring ratio of the test data set ( 0.048 ). Our models show better probabilistic predictions than the climatological model. It is also confirmed that there is not a significant difference in the performances of the empirical and MLP models from a probabilistic forecasting perspective.

We also compare the deterministic prediction performances of our empirical and MLP models. When examining the model's performance by changing the threshold of deterministic prediction, we find that HSS and TSS have slightly different thresholds at which they reach their maximum values.

Table 1 shows the best HSS and TSS of our empirical and MLP models with BCE or SOL, along with their corresponding thresholds. For TSS, the empirical model achieves TSS $=0.53$ with a threshold of 5.65 , while the MLP model with SOL reaches a maximum TSS $=0.54$ with a probability threshold of 0.42 . As for HSS, the empirical model obtains HSS $=0.27$ with a threshold of 5.77 , and the MLP model with BCE shows HSS $=0.27$ with a probability threshold of 0.18 . Even when optimized for TSS and HSS, MLP models using the SOL function show better performance in terms of HSS and TSS compared to the MLPs using BCE or empirical models. This is because the SOL function leads to less variation in the contingency table components (TP, FP, FN, TN) depending on the threshold, making it more robust. Both TSS and HSS show that there is no significant difference in the performance among the models.

Figure 3 shows the ROC-AUC of our models and random forecast. The AUC values of our empirical and MLP models
![img-2.jpeg](img-2.jpeg)

Figure 3. ROC curves of our empirical and MLP models. Both the empirical and the MLP models show better performance than a random forecast, and the difference in performance between the two models is not significant.
are 0.85 and 0.84 , respectively, while the random forecast has $\mathrm{AUC}=0.50$. Similar to the previous results, there is no significant difference in performance between the two models.

### 4.2. Comparison between Our Model and NASA/CCMC Flare Scoreboard

Our model can provide not only deterministic prediction expressed as ""yes"" or ""no"" but also probabilistic prediction as previously shown with BSS. For the more objective evaluations, as well as the performance comparisons between the empirical and MLP models, we compare the results of our model with those of the existing models in the NASA/CCMC flare scoreboard. Most existing models in the NASA/CCMC flare scoreboard forecast the probability of flares with M-class or higher within 24 hr like our model. It is noted that few models show the probability of limb flares. Hence, we match the source locations of limb flares during the test period according to the NASA/CCMC flare scoreboard. For our comparison, we use our MLP model with BCE loss since its results are similar to those of the empirical one and give us relatively continuous probabilities. If at least one model in the NASA/CCMC flare scoreboard provides a prediction for ARs near the solar limb regions at UT 00:00 before the flare onset, we conduct a comparison. If multiple predictions are available for the NASA/CCMC flare scoreboard, we compare the average probability of those predictions with our model's probability. It is very impressive to say that out of a total of 52 events, 30 events (about $60 \%$ ) are not provided by the NASA/ CCMC flare scoreboard. Among these, 24 events occur near the solar eastern limb and 6 near the western limb. Only 22 events are available for comparison, with 5 occurring near the eastern limb and 17 near the western limb.

Figure 4 shows the scatterplot for these 22 events. For 9 events, the NASA/CCMC flare scoreboard has a higher average flare occurrence probability, while our MLP model's probability is higher for the remaining 13 events. For about
![img-3.jpeg](img-3.jpeg)

Figure 4. Scatterplot of 22 events for the available models in NASA/CCMC flare scoreboard. The x-axis represents the average flare occurrence probability of the NASA/CCMC flare scoreboard, and the y-axis represents the flare occurrence probability predicted by our MLP model.

53% of the western limb flares, our model predicts higher probabilities than those of the NASA/CCMC flare scoreboard. For the eastern limb flares, this rate is 80%. It is noted that flares occurring near the eastern limb are relatively more challenging to predict using existing flare models such as ones in NASA/CCMC flare scoreboard, as the ARs associated with them may not be visible on magnetogram and white light data. In contrast, solar EUV data can forecast flare occurrence near the eastern limb, even before they have significant information on possible flaring activity since they are optically thin. For flares occurring near the western limb of the Sun, the corresponding ARs may be better observed using white light and magnetogram data that are conventionally used for solar flare prediction. When calculating BSS values using the climatological model as a reference, we find that our MLP model and the NASA/CCMC flare scoreboard achieve BSS values of 0.30 and 0.22, respectively. This indicates that our model can provide relatively accurate probabilistic predictions of limb flares compared to the NASA/CCMC flare scoreboard for the 22 events.

# 5. Summary and Conclusion

In this study, we present empirical and MLP models using solar EUV data to forecast the occurrence of major solar limb flares within the next 24 hr. Our major results are summarized as follows. First, without magnetogram or white light data, we can forecast major solar limb flares using EUV data. There are strong positive linear correlations between the EUV intensities and the probabilities of flare occurrence. This can be attributed to the use of 94 and 131 Å whose peak intensities have strong correlations having large slopes with GOES X-ray fluxes. Second, when compared to the reference climatological model, which has an event ratio of 0.048, both empirical and MLP models show better probabilistic predictions. Third, the empirical model (AUC = 0.85) and the MLP model (AUC = 0.84) outperform the random forecast (AUC = 0.50) for ROC-AUC. Additionally, both models show very similar performance from the perspective of TSS, HSS, BSS, and ROC-AUC. Finally, when comparing the flare probability predictions between the NASA/CCMC flare scoreboard and our models, we find that 30 events are not provided by the NASA/CCMC flare scoreboard, while our model can provide probability predictions for all 52 events. It is notable that for the flares occurring near the eastern limb of the Sun, our model tends to predict a higher probability.

Through the above results, we can conclude the following facts. First, for the first time, we provide the major solar limb flare prediction models using solar EUV data. They can offer the probability of solar flare occurrences near the limb regions, which are not provided by the traditional flare prediction models. Furthermore, since there was no study specifically targeting solar limb flare prediction before, we can anticipate that the results of this study can serve as a baseline model for future solar limb flare studies. Second, we compare our empirical and MLP models, and their performances do not show significant differences. In the case of solar limb flare occurrence probabilities, as observed in the results of the empirical model, there is already a strong linear correlation with the EUV intensities. Hence, MLP, which excels in solving nonlinear relationships, does not show a noticeably higher performance. The imbalance of the flare data, which is dominated by the nonflaring samples is a problem that we must solve to improve MLP. To solve this problem, we test the following methods: weighted cross-entropy loss, SOL function, and oversampling, but a trade-off occurs in which the forecast performance for the nonflaring data deteriorates. Finally, our model can provide predictions for events that the NASA/CCMC flare scoreboard models could not forecast. Existing models, due to distortions near the solar limb in magnetogram and white light data, provide predictions for a limited longitudinal range of ARs. In other words, predictions are often not provided for ARs close to the solar limb. However, our model can provide flare predictions for all cases since it can predict the probability of major solar limb flare occurrences based on the defined intensity from EUV data, regardless of the location of the ARs. Moreover, from the perspective of BSS for comparable events, our models show higher performance than the average probability of the models in the NASA/CCMC flare scoreboard, indicating that the use of EUV data on limb flare prediction may be more effective than traditional input data such as magnetogram and white light, especially for eastern limb events.

From an operational perspective, our models cannot only forecast solar limb flares but also construct a part of an ensemble model together with existing flare models. It would potentially provide better forecasting outcomes for flare occurrence near the solar limb. When future deep space missions like L4 (Cho et al. 2023) and L5 (Palomba & Luntama 2022) combine observational data, a comprehensive study on the front, limb, and backside flares of the Sun can be conducted. We can expect that the methods proposed in this study, along with solar EUV data, have the potential to guide diverse directions for that study.

## Acknowledgments

We thank those who contributed to the SDO mission and successful AIA, EVE, and HMI operations. We give a grant to the community's devotion to developing the open-source
packages that were used in this work such as Python, Numpy, Matplotlib, Pandas, SunPy, and Pytorch. We also acknowledge the CCMC at Goddard Space Flight Center (GSFC) of NASA for the use of the Flare Scoreboard (https://ccmc.gsfc.nasa.gov/ scoreboards/flare/), and the LMSAL for the use of flare data from HEK. This work was supported by the BK21 FOUR program through National Research Foundation of Korea (NRF) under Ministry of Education (MoE) (Kyung Hee University, Human Education Team for the Next Generation of Space Exploration), the Korea Astronomy and Space Science Institute under the R\&D program (Project No. 2024-1-850-12) supervised by the Ministry of Science and ICT (MSIT), Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korean government MSIT (No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15\%), and Basic Science Research Program through the NRF funded by the MoE (NRF-2021R1I1A1A01049615, NRF-2021R1A6A3A01088835, and RS-2023-00248916).

Facility: SDO (AIA)
Software: Python 3 (Van Rossum \& Drake 2009), Numpy (Harris et al. 2020), Matplotlib (Hunter 2007), Pandas (McKinney 2011), SunPy (Mumford et al. 2020), Pytorch (Paszke et al. 2019).

## ORCID IDs

Jaewon Lee (2) https://orcid.org/0000-0002-8287-956X
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Hyun-Jin Jeong (2) https://orcid.org/0000-0003-4616-947X
Kangwoo Yi (2) https://orcid.org/0000-0003-4342-9483
Harim Lee (2) https://orcid.org/0000-0002-9300-8073

## References

Ahmed, O. W., Qahwaji, R., Colak, T., et al. 2013, SoPh, 283, 157
Allouche, O., Tsoar, A., \& Kadmon, R. 2006, JApEc, 43, 1223
Battaglia, M., \& Kontar, E. P. 2011, A\&A, 533, L2
Berghmans, D., van der Linden, R. A. M., Vanlommel, P., et al. 2005, AnGeo, 23, 3115
Bloomfield, D. S., Higgins, P. A., McAteer, R. J., \& Gallagher, P. T. 2012, ApJL, 747, L41
Boerner, P., Testa, P., Warren, H., Weber, M., \& Schrijver, C. 2014, SoPh, 289, 2377
Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SoPh, 289, 3549
Boerner, P., Edwards, C., Lemen, J., et al. 2012, SoPh, 275, 41
Boyer, R., Machado, M., Rust, D., \& Sotirovski, P. 1985, SoPh, 98, 255
Brier, G. W. 1950, MWRv, 78, 1
Cho, K.-S., Hwang, J., Han, J.-Y., et al. 2023, JKAS, 56, 263
Colak, T., \& Qahwaji, R. 2009, SpWea, 7, S06001
Cox, D. R. 1958, J. R. Stat. Soc. Ser. B Stat. Method, 20, 215
Crown, M. D. 2012, SpWea, 10, S06006
Cui, Y., Li, R., Zhang, L., He, Y., \& Wang, H. 2006, SoPh, 237, 45
Deng, Z., Wang, F., Deng, H., et al. 2021, ApJ, 922, 232
Devos, A., Verbeeck, C., \& Robbrecht, E. 2014, JSWSC, 4, A29
Dierckxsens, M., Tziotziou, K., Dalla, S., et al. 2015, SoPh, 290, 841
Falconer, D., Barghouty, A. F., Khazanov, I., \& Moore, R. 2011, SpWea, 9, S04003
Fawcett, T. 2006, PaReL, 27, 861
Gallagher, P. T., Moon, Y.-J., \& Wang, H. 2002, SoPh, 209, 171
Gardner, M. W., \& Dorling, S. 1998, AtmEn, 32, 2627
Georgoulis, M. K., \& Rust, D. M. 2007, ApJ, 661, L109
Guastavino, S., Marchetti, F., Benvenuto, F., Campi, C., \& Piana, M. 2022a, A\&A, 662, A105
Guastavino, S., Piana, M., \& Benvenuto, F. 2022b, ITNN, 35, 1993

Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585,357
He, K., Zhang, X., Ren, S., \& Sun, J. 2016, in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (Piscataway, NJ: IEEE), 770
Heidke, P. 1926, GeAnA, 8, 301
Hesse, M., Bellaire, P., \& Robinson, R. 2001, in Proc. of the Space Weather Workshop: Looking Toward a European Space Weather Programme, ed. R. Gendrin et al. (Paris: ESA), 17

Hong, S., Kim, J., Han, J., \& Kim, Y. 2014, AGUFM, 2014, SH21A
Huang, X., Wang, H., Xu, L., et al. 2018, ApJ, 856, 7
Hudson, H. S. 1972, SoPh, 24, 414
Hunter, J. D. 2007, CSE, 9, 90
Hurlburt, N., Cheung, M., Schrijver, C., et al. 2012, SoPh, 275, 67
Hyvärinen, O. 2014, WiFor, 29, 177
Ioffe, S., \& Szegedy, C. 2015, arXiv:1502.03167
Kubo, Y., Den, M., \& Ishii, M. 2017, JSWSC, 7, A20
Lee, K., Moon, Y.-J., Lee, J.-Y., Lee, K.-S., \& Na, H. 2012, SoPh, 281, 639
Leka, K., Barnes, G., \& Wagner, E. 2018b, JSWSC, 8, A25
Leka, K., Park, S.-H., Kusano, K., et al. 2019, ApJ, 881, 101
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Li, Z., Liu, F., Yang, W., Peng, S., \& Zhou, J. 2021, ITNN, 33, 6999
Lim, D., Moon, Y.-J., Park, J., et al. 2019, JKAS, 52, 133
Liu, Y., Hoeksema, J., Scherrer, P., et al. 2012, SoPh, 279, 295
Marchetti, F., Guastavino, S., Piana, M., \& Campi, C. 2022, PatRe, 132, 108913
McCloskey, A. E., Gallagher, P. T., \& Bloomfield, D. S. 2018, JSWSC, 8, A34
McIntosh, P. S. 1990, SoPh, 125, 251
McKinney, W. 2011, Python for High Performance and Scientific Computing, 14,1
Menzel, W. P., \& Purdom, J. F. 1994, BAMS, 75, 757
Mumford, S., Freij, N., Christe, S., et al. 2020, JOSS, 5, 1832
Murray, S. A., Bingham, S., Sharpe, M., \& Jackson, D. R. 2017, SpWea, 15, 577
Nair, V., \& Hinton, G. E. 2010, in Proc. of the 27th Int. Conf. on Machine Learning (ICML-10), ed. J. Furnkranz \& T. Joachims (Madison, WI: Omnipress), 807
Nishizuka, N., Kubo, Y., Sugiura, K., Den, M., \& Ishii, M. 2020, ApJ, 899, 150
Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., \& Ishii, M. 2018, ApJ, 858, 113
Palomba, M., \& Luntama, J.-P. 2022, in COSPAR Scientific Assembly, 44, 3544, https://www.cosparathens2022.org/
Pangestu, A. D., Muhamad, J., Nurzaman, M. Z., et al. 2023, JApA, 44, 25
Papaioannou, A., Sandberg, I., Anastasiadis, A., et al. 2016, JSWSC, 6, A42
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, S.-H., Chae, J., \& Wang, H. 2010, ApJ, 718, 43
Park, S.-H., Leka, K., Kusano, K., et al. 2020, ApJ, 890, 124
Paszke, A., Gross, S., Massa, F., et al. 2019, arXiv:1912.01703
Pearson, K. 1895, RSPS, 58, 240
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. 2012, The Solar Dynamics Observatory (SDO) (Berlin: Springer)
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Shibata, K., \& Magara, T. 2011, LRSP, 8, 6
Steward, G., Lobzin, V., Cairns, I. H., Li, B., \& Neudegg, D. 2017, SpWea, 15, 1151
Van der Sande, K., Flyer, N., Berger, T. E., \& Gagnon, R. 2022, FrASS, 9, 1031211
Van Rossum, G., \& Drake, F. L. 2009, Python 3 Reference Manual (Scotts Valley, CA: CreateSpace)
Venkatakrishnan, P., \& Gary, G. A. 1989, SoPh, 120, 235
Wang, X., Chen, Y., Toth, G., et al. 2020, ApJ, 895, 3
Wieman, S., Didkovsky, L., Woods, T., Jones, A., \& Moore, C. 2016, SoPh, 291, 3567
Wilkinson, L. K., Emslie, A. G., \& Gary, G. A. 1989, SoPh, 119, 77
Woo, S., Park, J., Lee, J.-Y., \& Kweon, I. S. 2018, in Proc. of the European Conf. on Computer Vision (ECCV), ed. V. Ferrari, M. Hebert, C. Sminchisescu, \& Y. Weiss (Springer), 3

Woods, T. N., Eparvier, F., Hock, R., et al. 2012, SoPh, 275, 115
Yasyukevich, Y., Astafyeva, E., Padokhin, A., et al. 2018, SpWea, 16, 1013
Yohananalan, S., Song, A., Dyer, A. G., \& Tao, D. 2018, in Proc. of the European Conf. on Computer Vision (ECCV), ed. V. Ferrari et al. (Cham: Springer), 235
Zheng, Y., Li, X., \& Wang, X. 2019, ApJ, 885, 73"
Kyoung-Sun Lee et al 2022 - Deep Learning–based Fast Spectral Inversion of Hα and Ca ii 8542 Line Spectra.pdf,"# Deep Learning-based Fast Spectral Inversion of $\mathrm{H} \alpha$ and Ca II 8542 Line Spectra 

Kyoung-Sun Lee ${ }^{1}$ (D) Jongchul Chae ${ }^{1}$ (D), Eunsu Park ${ }^{2}$ (D), Yong-Jae Moon ${ }^{3}$ (D), Hannah Kwak ${ }^{1,2}$ (D), and Kyuhyoun Cho ${ }^{1,4,5}$ (D)<br>${ }^{1}$ Astronomy Program, Department of Physics and Astronomy, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul 08826, Republic of Korea<br>lksun@astro.snu.ac.kr, rofritsun@gmail.com<br>${ }^{2}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon 305-348, Republic of Korea<br>${ }^{3}$ School of Space Research, Kyung Hee University, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea<br>${ }^{4}$ Bay Area Environmental Research Institute, NASA Research Park, Moffett Field, CA 94035, USA<br>${ }^{5}$ Lockheed Martin Solar \& Astrophysics Laboratory, 3251 Hanover Street, Palo Alto, CA 94304, USA<br>Received 2022 January 1; revised 2022 October 10; accepted 2022 October 16; published 2022 November 30


#### Abstract

A multilayer spectral inversion (MLSI) model has recently been proposed for inferring the physical parameters of plasmas in the solar chromosphere from strong absorption lines taken by the Fast Imaging Solar Spectrograph (FISS). We apply a deep neural network (DNN) technique in order to produce the MLSI outputs with reduced computational costs. We train the model using two absorption lines, $\mathrm{H} \alpha$ and Ca II $8542 \AA$, taken by FISS, and 13 physical parameters obtained from the application of MLSI to 49 raster scans ( $\sim 2,000,000$ spectra). We use a fully connected network with skip connections and multi-branch architecture to avoid the problem of vanishing gradients and to improve the model's performance. Our test shows that the DNN successfully reproduces the physical parameters for each line with high accuracy and a computing time of about $0.3-0.4 \mathrm{~ms}$ per line, which is about 250 times faster than the direct application of MLSI. We also confirm that the DNN reliably reproduces the temporal variations of the physical parameters generated by the MLSI inversion. By taking advantage of the high performance of the DNN, we plan to provide physical parameter maps for all the FISS observations, in order to understand the chromospheric plasma conditions in various solar features.


Unified Astronomy Thesaurus concepts: Solar chromosphere (1479); Neural networks (1933); Spectroscopy (1558)

## 1. Introduction

The solar chromosphere is the interface atmospheric layer between the photosphere and the corona, which is highly dynamic, finely structured, and complex. In order to understand the phenomena that occur in the chromosphere and their varied physical drivers, it is necessary to quantitatively diagnose the physical properties of chromospheric plasmas. We can infer the physical conditions in the chromosphere through the analysis of strong lines, such as those of $\mathrm{H}, \mathrm{Ca} \mathrm{II}$, and Mg II , in the visible, near-IR, and UV spectral regimes (Carlsson et al. 2019). These chromospheric lines are optically thick and formed under conditions of non-local thermodynamic equilibrium (NLTE), which requires NLTE radiative transfer modeling. The forward modeling makes use of simulations, a realistic 3D radiative magnetohydrodynamic model (Bifrost; Gudiksen et al. 2011), and NLTE radiative transfer (RHUitenbroek 2001; RADYN—Allred et al. 2005). These forward models provide powerful tools for inferring the physical conditions of the chromosphere, but are computationally expensive and complex, in terms of their calculations of atomic level populations from NLTE radiative transfer.

A practical way of inferring physical parameters from observed chromospheric lines involves an inversion process, which minimizes the deviation between the synthetic line profilederived from the physically based radiative transfer calculationsand the observed line profile. There are two kinds of inversion process. One is inversion that is based on forward modeling, as described above (Hazel-Asensio Ramos et al. 2008; NICOLE-

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.

Socas-Navarro et al. 2015; SNAPI—Milić \& van Noort 2018; STiC—de la Cruz Rodríguez et al. 2016, 2019). For example, the Non-LTE Inversion Code based on the Lorien Engine (NICOLE) derives stratified physical properties from chromospheric spectral line profiles, by synthesizing the profiles based on the NLTE calculation. Recently, de la Cruz Rodríguez et al. $(2016,2019)$ have developed the STockholm Inversion Code (STiC), which gives the inversion results of NLTE spectral lines, based on the RH forward synthesis code, allowing inversion with multiple atoms in NLTE and partial redistribution effects in terms of angle and frequency. These inversion codes that use forward models make it possible to infer the stratified physical properties in the solar chromosphere, by considering the complex underlying physical conditions. However, NLTE spectral synthesis is timeconsuming and it needs to be generated multiple times for each spectrum, as part of the inversion process.

The second kind of inversion process uses parameterized models, in which the line profiles are calculated analytically, based on the model parameters (such as line-shape parameters or source functions determined from the radiative transfer calculation), to fit the observed spectral lines. The parametric models tune the model parameters until a good fit to the observed spectral line is obtained, making it relatively simple to extract the relevant information. For example, cloud model inversions (Beckers 1964; Tziotziou 2007; Chae 2014) treat the source function as a free model parameter over optical depth to infer the physical parameters of cloud-like features lying above the solar surface.

Recently, Chae et al. $(2020,2021 a)$ have proposed a multilayer spectral inversion (MLSI) and applied it to the strong absorption spectral lines $\mathrm{H} \alpha$ and Ca II $8542 \AA$, as taken by the Fast Imaging Solar Spectrograph (FISS; Chae et al. 2013). MLSI successfully provides height-varying physical quantities in a simplified three-layer atmosphere, including the
source function, Doppler velocity, and Doppler width. MLSI is much faster than forward modeling that requires NLTE calculations. Nevertheless, its speed is not yet quick enough, especially when larger numbers of line profiles have to be analyzed. With the higher spatial, temporal, and spectral resolutions of current observations, the amounts of data become larger and larger. To study spatiotemporal variations of physical parameters in the chromosphere, a huge number of line profiles should be analyzed based on spectral inversion. Even though MLSI represents a relatively fast inversion, it still requires unbearably long computing times for these kinds of data (big data). For example, the fast scan capability of FISS means that it can produce raster scan spectral data every 20 s , with a $30^{\circ} \times 40^{\circ}$ field of view ( $\sim 46,000$ pixels). It then takes about an hour to obtain the inverted physical parameters for a single instance of raster scan data using MLSI, meaning that it would take 180 hr to obtain the inversion results for an hour of observations (180 raster scans). Therefore, a much faster implementation of the inversion process is strongly required. This is the motivation of the present work, which aims to apply a deep learning technique to MLSI.

Various ""deep learning"" algorithms-such as deep neural networks (DNNs; LeCun et al. 2015), convolutional neural networks (CNNs; Lecun \& Bengio 1995), or generative adversarial networks (Goodfellow et al. 2014)—have become popular means of dealing with big data in solar research. For instance, the Atmospheric Imaging Assembly (Lemen et al. 2012) on board the Solar Dynamics Observatory (Pesnell et al. 2012) continuously provides a $4096 \times 4096$ full-Sun image every 12 s , with multiple wavelength channels. This vast data set has been utilized by several deep learning applications to generate new observables, using EUV images at different wavelengths, as well as magnetograms (Kim et al. 2019; Park et al. 2019; Lee et al. 2021; Lim et al. 2021), or the calculations of differential emission measures (DeepEM; Cheung et al. 2018). Our approach is similar to DeepEM, which is a DNN implementation of differential emission measure (DEM) inversion, achieved by training the DNN on a set of imaging observations and their inverted DEM solutions, in order to reduce the calculation time.

Deep learning has also been applied to spectral inversions in solar physics (Carroll \& Staude 2001; Asensio Ramos \& Díaz Baso 2019; Osborne et al. 2019; Sainz Dalda et al. 2019). Sainz Dalda et al. (2019) applied a DNN to the STiC inversion on a large collection of observed IRIS Mg II $\mathrm{h} \& \mathrm{k}$ spectra, in order to construct a database of profiles and their underlying atmospheres. The trained model then estimated the thermodynamic parameters (temperature, line-of-sight velocity, electron density, etc.) in the chromosphere and upper photosphere, by using a lookup approach to the database, in a fast way. Asensio Ramos \& Díaz Baso (2019) used the SIR inversion code to train a model on the Stokes parameters from 3D MHD simulations, in order to infer physical properties from observations of Stokes profiles. In another approach, Osborne et al. (2019) used an invertible neural network (INN; Ardizzone et al. 2018), trained with the radiation hydrodynamic model (RADYN) and synthesized spectra of $\mathrm{H} \alpha$ and Ca II line profiles, to produce inverted atmospheres with physical information from the observed spectra of flaring chromosphere.

In this study, we present a DNN implementation of the MLSI model developed by Chae et al. (2020, 2021a). We apply a fully connected network to model and determine the physical
parameters as a regression problem. The DNN is able to infer the possible underlying physical parameters from an observed spectral line much more quickly, with comparable precision to MLSI, thereby allowing us to investigate the physical processes in various phenomena in the solar photosphere and chromosphere. This study is organized as follows. The basic concept and the applied DNN are described in Section 2, while the training and test data sets are described in Section 3. In Section 4, we present the results from our DNN and discuss its performance compared to the original inversion model. Finally, a summary is given in Section 5.

## 2. Idea and Model

### 2.1. Basic Idea

Our deep learning model involves a kind of supervised learning. Supervised learning means that the data used for training have a defined structure. Each example comprises a pair consisting of an input object and the desired (labeled) output value. We want the algorithm to learn a functional approximation, by understanding the predefined structure between the input and output. Once the algorithm has learned the proper relation between the predefined examples of inputs and outputs by means of its training, it should then correctly predict the values or class labels for new inputs.

A DNN is a deep learning algorithm that consists of multiple hidden layers between the input and output layers. Each hidden layer has nodes (artificial neurons) that transform the input data by means of an inner product with a different weight vector and a linear transformation. A nonlinear function is then applied to the node, which will determine whether the output signal of the node is activated and passed to another node. Training the network with many data sets reduces the difference between the output of the network and the desired output value, by determining the proper weights of each node and establishing a complex nonlinear function between the input and output data. This means that the network is able to learn functionally important relationships.

For the supervised learning, we used pairs-an observed spectrum from FISS and the physical parameters for that spectrum as inferred from the updated MLSI (Chae et al. 2021a)-comprising the input and target data, respectively (Figure 1). We expect the DNN to learn the relationship between the input and output pairs in the training of the network. The details of the input and output data are described in Section 2.2, and our model architecture and hyperparameters are presented in Section 2.3.

### 2.2. Input and Output

We use spectral line profiles taken with FISS on the Goode Solar Telescope (GST) at the Big Bear Solar Observatory as the model inputs. FISS simultaneously records two main spectral bands, $\mathrm{H} \alpha$ and Ca II $8542 \AA$, using a $32 \mu \mathrm{~m}$ slit and two cameras. The spectral coverages of the $\mathrm{H} \alpha$ and Ca II 8542 bands are $9.7 \AA$ and $12.9 \AA$, respectively. The spectral samplings for $\mathrm{H} \alpha$ and Ca II $8542 \AA$ are $0.019 \AA$ and $0.025 \AA$, respectively. The spectral data were calibrated-corrected for flat fields, dark currents, and stray light-and compressed by the Principal Component Analysis method (Chae et al. 2013).

We also conducted a data reduction for the wavelength calibration and intensity normalization, as described in Chae et al. (2020). The absolute wavelength calibration was done by
![img-0.jpeg](img-0.jpeg)

Figure 1. Example pairs—the input object and the desired output value—for supervised learning. The left panels display the inputs for the observed line profiles of Hα and CaII 8542 Å (in black) and their shapes after the standardization processes described in Section 2.2 (in red). Details of the preprocessing are provided in the text. The right panels show the physical parameters calculated from the MLSI model using the input spectra.

Using the flat/calibration data of the quiet region (QR), which are taken more than once every observing day. We decide the wavelengths using telluric lines from the Earth's atmosphere or metal lines from the solar photosphere. We can then calculate the positions of the line centers at Hα and CaII, the wavelength per pixel, and the wavelength coverage of the data. We normalize the spectral profiles to a mean continuum intensity. First, the average spectral profile for each data set (i.e., the raster scan) is taken as a reference profile for diverse data sets. Then, we normalize all the spectral profiles using the maximum intensity of this reference profile, which is a proxy of the mean continuum intensity. In addition, we subtract the terrestrial absorption lines from the spectra, as described by Chae et al. (2021a).

To arrive at the homogeneous input parameters for the training, the data were preprocessed. First, we note that the wavelength ranges and scales of the spectral lines vary slightly from day to day, so we restrict the input wavelength ranges for all the data sets—6558.82 Å—6566.82 Å for Hα and 8535.99 Å—8547.29 Å for CaII—to ensure that all the data have the same wavelength ranges. We then interpolate all the spectra using the same absolute wavelength scale, with samplings of 0.02 Å and 0.025 Å for Hα and CaII, respectively. Second, we perform input data scaling, to avoid unstable or slow learning processes. Since the ranges of the input data values vary widely, if the data are not scaled, the learning algorithm may become confused and place more importance on the pixels, only because they have higher intensity values, which would make the learning process unstable. In addition, for the optimization of our model, we use gradient descent-based algorithms. The input data will affect the step size of the gradient descent, and the differences in the ranges of the input values will result in different step sizes for each of the input values. Therefore, we need to scale the data, before training the model to update the steps for the gradient descent at the same rate for all the input spectra, resulting in the gradient descent smoothly converging to the minima much faster. We apply the data scaling method, normalization, and standardization to our input data, then compare the model's performance. Standardization, which makes the mean of the training input data values zero, with the standard deviation set to 1, shows better performance. The standardization is conducted using all the training input data sets, but separately, along each wavelength. Examples of the input spectra for Hα and CaII are displayed in the left panels of Figure 1. Each intensity along the wavelength is compared to the intensity of the entire input data set at the same wavelength. The solid black and red lines indicate the observed line profiles and the standardized profiles of the whole training input data set, respectively. If the wing value is close to the mean value of all the input data, then the profile value is near zero. But for the standardized (red) profile of Hα at the center, the intensity value of the example is larger than the mean value of the intensity of the whole input data set, so it is larger than zero.

We use the physical parameters inferred from the updated MLSI (Chae et al. 2021a) as the target data. The model solves the radiative transfer equation with two strong absorption profiles, the Hα and CaII lines, assuming a three-layer atmosphere, consisting of the photosphere, the lower chromosphere, and the upper chromosphere (see Figure 3 in Chae et al. 2021a). The modeled line profile of MLSI is fully specified by 15 parameters. The two parameters of optical thickness, in the low chromosphere (τp,2) and upper chromosphere (τp,1), are fixed values. The 10 physical quantities as free parameters are determined from the model fitting, the source functions at the boundaries between each layer (Sp, Sp, S1, and Sp), the Doppler velocities, the Doppler widths at the boundaries of each layer in the chromosphere (v1, vp, w1, and wp), and the parameters for the absorption profiles of the photosphere (the dimensionless damping parameter (ap) and the ratio of peak line absorption to continuum absorption (η)). The Doppler velocity and Doppler width in the photosphere (vp and wp) are determined from analysis of the photospheric lines in the wings of the chromospheric profiles, while the parameter for collisional damping in the low chromosphere (γ2) is determined from the shapes of the broad wings of the strong lines. Among the inferred physical quantities from MLSI, we use 13 parameters, except for the two fixed parameters (τp,2, τp,1). Examples of the output values for the physical quantities are listed in the right panels of Figure 1.

### 2.3. DNN Model

After defining the pair of input and output layers, we create a DNN of hidden layers. Figure 2 presents the architecture of our model. We use a fully connected network, adopting two concepts: skip (shortcut) connection and branching architecture.

The skip connection, or shortcut layer, is a widely used technique for improving the performance and the convergence of deep learning models, which is used in residual networks (He et al. 2015) or U-Net networks (Ronneberger et al. 2015). Generally, if we add more layers and deepen the neural network models, the accuracy of the results or the success rate of the prediction increases. However, if the depth further increases, the gradients used during the training become
![img-1.jpeg](img-1.jpeg)

**Figure 2.** Flowchart of our DNN with skip connection and branching architecture. The red numbers for each of the layers indicate the numbers of nodes. The black number above the ellipsis denotes the number of layers that have the same number of nodes as the previous layer.

exponentially small, and the information cannot pass through the deeper layers, which is called the vanishing gradient problem. We utilize skip connections (the gray arrows in our model architecture) to allow gradient information to pass through the layers, with the previous layer's output being added to the output of a deeper layer. This connection, therefore, enables the signal to propagate in deeper networks, while accelerating the training by avoiding the effect of vanishing gradients.

After the shortcut layers, we create multiple parallel branches for processing the data independently, using different parameters with different weights. The branching architecture has been used in CNNs, such as inception networks (Szegedy et al. 2016) and Xception (Chollet 2017). The multiple branches increase the success of the model by reducing the duality gap of empirical risk, thereby making it easier to optimize the network (Zhang et al. 2018). Here, we introduce three parallel branches, consisting of a different number of layers with a different number of nodes. Following these multiple branches, the outputs are concatenated, before they pass through the fully connected dense layers. The final output layer is a dense layer, without an activation function to give the regression values. The outputs are 13 physical parameters for specifying the modeled line profiles.

In this network, we use a Swish activation function (Ramachandran et al. 2017), which is defined as

$$f(x) = x \times \text{sigmoid}(x) = \frac{x}{1 + e^{-x}}\tag{1}$$

at each layer, and which determines how the weighted sum of the input is transformed into an output from a node or nodes, making the network nonlinear. Since the Swish function helps to alleviate the vanishing gradient problem during backpropagation, it is known to have better performance on deeper models than the Rectified Linear Unit, ReLU, which is the most widely used activation function. In addition, we use Xavier initialization (glorot_normal: Glorot & Bengio 2010) at each layer, in order to determine the proper initial weight suitable for deeper networks, by avoiding the saturation of the weight.

We train the network using the Adam (Kingma & Ba 2014) optimizer, over 2000 epochs, with a batch size of 10,000. For the other hyperparameters, the initial learning rate is 0.2825 × 10<sup>−3</sup>, which is then reduced exponentially by a factor of 0.45 every 50 epochs, until the running rate becomes 10<sup>−5</sup>. To avoid overfitting, we introduce L2 regularization, with the lambda value of 5 × 10<sup>−5</sup>, after the multiple branch layers. In addition, we set early stopping by monitoring the validation loss, with the value of patience being 30. Our model then stops early, at around 450 and 650 epochs for the Hα and Ca II parameters, respectively. To train our DNN, we use the mean squared error (MSE) as a scalar loss function and the mean absolute error (MAE) as the metric that is used to estimate the accuracy of the thirteen predicted physical parameters. We finally obtain the model that has the best minimum loss function, 0.003 and 0.009 of the MSE values for the Hα and Ca II parameters, respectively. Our DNN is implemented and trained in Tensorflow, using Graphical Processing Units (GPUs) to accelerate the calculations. We used an NVIDIA GeForce RTX 3090 for training, during the exploration of the hyperparameters.

### 3. Data Sets: Training and Test

For the supervised learning, the essential element is a well-prepared data set, covering as much of the parameter space as possible, with reliable mappings between the observed spectra and the physical parameters derived from MLSI. We prepare the training and validation data sets by choosing a scan raster from each of the different observing targets or pointings from the FISS/GST observations, covering both the Hα and Ca II 8542 lines in the period from 2013 to 2015. Table 1 presents a list of the data sets. We used 49 raster scans, consisting of about 2,500,000 spatial pixels, with observed spectral profiles and inverted physical parameters. Here, we note that the selected observing targets of FISS are mostly active regions (ARs), emerging flux regions, or pores, as well as regions around the disk center (µ > 0.5), rather than QRs, due to the limitations of the seeing conditions. GST uses an adaptive optics (AO) system to improve its seeing. The AO system needs to choose a stable (slowly evolving) structure with high contrast, to continuously track the structure, and to correct the seeing by moving the tip-tilt mirror. It is easy to select the structures inside a sunspot (AR), pore, or emerging flux region that have highly contrasting features. However, if the atmospheric conditions are not good, with high turbulence, we cannot follow the QRs or coronal holes, which do not have
Table 1
The Data Sets for the DNN: Training and Validation

| Number <br> (1) | Date <br> (2) | Time (UT) <br> (3) | Observing Target <br> (4) | NOAA Number <br> (5) | Location $(x, y)$ <br> (6) |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 1 | 2013-Jul-16 | 19:17:25 | Sunspot | 11791 | $\left(62^{\prime \prime},-30^{\prime \prime}\right)$ |
| 2 | 2013-Jul-16 | 21:44:43 | QR | $\cdots$ | $\left(447^{\prime \prime}, 87^{\prime \prime}\right)$ |
| 3 | 2013-Jul-17 | 17:02:38 | Trailing pore | 11791 | $\left(247^{\prime \prime},-278^{\prime \prime}\right)$ |
| 4 | 2013-Jul-17 | 18:41:07 | QR | $\cdots$ | $\left(-15^{\prime \prime},-126^{\prime \prime}\right)$ |
| 5 | 2013-Jul-17 | 20:48:15 | Emerging flux region, pore | $\cdots$ | $\left(556^{\prime \prime},-291^{\prime \prime}\right)$ |
| 6 | 2013-Jul-18 | 17:47:50 | Sunspot | 11793 | $\left(-332^{\prime \prime}, 272^{\prime \prime}\right)$ |
| 7 | 2013-Jul-18 | 21:04:43 | Pore | 11793 | $\left(-365^{\prime \prime}, 259^{\prime \prime}\right)$ |
| 8 | 2013-Jul-19 | 17:06:29 | Pore | 11793 | $\left(-17^{\prime \prime}, 302^{\prime \prime}\right)$ |
| 9 | 2013-Jul-28 | 18:19:30 | AR, pore | 11801 | $\left(43^{\prime \prime}, 391^{\prime \prime}\right)$ |
| 10 | 2013-Jul-29 | 17:20:57 | Pore | 11801 | $\left(194^{\prime \prime}, 381^{\prime \prime}\right)$ |
| 11 | 2013-Jul-29 | 17:45:21 | AR near east limb | 11808 | $\left(-741^{\prime \prime}, 196^{\prime \prime}\right)$ |
| 12 | 2013-Jul-30 | 17:53:46 | AR | 11808 | $\left(-640^{\prime \prime}, 134^{\prime \prime}\right)$ |
| 13 | 2013-Jul-30 | 19:35:57 | Pore | 11806 | $\left(-357^{\prime \prime},-373^{\prime \prime}\right)$ |
| 14 | 2013-Jul-31 | 18:27:29 | Sunspot | 11801 | $\left(520^{\prime \prime}, 228^{\prime \prime}\right)$ |
| 15 | 2013-Aug-16 | 17:04:16 | Sunspot | 11818 | $\left(222^{\prime \prime},-202^{\prime \prime}\right)$ |
| 16 | 2013-Aug-16 | 18:26:39 | AR | 11817 | $\left(593^{\prime \prime},-433^{\prime \prime}\right)$ |
| 17 | 2013-Aug-16 | 19:45:16 | Sunspot | 11818 | $\left(216^{\prime \prime},-220^{\prime \prime}\right)$ |
| 18 | 2013-Aug-16 | 21:11:10 | Sunspot | 11820 | $\left(-392^{\prime \prime},-318^{\prime \prime}\right)$ |
| 19 | 2013-Aug-17 | 18:08:29 | Flaring AR (before flare) | 11818 | $\left(472^{\prime \prime},-220^{\prime \prime}\right)$ |
| 20 | 2013-Aug-17 | 18:21:41 | Flaring AR (flare peak) | 11818 | $\left(472^{\prime \prime},-220^{\prime \prime}\right)$ |
| 21 | 2013-Aug-17 | 18:43:10 | Flaring AR (after flare) | 11818 | $\left(472^{\prime \prime},-220^{\prime \prime}\right)$ |
| 22 | 2013-Aug-18 | 16:26:35 | Emerging flux region, pore | $11825^{\text {b }}$ | $\left(131^{\prime \prime}, 140^{\prime \prime}\right)$ |
| 23 | 2013-Aug-18 | 17:39:08 | Emerging flux region, pore | $11826^{\text {b }}$ | $\left(142^{\prime \prime}, 137^{\prime \prime}\right)$ |
| 24 | 2013-Aug-23 | 16:30:40 | Trailing spot | 11827 | $\left(-152^{\prime \prime},-402^{\prime \prime}\right)$ |
| 25 | 2013-Aug-23 | 19:18:45 | Trailing spot | 11827 | $\left(-152^{\prime \prime},-402^{\prime \prime}\right)$ |
| 26 | 2013-Aug-24 | 16:40:53 | Sunspot | 11828 | $\left(-59^{\prime \prime}, 162^{\prime \prime}\right)$ |
| 27 | 2013-Aug-24 | 19:05:35 | Pore | 11828 | $\left(-177^{\prime \prime}, 189^{\prime \prime}\right)$ |
| 28 | 2013-Aug-25 | 16:27:49 | Trailing pore | 11828 | $\left(116^{\prime \prime}, 168^{\prime \prime}\right)$ |
| 29 | 2013-Aug-25 | 17:53:37 | Leading pore | 11828 | $\left(167^{\prime \prime}, 150^{\prime \prime}\right)$ |
| 30 | 2013-Aug-25 | 20:04:57 | Sunspot near west limb | 11823 | $\left(818^{\prime \prime},-157^{\prime \prime}\right)$ |
| 31 | 2014-Jun-3 | 16:52:01 | Pore | 12078 | $\left(134^{\prime \prime},-318^{\prime \prime}\right)$ |
| 32 | 2014-Jun-3 | 18:17:35 | Trailing pore | 12077 | $\left(-329^{\prime \prime},-106^{\prime \prime}\right)$ |
| 33 | 2014-Jun-3 | 19:57:57 | Sunspot | 12077 | $\left(-344^{\prime \prime},-77^{\prime \prime}\right)$ |
| 34 | 2014-Jun-3 | 21:44:07 | Emerging flux region, pore | 12079 | $\left(-696^{\prime \prime}, 51^{\prime \prime}\right)$ |
| 35 | 2014-Jun-5 | 16:47:05 | Sunspot | 12080 | $\left(-527^{\prime \prime},-204^{\prime \prime}\right)$ |
| 36 | 2014-Jun-5 | 18:34:01 | Filament | $\cdots$ | $\left(-420^{\prime \prime},-104^{\prime \prime}\right)$ |
| 37 | 2014-Jun-5 | 18:58:32 | AR | $12082^{\text {b }}$ | $\left(-605^{\prime \prime}, 236^{\prime \prime}\right)$ |
| 38 | 2014-Jun-6 | 17:02:45 | Sunspot | 12079 | $\left(-74^{\prime \prime}, 188^{\prime \prime}\right)$ |
| 39 | 2014-Jun-6 | 18:42:17 | AR | $12085^{\text {b }}$ | $\left(-394^{\prime \prime},-343^{\prime \prime}\right)$ |
| 40 | 2014-Jun-6 | 20:08:19 | Light bridge | 12082 | $\left(-436^{\prime \prime}, 222^{\prime \prime}\right)$ |
| 41 | 2015-Jun-15 | 16:55:49 | Leading sunspot | 12367 | $\left(-310^{\prime \prime},-330^{\prime \prime}\right)$ |
| 42 | 2015-Jun-15 | 17:49:38 | QR | $\cdots$ | $\left(-215^{\prime \prime},-378^{\prime \prime}\right)$ |
| 43 | 2015-Jun-15 | 17:54:34 | QR | $\cdots$ | $\left(-215^{\prime \prime},-378^{\prime \prime}\right)$ |
| 44 | 2015-Jun-15 | 18:46:58 | Pore near AR | 12367 | $\left(-341^{\prime \prime},-321^{\prime \prime}\right)$ |
| 45 | 2015-Jun-15 | 20:16:12 | Leading pore | 12370 | $\left(-190^{\prime \prime}, 271^{\prime \prime}\right)$ |
| 46 | 2015-Jul-24 | 20:15:52 | AR | 12387 | $\left(595^{\prime \prime}, 219^{\prime \prime}\right)$ |
| 47 | 2015-Jul-24 | 22:15:21 | Sunspot near east limb | $12389^{\text {b }}$ | $\left(-768^{\prime \prime},-256^{\prime \prime}\right)$ |
| 48 | 2015-Jul-28 | 20:30:52 | Filament | $\cdots$ | $\left(-561^{\prime \prime},-25^{\prime \prime}\right)$ |
| 49 | 2015-Jul-28 | 22:06:56 | Filament | $\cdots$ | $\left(-554^{\prime \prime},-18^{\prime \prime}\right)$ |

# Note. 

${ }^{\text {a }}$ Newly emerging ARs. At the observing time, the NOAA number had not yet been assigned.
high-contrast structures. Limb observations are also tricky, for the same reason. Since most training data sets are biased to ARs or near the disk center, the application of this model to QRs or observations near limbs may be less reliable.

We note that the updated MLSI introduced several physical constraints for inferring physically reasonable parameters, the details of which can be found in Chae et al. (2021a). The least squares fit minimizes the difference between the model and the data, considering the physical constraints. Then, the two
dimensionless parameters, $\epsilon_{D}$ and $\epsilon_{P}$, as defined in Chae et al. (2021a), provide estimates of the goodness of fit for the spectral profile and for the physical constraints, respectively. We examine the values of $\epsilon_{D}$ and $\epsilon_{P}$ for all the training data sets. The majority of the spectra that are acceptably fitted by MLSI have values of $\epsilon_{D}$ and $\epsilon_{P}$ that are less than 3 . Therefore, we only use input and target data that meet the criteria $\epsilon_{D}<3$ and $\epsilon_{P}<3$. Among the training data sets in Table 1, the number of line profiles and inverted output parameters that meet the
Table 2
The Data Sets for the DNN: Testing

| Date <br> $(1)$ | Time (UT) <br> $(2)$ | Target <br> $(3)$ | NOAA <br> $(4)$ |
| :-- | :--: | :--: | :--: |
| $2020-\mathrm{July}-30$ | $16: 48: 21-17: 57: 43$ | QR | $\ldots$ |
| $2013-\mathrm{July}-17$ | $18: 32: 24-19: 56: 18$ | AR | 11791 |

criteria is about $2,000,000$, or $80 \%$ of the data sets. For the other $20 \%$ of the data sets, $\epsilon_{D}$ and $\epsilon_{P}$ are larger than 3 , with some of them even reaching over 10, showing profiles with emission for flares or profiles with strong secondary shifted components. We note that the features with a high goodness of fit parameter (outliers) are not evenly spread across all the target data sets, but rather a large number are to be found in the flare data sets. We exclude any poorly fitting pairs of spectral profiles and their inverted physical parameters from our DNN model training. We shuffle the selected data sets and randomly use $80 \%$ of the data for training and $20 \%$ for validation.

To verify the DNN's performance, we need to test the DNN on separate data sets that are not included in the training or validation data sets. The test data sets are listed in Table 2. One set is a time series observation of a QR on 2020 July 30. The other set is a time series observation of an AR on 2013 July 17, which is described in detail in Chae et al. (2014, 2021b). A set of monochromatic maps, constructed at some of the wavelengths from the raster scans for the QR and the AR, are presented in Figure 3.

## 4. Results and Discussion

Our model provides physical parameter values, given the input of the observed spectra. First, we compare each parameter value from the DNN to the parameters that are derived directly from MLSI, which are the target parameters (the ground truth), to evaluate the accuracy of the DNN parameter predictions. Second, if the model is able to learn the relationship between the pairs reasonably well, we plan to use the stand-alone DNN to infer the parameters, without performing the full MLSI calculation. To directly confirm that the obtained physical parameters are explainable in the observed spectra, we reproduce the line profiles using the parameters from our DNN and compare them to the observed spectra. Third, we also check the temporal variations of the parameters, to confirm that the temporal variations of the parameters recovered by the DNN are physically reasonable.

### 4.1. Comparison of Physical Parameters

To test our model, we use spectral line profiles in a QR (2020 July 30) and an AR (2013 July 17), taken from FISS (Table 2). Figure 3 shows the sample images of the observed scan rasters of the QR and the AR. The raster images of the QR have $150 \times 250$ pixels-that is, 37,500 line profiles. The raster image of the AR consists of 64,000 line profiles. We determine the physical parameters for each line profile of the data points using MLSI and the trained DNN, and reproduce the physical parameter maps.

Figures 4 and 5 show the sample images of the parameter maps, the source functions at each layer $\left(S_{p}, S_{2}, S_{1}\right.$, and $\left.S_{0}\right)$, the Doppler velocities $\left(v_{p}, v_{1}\right.$, and $\left.v_{0}\right)$, and the Doppler widths $\left(w_{p}\right.$, $w_{1}$, and $\left.w_{0}\right)$, from Ca II in the QR and the $\mathrm{H} \alpha$ in the AR. The upper three rows display the inverted physical parameters from

MLSI that are used as ground truth. The lower three rows present the target parameters produced by our trained model. Compared to the parameter maps from MLSI, the parameter maps from our DNN are successfully reproductions. In addition, we present the values of $\epsilon_{D}$ and radiative loss at the lower chromosphere, calculated by both the inverted and the predicted parameters, in Figures 4 and $5 . \epsilon_{D}$ gives the goodness of fit, by comparing the observed and synthesized spectra. The radiative losses for each layer of each line have been calculated from the values of the model parameters, and the values of the radiative losses from the inverted and from the predicted parameters are consistent.

We measure the means and standard deviations of the parameter maps from the DNN, along with four other types of metric, to compare the physical parameters from MLSI with the predicted ones from our DNN: (1) the MAE; (2) the normalized rms error (NRMSE); (3) the correlation coefficient (CC); and (4) the coefficient of determination ( $R^{2}$ score). The values for each of these parameters are listed in Table 3.

First, the mean and standard deviation values of the physical parameters from our DNN display similar trends to the values of the parameters from MLSI, as discussed in Chae et al. (2021a). For example, the source function of $\mathrm{H} \alpha$ monotonically decreases with height, while that of Ca II has a local maximum in the chromosphere, which is physically expected.

Second, MAE and NRMSE are measured to evaluate the accuracy of the predicted parameters from the DNN compared to the values derived from MLSI. NRMSE is RMSE normalized by the standard deviation of the MLSI parameters. The smaller the values of MAE and NRMSE, the higher the accuracy of the predictions. Both values are pretty small for most parameters, implying that the model performance is quite good. The MAE values range from 0.001 to 0.01 for the source functions and Doppler widths, while those for the Doppler velocity are less than $250 \mathrm{~m} \mathrm{~s}^{-1}$. The values of NRMSE are less than $30 \%$ for most of the parameters, except for the Doppler widths in the photosphere $\left(w_{p}\right)$.

Third, we calculate the CC and $R^{2}$ score for each parameter, to examine the linearity of the relationship of each model parameter and the percentage of correct predictions returned by our DNN compared to the parameters from MLSI. If these values are 1 , the regression model predictions perfectly fit the data. In relation to the CC, we display 2D density plots for the physical parameters from both spectral lines for the QR and AR in Figures 6-9. The 2D density plots display a tight correlation between the physical parameters derived from MLSI (ground truth) and the DNN (predicted). Here, even though we assume that the parameters derived from MLSI are the ground truth, MLSI itself is an inversion model, which results in some of the observed spectral lines not being well fitted; the cases of $\epsilon_{D}$ and $\epsilon_{P}$ are larger than 1 . These data points are denoted by the red dots, which amount to less than $1.5 \%$ of the total data. The CCs are mostly close to 1 (larger than 0.8 ) for each of the physical parameters. Also, the $R^{2}$ scores are larger than 0.6 , except for the Doppler width in the photosphere $\left(w_{p}\right)$ of the Ca II line in the QR.

Overall, the DNN predicts the target parameters well, except for $w_{p}$. Regarding the parameter $w_{p}$, the NRMSEs are larger than the those of the other parameters. This is primarily due to the small standard deviation of $w_{p}$. The smaller standard deviation makes the NRMSE larger, even though the RMSE itself is not significant. Although the NRMSE has
![img-2.jpeg](img-2.jpeg)

Figure 3. Monochromatic images constructed at the wavelengths of the Hα and the Ca II 8542 Å lines, obtained from FISS. The upper (first and second rows) and lower (third and fourth rows) panels show sample images of a QR and an AR, respectively. The cross symbols in the QR and AR images mark the positions of a network feature and a superpenumbral fibril that we selected to compare the observed line profiles with the modeled spectra, using the physical parameters inferred from the DNN in Figures 10 and 11. The white vertical slits in the scanned raster images of the AR indicate that data are missing.

large values, the high values of the CC (>0.8) and the R² score (>0.6) imply that the distributions of the wp are well matched between the inverted and predicted ones. The wp maps in Figures 4 and 5 also confirm that their distributions are consistent.

We note that the value of the NRMSE of the wp of the Ca II line in the QR is extremely large (593%). Figure 7 shows that the values predicted from the DNN are systematically larger than those from MLSI, resulting in the NRMSE (593%) and R² score (≪1) being poor. However, the CC is 0.826, which shows that their distributions are well correlated. Also, the wp parameter map in Figure 4 shows that their distributions are consistent. By applying the DNN to the different quiet Sun observations, we also found systematic differences for the predicted wp parameter of the Ca II line, which are not shown here. It seems that our trained DNN has a limit on low values of wp since it is not able to reproduce the significantly low values of wp from the Ca II line in the QRs derived by MLSI. Even so, the DNN seems to predict the spatial distributions. This discrepancy may result from our training data sets being biased to the ARs, emerging flux regions, or pores, rather than to QRs due to the limitations of the seeing conditions. As we have mentioned, the wp are small values, and their deviations are very small. The predicted value converges to the mean value of all the training data sets, which are biased to ARs or pores with larger values of wp. This resulted in the DNN predicting the larger values of wp for the QR than those measured by MLSI.

Actually, the Doppler width in the photosphere (wp) is a pretty tiny value, considering the thermal width from the radiative intensity. In the MLSI calculations of Chae et al. (2020), therefore, the wp was fixed to a small value for all the regions, which made the MLSI model fit well. To know the spatial variation, even though the wp is tiny, the updated MLSI model of Chae et al. (2021a) indirectly inferred the wp, by combining the thermal width converted from the far-wing intensity of each spatial location with the assumed nonthermal speed (1 km s⁻¹). Practically, the systematic differences in the tiny value of wp do not significantly affect either the inversion or the prediction of the physical parameters in the chromosphere.
![img-3.jpeg](img-3.jpeg)

Figure 4. Physical parameter maps of the Ca II $8542 \AA$ line using MLSI (upper three rows) and the DNN (lower three rows) for the QR. The cross and square symbols mark the locations of a network feature and a region with a large standard error, respectively.

### 4.2. Comparison of Line Profiles

We reproduce the spectral line profiles based on the threelayer atmosphere model using the predicted physical parameters derived from our DNN as the input. We then compare
the observed spectral lines of $\mathrm{H} \alpha$ and Ca II $8542 \AA$ with the synthesized line profiles using the physical parameters from the DNN. Figure 10 presents the intensity profiles of the network feature observed in the QR compared with the modeled intensity profiles from the DNN parameters for $\mathrm{H} \alpha$ (left panel)
![img-4.jpeg](img-4.jpeg)

Figure 5. Physical parameter maps of H<sub>O</sub> using MLSI (upper three rows) and the DNN (lower three rows) for the AR. The cross and square symbols mark the locations of a superpenumbral fibril and a region with a large standard error, respectively.

and Ca II 8542 Å (right panel). The synthesized intensity profile at the top of the upper chromosphere (*I*<sub>0</sub>(*p*)) using the parameters (*p* = *S*<sub>*p*</sub>, *a*<sub>*p*</sub>, *η*, *S*<sub>2</sub>, *S*<sub>1</sub>, *ν*<sub>1</sub>, *w*<sub>1</sub>, *S*<sub>0</sub>, *ν*<sub>0</sub>, *w*<sub>0</sub>) from the DNN is well matched with the observed spectral line profiles. Figure 11 displays the intensity profiles of the superpenumbral fibril observed in the AR, again showing the good agreement of the observed spectral line profile and the modeled intensity profile using the parameters predicted by the DNN.

Comparing the observed and the synthesized intensity profiles directly, we measure the parameter of the goodness of fit for the synthesized spectra reproduced by the physical parameters from the DNN, *ε*<sub>*D*</sub>, which is defined in Equations (18–20) in Chae et al. (2021a):

$$
\epsilon_D \equiv \left[ \frac{1}{N} \sum_{k=1}^{N} f_k^2 (p) \right]^{\frac{1}{k}}. \tag{2}
$$

The *f*<sub>*k*</sub> is the difference at each wavelength point (*λ*<sub>*k*</sub>) between the observed intensity data and the synthesized intensity data using the DNN predicted parameters against the control parameters of the tightness of the constraint at each
![img-5.jpeg](img-5.jpeg)

Figure 6. 2D density plots for each model parameter calculated from MLSI and our DNN for the $\mathrm{H} \alpha$ line profiles observed in the QR. The red dots denote the parameters with large errors of MLSI model fitting ( $\epsilon_{P}>1$ and $\epsilon_{D}>1$ ). The dashed line indicates the equality of both model parameters, while the solid cyan line shows a linear fit to all the data points.

Table 3
Means and Standard Deviations of the Physical Parameters Derived from the DNN, with the MAE, NRMSE, CC, and Coefficient of Determination ( $R^{2}$ Score) Metrics, for Evaluating the Accuracy of the DNN Parameter Predictions as Compared to Direct MLSI Calculations

| Region | Physical <br> Parameter | Mean $\pm$ Standard Deviation |  | MAE |  | NRMSE |  | CC |  | $R^{2}$ Score |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | H $\alpha$ | Ca II | $\mathrm{H} \alpha$ | Ca II | $\mathrm{H} \alpha$ | Ca II | [\%] |  | H $\alpha$ | Ca II |
|  |  |  |  |  |  |  |  | H $\alpha$ | Ca II |  |  |
|  | $\log S_{p}\left[I_{0}\right]$ | $0.003 \pm 0.014$ | $0.017 \pm 0.020$ | 0.003 | 0.008 | 26.8 | 47.2 | 0.970 | 0.979 | 0.928 | 0.777 |
|  | $\log S_{2}\left[I_{0}\right]$ | $-0.251 \pm 0.020$ | $-0.487 \pm 0.061$ | 0.004 | 0.004 | 28.0 | 8.8 | 0.981 | 0.996 | 0.921 | 0.992 |
|  | $\log S_{1}\left[I_{0}\right]$ | $-0.448 \pm 0.033$ | $-0.405 \pm 0.037$ | 0.003 | 0.003 | 12.9 | 12.0 | 0.995 | 0.994 | 0.983 | 0.985 |
|  | $\log S_{0}\left[I_{0}\right]$ | $-0.881 \pm 0.064$ | $-1.038 \pm 0.165$ | 0.005 | 0.012 | 9.7 | 10.5 | 0.996 | 0.997 | 0.991 | 0.989 |
| QR | $v_{p}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $0.083 \pm 0.469$ | $-0.425 \pm 0.501$ | 0.032 | 0.055 | 8.1 | 14.6 | 0.998 | 0.993 | 0.993 | 0.979 |
|  | $v_{1}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $-0.217 \pm 1.574$ | $-0.504 \pm 1.830$ | 0.226 | 0.173 | 15.4 | 11.6 | 0.998 | 0.997 | 0.976 | 0.987 |
|  | $v_{0}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $0.119 \pm 2.139$ | $0.425 \pm 1.465$ | 0.136 | 0.046 | 7.0 | 4.2 | 1.000 | 0.999 | 0.995 | 0.998 |
|  | $\log w_{p}[\AA]$ | $-0.656 \pm 0.002$ | $-1.268 \pm 0.002$ | 0.001 | 0.010 | 62.2 | 593.2 | 0.843 | 0.826 | 0.613 | $-34.189$ |
|  | $\log w_{1}[\AA]$ | $-0.496 \pm 0.040$ | $-0.741 \pm 0.053$ | 0.003 | 0.004 | 11.5 | 10.1 | 0.996 | 0.996 | 0.987 | 0.990 |
|  | $\log w_{0}[\AA]$ | $-0.441 \pm 0.046$ | $-0.784 \pm 0.059$ | 0.005 | 0.005 | 7.3 | 10.3 | 0.997 | 0.996 | 0.995 | 0.989 |
|  | $\log S_{p}\left[I_{0}\right]$ | $-0.011 \pm 0.027$ | $0.003 \pm 0.029$ | 0.001 | 0.002 | 4.7 | 10.8 | 0.999 | 0.994 | 0.998 | 0.988 |
|  | $\log S_{2}\left[I_{0}\right]$ | $-0.220 \pm 0.027$ | $-0.443 \pm 0.050$ | 0.002 | 0.002 | 9.7 | 6.4 | 0.996 | 0.998 | 0.991 | 0.996 |
|  | $\log S_{1}\left[I_{0}\right]$ | $-0.426 \pm 0.042$ | $-0.335 \pm 0.057$ | 0.001 | 0.002 | 4.5 | 4.8 | 0.999 | 0.999 | 0.998 | 0.998 |
|  | $\log S_{0}\left[I_{0}\right]$ | $-0.788 \pm 0.098$ | $-0.749 \pm 0.182$ | 0.002 | 0.003 | 2.5 | 2.7 | 1.000 | 1.000 | 0.999 | 0.999 |
| AR | $v_{p}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $0.007 \pm 0.423$ | $0.001 \pm 0.498$ | 0.020 | 0.039 | 5.5 | 12.0 | 0.998 | 0.993 | 0.997 | 0.986 |
|  | $v_{1}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $-0.663 \pm 1.241$ | $-0.483 \pm 1.663$ | 0.087 | 0.047 | 9.6 | 4.6 | 0.995 | 0.999 | 0.991 | 0.998 |
|  | $v_{0}\left[\mathrm{~km} \mathrm{~s}^{-1}\right]$ | $-0.071 \pm 1.412$ | $0.352 \pm 1.029$ | 0.041 | 0.030 | 3.9 | 4.7 | 0.999 | 0.999 | 0.998 | 0.998 |
|  | $\log w_{p}[\AA]$ | $-0.656 \pm 0.003$ | $-1.268 \pm 0.003$ | 0.001 | 0.001 | 26.7 | 40.5 | 0.966 | 0.934 | 0.928 | 0.836 |
|  | $\log w_{1}[\AA]$ | $-0.414 \pm 0.039$ | $-0.705 \pm 0.046$ | 0.001 | 0.003 | 5.2 | 8.0 | 0.999 | 0.997 | 0.997 | 0.994 |
|  | $\log w_{0}[\AA]$ | $-0.448 \pm 0.035$ | $-0.739 \pm 0.055$ | 0.001 | 0.002 | 5.1 | 6.2 | 0.999 | 0.998 | 0.997 | 0.996 |
![img-6.jpeg](img-6.jpeg)

Figure 7. The same as Figure 6, but for the Ca II line profiles observed in the QR.
![img-7.jpeg](img-7.jpeg)

Figure 8. The same as Figure 6, but for the $\mathrm{H} \alpha$ line profiles observed in the AR.
![img-8.jpeg](img-8.jpeg)

Figure 9. The same as Figure 6, but for the Ca II line profiles observed in the AR.
![img-9.jpeg](img-9.jpeg)

Figure 10. Three-layer model fitting of the $\mathrm{H}_{\mathrm{O}}$ (left) and Ca II $8542 \AA$ (right) line profiles taken from a network feature in the QR, marked by the cross symbol in Figure 4, using the physical parameters derived from the DNN. The parameter values are listed at the bottom of the figure. The thick solid black line indicates the observed line profile, while the solid green, blue, and red lines represent the modeled emergent intensity profiles at different layers-the top of the photosphere (layer 2), the lower chromosphere (layer 1), and the upper chromosphere (layer 0 ), respectively. The dashed cyan line indicates each reference profile obtained by taking the spatial average of the line spectra over the observed scan raster. The lower panels present the plots of the residuals between the observed and modeled intensity profiles. For comparison, the $\epsilon_{D}$ calculated from MLSI fitting for the observed spectra is also given.
![img-10.jpeg](img-10.jpeg)

Figure 11. The same as Figure 10, but for a superpenumbral fibril in the AR, marked by the cross symbol in Figure 5.

Data point, σ<sub>k</sub>:

$$f_k(p) \equiv \frac{I_{\lambda_k,obs} - I_{\lambda_k,0}(p)}{\sigma_k}.\tag{3}$$

Here, the control parameters for Hα and CaII are set as σ<sub>k</sub> = σ<sub>c</sub> √I<sub>λ_c,obs</sub>, which is adopted from the photon noise form. Then, σ<sub>k</sub> is equal to σ<sub>c</sub> in the continuum I<sub>λ_c,obs</sub> = 1, and to 0.5σ<sub>c</sub> in the core, with I<sub>λ_c,obs</sub> = 0.25. The noise level value of σ<sub>c</sub> is set to 0.01, the same as the σ<sub>c</sub> value in the MLSI model fitting (Chae et al. 2021a).

When we look at the scatter plots of ε<sub>D</sub> in Figures 6–9, the ε<sub>D</sub> values from the DNN for the well-fitting data sets with MLSI (ε<sub>D</sub> < 1 and ε<sub>D</sub> < 1) have slightly larger values than the ε<sub>D</sub> measured by MLSI fitting. Even so, the values of ε<sub>D</sub> are mostly less than 2, which indicates that the predicted parameters from the DNN are satisfactory for those data sets. On the other hand, the acceptably fitted profiles (1 < ε<sub>D</sub> < 3 and 1 < ε<sub>D</sub> < 3 for MLSI; the red dots in the scatter plots) present large deviations between the observed and synthesized spectra. This implies that the regions where ε<sub>D</sub> has large values in MLSI also have large ε<sub>D</sub> values in the DNN. It is natural for our DNN not to be able to reproduce the specific features that are not well fitted by MLSI since the DNN is trained on the MLSI results. We take a look at the line profiles in the regions with large ε<sub>D</sub> in Figures 12 and 13. The spectral profiles have significantly shifted components, which may be attributed to the presence of fast-moving plasma or cloud-like plasma, such as chromospheric jets.

### 4.3. Temporal Variations

We examine whether the physical parameters from the DNN are consistent with those MLSI over time. Figures 14 and 15 present the temporal variations of the Doppler velocity (v<sub>0</sub>), the hydrogen temperature (T<sub>H</sub>), and the nonthermal velocity (ξ) that were taken from the network feature in the QR and the superpenumbral fibril in the AR. The estimates of T<sub>H</sub> and ξ are determined by the combination of the inferred Doppler widths at the upper chromosphere (w<sub>0</sub>) from two absorption lines, the method for which is explained in Chae et al. (2020, 2021a). The temporal variations show that the Doppler velocity, temperature, and nonthermal velocity fluctuate over time. We find that the values inferred from the DNN are comparable to the values from MLSI. These values are physically reasonable for the features, as the T<sub>H</sub> (ξ) for the network and the superpenumbral fibril are 11,400 K (9.6 km s<sup>−1</sup>) and 10,200 K (8.4 km s<sup>−1</sup>), on average, indicating that the DNN reliably predicts even the temporal variations of the physical parameters derived by MLSI.

### 4.4. Computing Time

Our investigation aims to reduce the cost of the MSLI calculations for inferring the chromospheric plasma properties from the enormous number of line profiles over position and time. The MLSI proposed in Chae et al. (2021a) is slower in fitting the observed absorption profiles. It takes 95 ms for each Hα line profile, and 81 ms for each CaII line profile, with Python 3.8 software on a computer server with a 3.8 GHz AMD Ryzen Threadripper 3960X CPU. Our server is equipped with GPUs, but the MLSI calculation is only conducted using the CPU. It therefore takes about 2.5 hr for Hα and 2 hr for CaII when reproducing the same field of view (40° × 40°) of the sample image of the AR.

By comparison, using the same system, the DNN reproduces the physical parameter maps for the field of view of the AR within 20 s, using the CPU. It takes 0.3–0.4 ms on average to reproduce the physical parameters for each absorption line profile. To synthesize the spectral profiles using the physical parameters recovered from the DNN takes 3.4 ms for each Hα
![img-11.jpeg](img-11.jpeg)

Figure 12. Three-layer model fitting of the Ca II $8542 \AA$ line profiles taken from the region with a large value of $\epsilon_{D}$, marked by the square symbol in Figure 4 (QR), using the different models (left: our DNN; right: MLSI).
![img-12.jpeg](img-12.jpeg)

Figure 13. Three-layer model fitting of the $\mathrm{H}_{\mathrm{O}}$ line profiles taken from the region with a large value of $\epsilon_{D}$, marked by the square symbol in Figure 5 (AR), using the different models (left: our DNN; right: MLSI).
and Ca II line profile, on average. The computing time of the DNN for the parameters (line profiles) is 250 (25) times faster than MLSI. Moreover, if we use the GPUs that this system is equipped with, for acceleration, it takes 0.03 ms to predict the physical parameters and 2.84 ms to synthesize the spectra. This advantage of the fast inversion model allows us to investigate spatial distributions and temporal evolutions of chromospheric plasma over a reasonably short period of time.

## 5. Summary

We have produced a DNN application for estimating the physical parameter outputs from MLSI, which was developed by Chae et al. $(2020,2021 a)$, in order to make applications to large data sets feasible. Our DNN is extremely fast, producing physical parameters in the photosphere and chromosphere, as well as synthesized intensity profiles of both absorption lines, in about 2.84 ms for each line profile, using GPUs. The test
![img-13.jpeg](img-13.jpeg)

Figure 14. Temporal variations of the Doppler velocity ( $v_{0}$ ), the temperature of hydrogen, and the nonthermal velocity at the upper chromosphere of the network feature in the QR, determined from the DNN and MLSI.
![img-14.jpeg](img-14.jpeg)

Figure 15. Temporal variation of the Doppler velocity ( $v_{0}$ ), the temperature of hydrogen, and the nonthermal velocity at the upper chromosphere of the superpenumbral fibril in the AR, determined from the DNN and MLSI.
results of the DNN described in Section 4 prove that this model is able to accurately generate the physical parameters of chromosphere plasma in both QRs and ARs. Moreover, the
variations of hydrogen temperature and nonthermal velocity in the upper chromosphere can also be investigated, by virtue of the high precision of the Doppler width measurements, as with MLSI.

Despite the fact that the high performance of the DNN has been demonstrated, there are still caveats to consider. Our approach, using a deep learning tool, is a kind of supervised learning, which is significantly affected by the training data, the observed spectra, and the derived parameters, based on MLSI. If the MLSI model is not able to make a good fit to an observed spectrum, the trained DNN will not well reproduce the physical parameters or the modeled intensity profile. Therefore, first, the model is applicable to QRs and ARs, but not to flares that have abnormal profiles, such as emission profiles due to strongly heated plasma or sudden density enhancement. Those flare spectra could possibly be analyzed by a sophisticated model, adding more atmospheric layers or combining a specific model for flare spectra, such as an INN using RADYN with $\mathrm{H}_{\mathrm{O}}$ and Ca II, as proposed by Osborne et al. (2019). Second, most of the training data sets for the DNN from FISS observations consist of sunspots, pores, or emerging flux regions near the disk center. Therefore, applying our DNN to QRs or coronal holes may be less reliable, and it may not be appropriate for observations closer to the limb. Third, atypical profiles-e.g., spectra with largely shifted components or very broad profiles, which imply fast-moving or cloud-like features in the chromosphere-are fairly fitted $\left(1<\epsilon_{D}<3\right)$ compared to the flaring spectra. Still, when the predicted physical parameters result in the synthesized spectra being significantly different from the observations, and hence having large values of $\epsilon_{D}$, the DNN results should be interpreted with caution. In the same way, however, it may be possible to utilize the value of $\epsilon_{D}$ as an identifier of anomalous profiles, sometimes the result of fastmoving or strongly heated plasma, which would need a more detailed analysis.

We are planning to adapt our DNN to the FISS observation database. FISS provides good-quality imaging data and spectra with a high temporal resolution, which is appropriate for investigating oscillations, waves, or small flaring events (Yang et al. 2014; Cho et al. 2016; Kang et al. 2019; Kwak et al. 2020; Chae et al. 2021b). The DNN will allow us to analyze the huge amount of FISS data much faster than by using MLSI. For example, in 2013, FISS observed about 7400 scan rasters for both $\mathrm{H}_{\mathrm{O}}$ and Ca II ( $\sim 8.8 \times 10^{8}$ spectra). It could take 2.5 yr to obtain the inverted parameters and synthesized spectra using MLSI, but our DNN reproduced the parameters and synthesized spectra in about a month. We can be much faster in applying the DNN to FISS observations. Moreover, the measured model parameters-such as source functions over position and time, with their high precision-make it possible to investigate the height and temporal variations of temperature or radiative losses in the photosphere and chromosphere, which is important for understanding the heating in the solar chromosphere. We also note that our DNN could possibly be applied to other observations or simulations of $\mathrm{H}_{\mathrm{O}}$ and Ca II , for instruments or models covering a similar wavelength range as FISS, by preprocessing the data set. For instance, by optimizing their wavelength scales to FISS spectra, we have been able to apply our DNN to other data sets. The trained model and code will be added to the FISSPy ${ }^{6}$ python package

[^0]
[^0]:    ${ }^{6}$ http://fiss.snu.ac.kr/fisspy/
soon. We also plan to provide physical parameter maps from the DNN, based on MLSI, via the FISS web page. ${ }^{7}$

This research was supported by the National Research Foundation of Korea (NRF-2021R1A2C1010881 and NRF2020R1A2C2004616), funded by the Korean Ministry of Science and ICT. Y.J.M. acknowledges support from the Basic Science Research Program through the National Research Foundation, funded by the Ministry of Education (NRF-2019R1A2C1002634). K.C. was supported by the Basic Science Research Program through the National Research Foundation of Korea, funded by the Ministry of Education (NRF-2020R1I1A1A01068789). The GST operation is partly supported by the Korea Astronomy and Space Science Institute, the Seoul National University, the Key Laboratory of Solar Activities of the Chinese Academy of Sciences (CAS), and the Operation, Maintenance and Upgrading Fund of CAS for Astronomical Telescopes and Facility Instruments.

Facility: Big Bear Solar Observatory (Fast Imaging Solar Spectrograph).

Software: Astropy (Astropy Collaboration et al. 2013, 2018), Fisspy (http://fiss.snu.ac.kr/fisspy), Keras (Chollet et al. 2015), Matplotlib (Hunter 2007), Numpy (Harris et al. 2020), Scikit-learn (Pedregosa et al. 2011), Scipy (Virtanen et al. 2020), TensorFlow (Abadi et al. 2015).

## ORCID iDs

Kyoung-Sun Lee (1) https://orcid.org/0000-0002-4329-9546 Jongchul Chae (1) https://orcid.org/0000-0002-7073-868X Eunsu Park (1) https://orcid.org/0000-0003-0969-286X Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944 Hannah Kwak (1) https://orcid.org/0000-0001-8619-9345 Kyuhyoun Cho (1) https://orcid.org/0000-0001-7460-725X

## References

Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, https://www.tensorflow.org/ Allred, J. C., Hawley, S. L., Abbett, W. P., \& Carlsson, M. 2005, ApJ, 630, 573 Ardizzone, L., Kruse, J., Wirkert, S., et al. 2018, arXiv:1808.04730 Asensio Ramos, A., \& Díaz Baso, C. J. 2019, A\&A, 626, A102
Asensio Ramos, A., Trujillo Bueno, J., \& Landi Degl Innocenti, E. 2008, ApJ, 683, 542
Astropy Collaboration, Price-Whelan, A. M., Sipócz, B. M., et al. 2018, AJ, 156, 123
Astropy Collaboration, Robitaille, T. P., Tollerud, E. J., et al. 2013, A\&A, 558, A33

Beckers, J. M. 1964, PhD thesis, Sacramento Peak Observatory, Air Force Cambridge Research Laboratories, Mass., USA
Carlsson, M., De Pontieu, B., \& Hansteen, V. H. 2019, ARA\&A, 57, 189
Carroll, T. A., \& Staude, J. 2001, A\&A, 378, 316
Chae, J. 2014, ApJ, 780, 109
Chae, J., Cho, K., Kang, J., et al. 2021a, JKAS, 54, 139
Chae, J., Cho, K., Nakariakov, V. M., Cho, K.-S., \& Kwon, R.-Y. 2021b, ApJL, 914, L16
Chae, J., Madjarska, M. S., Kwak, H., \& Cho, K. 2020, A\&A, 640, A45
Chae, J., Park, H.-M., Ahn, K., et al. 2013, SoPh, 288, 1
Chae, J., Yang, H., Park, H., et al. 2014, ApJ, 789, 108
Cheung, C. M. M., Wright, P. J., Galvez, R., et al. 2018, AGUFM, 2018, SM31D-3536
Cho, K., Lee, J., Chae, J., et al. 2016, SoPh, 291, 2391
Chollet, F. 2015, Keras, https://keras.io
Chollet, F. 2017, in 2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (Piscataway, NJ: IEEE)
de la Cruz Rodríguez, J., Leenaarts, J., \& Ramos, A. A. 2016, ApJL, 830, L30 de la Cruz Rodríguez, J., Leenaarts, J., Danilovic, S., \& Uitenbroek, H. 2019, A\&A, 623, A74
Glorot, X., \& Bengio, Y. 2010, PMLR, 9, 249
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Gudiksen, B. V., Carlsson, M., Hansteen, V. H., et al. 2011, A\&A, 531, A154
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585, 357
He, K., Zhang, X., Ren, S., \& Sun, J. 2015, arXiv:1512.03385
Hunter, J. D. 2007, CSE, 9, 90
Kang, J., Chae, J., Nakariakov, V. M., et al. 2019, ApJL, 877, L9
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kwak, H., Chae, J., Madjarska, M. S., Cho, K., \& Song, D. 2020, A\&A, 642, A154
Lecun, Y., \& Bengio, Y. 1995, in Convolutional Networks for Images, Speech, and Time-series, ed. M. Arbib (Cambridge, MA: MIT Press)
LeCun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436
Lee, H., Park, E., \& Moon, Y.-J. 2021, ApJ, 907, 118
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Lim, D., Moon, Y.-J., Park, E., \& Lee, J.-Y. 2021, ApJL, 915, L31
Milić, I., \& van Noort, M. 2018, A\&A, 617, A24
Osborne, C. M. J., Armstrong, J. A., \& Fletcher, L. 2019, ApJ, 873, 128
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJ, 884, L23
Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, JMLR, 12, 2825
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Ramachandran, P., Zoph, B., \& Le, Q. V. 2017, arXiv:1710.05941
Ronneberger, O., Fischer, P., \& Brox, T. 2015, arXiv:1505.04597
Sainz Dalda, A., de la Cruz Rodríguez, J., De Pontieu, B., \& Gošić, M. 2019, ApJL, 875, L18
Socas-Navarro, H., de la Cruz Rodríguez, J., Asensio Ramos, A., Trujillo Bueno, J., \& Ruiz Cobo, B. 2015, A\&A, 577, A7
Szegedy, C., Ioffe, S., Vanhoucke, V., \& Alemi, A. 2016, arXiv:1602.07261
Tzintzou, K. 2007, in ASP Conf. Ser. 368, The Physics of Chromospheric Plasmas, ed. P. Heinzel, I Dorotović, \& R. J. Rutten (San Francisco, CA: ASP), 217
Uitenbroek, H. 2001, ApJ, 557, 389
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Yang, H., Chae, J., Lim, E.-K., et al. 2014, ApJL, 790, L4
Zhang, H., Shao, J., \& Salakhutdinov, R. 2018, arXiv:1806.01845"
Jinhye Park et al 2023 - Examining the Source Regions of Solar Energetic Particles Using an AI-generated Synchronic Potential Field Source Surface Model.pdf,"# Examining the Source Regions of Solar Energetic Particles Using an AI-generated Synchronic Potential Field Source Surface Model 

Jinhye Park ${ }^{1}$ (D) Hyun-Jin Jeong ${ }^{1}$ (D) and Yong-Jae Moon ${ }^{1,2}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2022 September 2; revised 2023 June 2; accepted 2023 June 7; published 2023 August 14


#### Abstract

We study the source regions of six solar energetic particle (SEP) events accelerated near or behind the limbs of the Sun. We use AI-generated farside magnetograms at a near real-time basis developed by Jeong et al. and $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations up to $2.5 R_{\odot}$ computed using the input of the synchronic data combining AI-generated farside and HMI magnetograms. By comparing the $\mathrm{AI}_{\mathrm{HMI}}$, HMI, Global Oscillations Network Group (GONG) synoptic magnetograms, and Air force Data Assimilative Photospheric flux Transport synchronic magnetograms, as well as the PFSS extrapolations, we find interesting differences between them in view of SEP source regions and magnetic field configurations. First, the structures and sizes of the source active regions (ARs) are changed. The total unsigned magnetic field fluxes of the ARs are mostly stronger in the $\mathrm{AI}_{\mathrm{HMI}}$ than in the HMI and GONG magnetograms. Second, newly emerging ARs are observed in the SEP source regions in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms for two events. Third, the alterations in the magnetic flux, the emergence, and the dissipation of ARs lead to modifications in the locations of the global polarity inversion lines (PILs). The EUV wave propagation is typically observed to be oriented nearly perpendicular with respect to the local PIL, suggesting that the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations around the source region are more realistic. This study shows that the continuous farside evolution of AR magnetic fields, which is accomplished by our AI synchronic magnetograms, can lead to an improved understanding of SEP source ARs.


Unified Astronomy Thesaurus concepts: Solar energetic particles (1491); Solar magnetic fields (1503);
Convolutional neural networks (1938); Solar flares (1496); Solar coronal mass ejections (310)

## 1. Introduction

Solar energetic particles (SEPs) consist of protons, electrons, and heavy ions with an energy range from a few hundred keV to MeV . They are accelerated in magnetic reconnection regions and by coronal mass ejection (CME)-driven shocks (Kahler 1994; Reames 1999; Kallenrode 2003; Cane et al. 2006; Tylka \& Lee 2006; Gopalswamy et al. 2008; Reames 2013). There are two types of SEP events, impulsive and gradual. Impulsive events are related to jets having open field lines and type III radio emissions. They generally have a narrow cone of emission and short durations of several hours. They are electron-rich and show enhanced $\mathrm{Fe} / \mathrm{O}$ and ${ }^{3} \mathrm{He} /{ }^{4} \mathrm{He}$ compared with the nominal coronal abundances (Bučík 2020; Reames 2020). Meanwhile, gradual events are mainly accelerated by wide and fast CME-driven shocks (Kahler 2001; Gopalswamy 2003; Park et al. 2012; Kahler \& Vourlidas 2013; Park et al. 2013, 2015). They are typically associated with gradual X-ray flares, type II, and type IV radio emissions. They have larger emission cones and longer durations than those in impulsive events (Kahler 1994; Reames 1999). Gradual events are proton-rich, and ion compositions exhibit typical coronal abundances (Reames 1998). Reames (2020) recently distinguished four SEP populations based on abundance patterns of chemical elements. In addition to pure impulsive and general gradual events, two more mixed types are suggested: impulsive events with ions accelerated by narrow CME shocks from jets

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
and gradual events with remnant ions accelerated by multiple jets. The SEP events are one of the crucial phenomena in terms of space weather. They are likely to have large fluences and can pose serious radiation hazards.

Potential field source surface (PFSS) models extrapolate the large-scale solar magnetic fields in the corona using the photospheric observations (Altschuler \& Newkirk 1969; Schatten et al. 1969; Riley et al. 2006). These models are widely used to examine the structure of the magnetic fields with diverse solar activities and provide the input data of heliospheric propagation models such as WSA-ENLIL (Arge \& Pizzo 2000). Conventional synoptic maps of the magnetic field lines are constructed by merging frontside magnetograms near the central meridian over one synodic solar rotation period of 27.3 days. The farside data was obtained when it was near the central meridian a few weeks earlier. Recently, Laker et al. (2021) investigated the solar wind structure and heliospheric current sheet (HCS) using multiple observations with latitude across two solar rotations between May and July 2020 during solar minimum. They showed that the HCS structure obtained from the variation in magnetic polarity observation are flat within $\pm 10^{\circ}$ latitude and they were broadly consistent with the shape of the HCS from the conventional PFSS extrapolation.

Jeong et al. (2020) generated the solar farside magnetograms using Solar Terrestrial Relations Observatory (STEREO)/ Extreme UltraViolet Imager (EUVI; Howard et al. 2008) A and B extreme ultraviolet (EUV) images (304, 195, and 171 A) by a Pix2PixHD model, which is a deep-learning method for image translation of high-resolution images, with Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) EUV images and Helioseismic and Magnetic Imager (HMI; Howard et al. 2008)
magnetograms. They made global magnetic field synchronic maps replaced by AI-generated farside magnetic fields on a near real-time basis. They showed that synchronic maps are more consistent with coronal observations compared with conventional maps in view of the distributions of solar active regions (ARs) and open magnetic field regions. Recently, Jeong et al. (2022) improved AI-generated solar farside magnetograms and generated more realistic magnetograms than the previous model. They used updated objective functions, which include correlation coefficients (CCs) between the real and generated data, as well as the construction of input data sets: STEREO EUV observations together with nearest frontside SDO data pairs of EUV observations and magnetograms. The improved model shows the high CCs ( $0.7-0.9$ ) between real and AI-generated magnetograms for full disk, ARs, and quiet regions. The total unsigned magnetic flux and net magnetic flux of the AI-generated magnetograms are well consistent with those in the real magnetograms. Currently, the farside magnetograms by this method are released at http:// sdo.kasi.re.kr.

In this study, we examine the magnetic field configurations of the source regions and global configurations for six SEP events accelerated near the eastern limb or behind the limbs, where the conventional PFSS models use photospheric magnetic data taken a few weeks ago. The six events were detected between 2010 and 2014 when solar activities increased. We use the synchronic magnetograms combining AI-generated farside magnetograms and HMI magnetograms (hereafter $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms), and the global coronal field extrapolations up to $2.5 R_{\odot}$ driven by the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms (hereafter $\mathrm{AI}_{\mathrm{HMI}}$ extrapolation). We use the HMI and Global Oscillations Network Group (GONG) synoptic maps, as well as the PFSS extrapolations from the magnetograms compared with the $\mathrm{AI}_{\mathrm{HMI}}$ ones. We also use data obtained from Air force Data Assimilative Photospheric flux Transport (ADAPT) model, which is a synchronic model that is based on localized ensemble Kalman filtering techniques (Hickmann et al. 2015). The model includes the effects of differential rotation, meridional flow, supergranulation, and random background flux. In the period between 2010 and 2014, the ADAPT model from the HMI magnetograms was not available, and so we use GONG magnetograms instead. Using the $\mathrm{AI}_{\mathrm{HMI}}$, the HMI, the GONG, and the surface flux transport magnetograms, we examine the magnetic field structures and strengths of SEP source regions and global configurations. The field connections between the sources and the source surface magnetic footpoints, as well as the global polarity inversion lines (PILs) are investigated using the PFSS extrapolations. The source surface magnetic footpoints of the spacecraft are estimated at $2.5 R_{\odot}$ based on Parker spiral approximation and ballistic backmapping method (Nolte \& Roelof 1973; Badman et al. 2020; Macneil et al. 2022). The structure of this paper is given as follows. We describe the data and analysis in Section 2, and we give the results in Section 3. The summary and discussion are given in Section 4.

## 2. Data and Analysis

We consider SEP events from 2010 to 2014 where STEREO-A, STEREO-B, and SDO were sufficiently well separated in the ecliptic plane to provide full-Sun EUV coverage. This allows the production of synchronic EUV maps directly from observations (using STEREO/EUVI and SDO/

AIA), and therefore the production of synchronic $\mathrm{AI}_{\mathrm{HMI}}$ maps for this time interval (Jeong et al. 2020, 2022). We use NOAA solar proton events, ${ }^{3}$ with SEP fluxes greater than or equal to 10 pfu in $>10 \mathrm{MeV}$ proton channel measured by GOES between 2010 and 2014. We also use proton fluxes in the ranges of $1.8-60 \mathrm{MeV}$ in STEREO Low Energy Telescope (LET; Mewaldt et al. 2008) and High Energy Telescope (HET; vonRosenvinge et al. 2008), 4.2-200 MeV in GOES15 Energetic Proton, Electron and Alpha Detector (EPEAD), and 2.11-67.3 MeV in SOHO Energetic and Relativistic Nuclei and Electron (ERNE; Torsti et al. 1995). We exclude events with complex flux profiles due to multiple SEP events and interplanetary shocks. We use the IP shock lists for ACE and WIND, ${ }^{4}$ STEREO, ${ }^{5}$ database of interplanetary shocks, ${ }^{6}$ and Near-Earth Interplanetary Coronal Mass Ejections list. ${ }^{7}$ From this analysis, we obtain 27 events and examine their magnetograms and PFSS extrapolations. Of the 27 events, we selected six events whose source regions were located at or near the solar limbs, and exhibited significant morphological differences in the resulting $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms compared to HMI, GONG, and ADAPT magnetograms for the same intervals.

Table 1 shows the six events that are studied in this paper and their associated solar activity. All six events have previously been associated with a specific source active region located on the farside or near the east limb. Hereafter, the events of 2011 September 21, 2011 September 22, 2012 September 27, 2013 March 5, 2013 June 21, and 2014 January 6 are termed the N1, N2, N3, N4, N5, and N6 events, respectively. Flare information is taken from ftp://ftp.ngdc. noaa.gov/STP/space-weather/solar-data/solar-features/solar-flares/x-rays/goes. Locations of the non-reported X-ray flares are examined in the full-Sun EUV Stonyhurst Heliographic images composed of STEREO/EUVI $195 \AA$ images and SDO/ AIA $193 \AA$ images with a cadence of 5 minutes. In the apparent locations based on EUV synchronic maps in Figure 1, four of the event's source regions were located behind the limbs ( N 1 at $\mathrm{W} 120^{\circ}, \mathrm{N} 3$ at $\mathrm{W} 150^{\circ}, \mathrm{N} 4$ at $\mathrm{E} 150^{\circ}$, and N 6 at $\mathrm{W} 110^{\circ}$ ). The remaining two were on disk near the east limb ( N 2 at $\mathrm{E} 74^{\circ}$ and N 5 at $\mathrm{E} 73^{\circ}$ ) and they were associated with X1.4 and M2.9 flares, respectively. CME information is taken from http:// cdaw.gsfc.nasa.gov/CME_list. All the events are associated with fast CMEs over $1000 \mathrm{~km} \mathrm{~s}^{-1}$. Three of the events (N2, N5, and N6) are included in the NOAA solar proton event list (see footnote 3).

Table 1 also shows the onset times of the SEP fluxes events and peak times (fluxes) measured by $6-10 \mathrm{MeV}$ proton channel of the STEREO/LET and $8.7-14.5 \mathrm{MeV}$ proton channel of the GOES/EPEAD. All six events are detected in situ by multiple spacecraft (see column 12). The quoted times and peak fluxes in Table 1 are taken from the spacecraft whose source surface footpoints are closest to the source regions. The peak fluxes are measured as the points at the top of the of the steep flux rise just after the solar eruptions associated the SEP events.

In this study, we use the improved AI-generated solar farside magnetograms of Jeong et al. (2022), as follows. First, we

[^0]
[^0]:    3 https://umbra.nascom.nasa.gov/SEP/
    4 https://www.cfa.harvard.edu/shocks/
    5 https://stereo-ssc.nascom.nasa.gov/pub/ins_data/impact/level3/
    STEREO_Level3_Shock.pdf
    6 http://ipshocks.fi
    7 http://www.srl.caltech.edu/ACE/ASC/DATA/level3/icmetable2.htm
![img-0.jpeg](img-0.jpeg)

Figure 1. The running difference images from the composite of STEREO-A, B EUVI $195 \AA$, and SDO AIA $193 \AA$ images at the times given. The source regions are enclosed by the yellow squares. The red, blue, and yellow crosses mark the magnetic footpoints of STEREO-A, B, and Earth at $2.5 R_{\odot}$. (a) the N1 event (2011 September 21), (b) the N2 event (2011 September 22), (c) the N3 event (2012 September 27), (d) the N4 event (2013 March 5), (e) the N5 event (2013 June 21), and (f) the N6 event (2014 January 6).

Table 1
The Associated Solar Activities for the Six SEP Events, the SEP Onset Times, and the Peak Times

| No. <br> (1) | Date <br> (2) | Flare $^{\mathrm{a}}$ <br> Peak <br> (3) | Class <br> (4) | Location <br> (5) | $\begin{gathered} \text { AR } \\ (6) \end{gathered}$ | $\begin{gathered} \text { CME }^{\mathrm{b}} \\ \text { Time } \\ (7) \end{gathered}$ | $V\left(\mathrm{~km} \mathrm{~s}^{-1}\right)$ <br> (8) | $\begin{gathered} \text { AW }\left({ }^{\circ}\right) \\ (9) \end{gathered}$ | Start <br> (10) | Peak <br> (11) | In Situ <br> (12) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| N1 | 20110921 | $\cdots$ | $\cdots$ | W120 | 11289 | 22:12 | 1007 | $>255$ | 22 01:20UT | 22 05:00UT (0.838) | S,A |
| N2 | 20110922 ${ }^{\text {d }}$ | 11:01 | X1.4 | N11E74 | 11302 | 10:48 | 1905 | 360 | 22 11:20UT | 22 22:50UT (297.730) | B,S(G),A |
| N3 | 20120927 | $\cdots$ | $\cdots$ | W150 | $\cdots$ | 10:12 | 1319 | 360 | 27 12:40UT | 27 18:30UT (30.836) | B,S(G),A |
| N4 | 20130305 | $\cdots$ | $\cdots$ | E150 | $\cdots$ | 03:48 | 1316 | 360 | 5 04:30UT | 5 09:20UT (175.340) | B,S,A |
| N5 | 20130621 ${ }^{\text {d }}$ | 03:14 | M2.9 | S16E73 | 11777 | 03:12 | 1900 | 207 | 21 04:10UT | 21 18:50UT (14.789) | B,S(G),A |
| N6 | 20140106 ${ }^{\text {d }}$ | $\cdots$ | $\cdots$ | W110 | 11936 | 08:00 | 1402 | 360 | 6 09:50UT | 6 12:25UT (1.710) | B,S(G),A |

Notes. AW denotes CME angular width. G, S, A, and B represent GOES/EPEAD, SOHO/EREN, STEREO-A/LET, and STEREO-B/LET, respectively. The quoted times and peak fluxes are taken from the spacecraft (underlines in the column 12) whose source surface footpoints are closest to the source regions.
${ }^{\text {a }}$ Flare information is taken from ftp://ftp.ngdc.noaa.gov/STP/space-weather/solar-data/solar-features/solar-flares/x-rays/goes. Locations for farside SEP events (no flare data) are estimated using full-Sun EUV Stonyhurst Heliographic images composed of STEREO Extreme Ultraviolet Imager EUVI $195 \AA$ images and SDO/ AIA $193 \AA$ images with a cadence of 5 minutes.
${ }^{\mathrm{b}}$ CME information is taken from http://cdaw.gsfc.nasa.gov/CME_list and http://secchi.nrl.navy.mil/cactus.
${ }^{\text {c }}$ The peak fluxes in the fourth column are measured as the points at the top of the of the steep flux rise in the $6-10 \mathrm{MeV}$ proton channel of the STEREO/LET and $8.7-14.5 \mathrm{MeV}$ proton channel of the GOES/EPEAD. Flux unit is $\mathrm{cm}^{-2} \mathrm{~s}^{-1} \mathrm{sr}^{-1} \mathrm{MeV}^{-1}$.
${ }^{\mathrm{d}}$ The events are in the NOAA solar proton event list (https://umbra.nascom.nasa.gov/SEP/).
convert the AI-generated ones from full disk data to heliographic coordinated maps. Second, we convert from the line of sight to the radial magnetic fields by applying the radial-
acute method (Wang \& Sheeley 1992) based on their coordinates. Third, we make HMI and AI synchronic data by replacing a farside part (Stonyhurst heliographic longitude $\geqslant|60|^{\circ}$ ) of the
conventional HMI synoptic data by the AI-generated data. For the HMI synoptic data, we use the HMI daily updated radial field synoptic map with polar field correction (Sun et al. 2011). Fourth, we extrapolate coronal magnetic fields from 1 to $2.5 R_{\odot}$ using the PFSS model, with the HMI and AI synchronic data as input. We use pfsspy software package (Stansby et al. 2020) to compute the synchronic PFSS model and trace the magnetic field lines.

## 3. Results

### 3.1. Magnetic Field Configurations and Strengths of Source Regions

Figure 2 shows the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS, HMI-PFSS, GONG-PFSS, and ADAPT-PFSS extrapolations of the N1 and N2 events, which are shown together because they occurred very close together in time. Figures 3-6 are the PFSS extrapolations for the N3, N4, N5, and N6 events, respectively. The source surface magnetic footpoints of the STEREO-B, Earth, and STEREO-A at $2.5 R_{\odot}$ (crosses in Figures 2-6) are calculated by a Parker spiral approximation using solar wind speeds obtained from ACE Solar Wind Electron Proton Alpha Monitor (SWEPAM; McComas et al. 1998), STEREO Plasma and Suprathermal Ion Composition (PLASTIC; Galvin et al. 2008) measurements at the event start times, and the ballistic backmapping method (Nolte \& Roelof 1973; Badman et al. 2020; Macneil et al. 2022).

The six events show that $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations contain significant alterations in coronal magnetic topology and PILs. At least in one example (N1 and N2 events in Figure 2), the alteration to the PIL is clearly not captured by any of the conventional PFSS extrapolations, even in the case of synchronic flux transport modeling (ADAPT). These changes are attributed to variations in the strengths of ARs and the appearance or disappearance of ARs in the farside region. Table 2 shows the number of the ARs in the range of the Stonyhurst heliographic longitude $\left(|\theta| \geqslant 60^{\circ}\right)$, where the conventional magnetograms replaced by the AI-ones and the polarities of the source regions and the source surface magnetic footpoints of the spacecraft. It also shows the polarities of the SEPx source regions (white squares in Figures 2-6), as well as the polarity of the source surface magnetic footpoints of Earth (yellow crosses), STEREO-A (red crosses), and STREREO-B (blue crosses) at $2.5 R_{\odot}$. There are newly emerged farside ARs for all six events and 1-2 ARs are decayed for the N3 and N6 events in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS model. There are significant variations in the configurations and polarity signs of the magnetic fields connected to the source regions, particularly for the N3, N4, and N6 events, when comparing the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS and conventional PFSS extrapolations. The positive open field lines are primarily connected to the newly emerging AR in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolation of the N3 event. The polarities of the sources for the N4 and N6 differ between the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS and the conventional extrapolations. The polarities of the source surface magnetic footpoints of the spacecraft in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations are mostly in agreement with those obtained from the conventional extrapolations, with the exception of the N4 event. In addition, we compare the magnetic polarities obtained from in situ measurements with those derived from the four different extrapolations. There are inconsistencies between the in situ measurements and the extrapolations for the N4 event.

Figure 7 shows the source regions of the six events in the $\mathrm{AI}_{\mathrm{HMI}}$, HMI, GONG, and ADAPT magnetograms. This figure
shows that the source regions have more shaped and more complex magnetic features in the AI-generated maps than those in the conventional maps. By comparing the locations of the source regions, those in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms are mostly consistent with those in the conventional magnetograms but the source regions are more elongated within $\sim 10^{\circ}$ east and westward for the N1, N2, and N6 events, and southward for the N3 event in the AI-generated magnetogram than those in the conventional magnetograms.

Table 3 shows the unsigned total magnetic field fluxes of the source regions for the six events in the $\mathrm{AI}_{\mathrm{HMI}}$, the HMI, the GONG, and the ADAPT magnetograms. The quantities computed from all pixels in the field of views are shown in Figure 7. The magnetic fluxes are mostly larger in the AIgenerated ones than those in the HMI and GONG maps except for N1 event. For the five events, the differences are between $20 \%$ and $140 \%$. In two of the six events, the total fluxes in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms are larger than the ADAPT magnetograms.

### 3.2. Newly Emerging ARs in the SEP Source Regions

In at least three cases, the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms show newly emerged ARs appearing in the vicinity of the previously identified source region and may in fact be the true source of the N3 and N5 events. The source of the N3 event is located behind the west limb $\left(\mathrm{W} 150^{\circ}\right)$, and it is associated with a fast halo CME with $1319 \mathrm{~km} \mathrm{~s}^{-1}$ at 10:12UT (SOHO LASCO C2 appearance time). In the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS magnetogram of Figure 3, the new AR (yellow square) appears very close to the source AR (white square), and the two ARs are connected, as seen in Figure 7(c). In Table 4, the total unsigned magnetic flux of the source, including the new AR is larger in the $\mathrm{AI}_{\mathrm{HMI}}$ than those in the conventional magnetograms. The fluxes are $3.071 \times 10^{22} \mathrm{Mx}$ in the $\mathrm{AI}_{\mathrm{HMI}}, 1.865 \times 10^{22} \mathrm{Mx}$ in the HMI, $1.347 \times 10^{22} \mathrm{Mx}$ in the GONG, and $2.113 \times 10^{22} \mathrm{Mx}$ magnetogram, respectively. Figure 8 shows the running difference image of the obtained from the EUV synchronic maps (SDO/ AIA $193 \AA$ and STREREO/EUVI $195 \AA$ ) combined with magnetic field extrapolations at $2.5 R_{\odot}$, which represents the evolutions of the solar eruptions. Figure 8(c) for the N3 event shows that a solar eruption happened in the newly appeared AR, which implies that the new AR is more likely to be associated with the SEP acceleration.

The N5 event is associated with an M2.9 flare at S16E73 ${ }^{\circ}$ which occurred at 03:14UT on 2013 June 21, and a rapid CME with a linear speed of $1900 \mathrm{~km} \mathrm{~s}^{-1}$, at 03:12UT. The associated eruption was observed on the far east side of the EUV running difference image (see Figure 8(e)). However, in the GONG and HMI magnetograms (Figure 7(e)), noticeable ARs associated with the eruption are not detected at the eruption location. The $\mathrm{AI}_{\mathrm{HMI}}$ magnetogram reveals the presence of a compact source region whose location is consistent with the observed EUV eruption. Additionally, the ADAPT magnetogram, which benefits from a wide observational range over $\pm 60^{\circ}$ based on the central meridian, due to data assimilations (Hickmann et al. 2015), also detects the AR at the location of the EUV eruption. ADAPT forward models on the farside (via flux transport) would also play a role in getting features that do not appear in synoptic magnetograms. While the $\mathrm{AI}_{\mathrm{HMI}}$ magnetogram provides the detailed structures of the source region, the ADAPT magnetogram has limited ability to resolve them.
![img-1.jpeg](img-1.jpeg)

Figure 2. From top to bottom: (a) AI $_{\text {HMI }}$-PFSS, (b) HMI-PFSS, (c) GONGPFSS, and (d) ADAPT-PFSS extrapolations for the N1 and N2 events (2011 September 21 and 22). The red, blue, and yellow crosses represent the magnetic footpoints of STEREO-A, B, and Earth at $2.5 R_{\odot}$, respectively. The red, blue, and yellow dots represent the photospheric magnetic footpoints of STEREO-A, B, and Earth at $1 R_{\odot}$, respectively. The blue lines are the PILs. The green and red lines indicate positive and negative open magnetic fields, respectively. The right-hand white square is the source region of the N1 event and the left-hand white square is the source region of the N2 event. Newly observed ARs are enclosed by yellow squares.

In the N1 event, newly generated ARs appear on both the east and west sides of the source region at $\mathrm{W} 120^{\circ}$, as shown in the $\mathrm{AI}_{\text {HMI }}$-PFSS extrapolation of Figure 2(a). The newly generated ARs are located at a distance of approximately
![img-2.jpeg](img-2.jpeg)

Figure 3. From top to bottom: (a) $\mathrm{AI}_{\text {HMI }}$-PFSS, (b) HMI-PFSS, (c) GONGPFSS, and (d) ADAPT-PFSS extrapolations for the N3 event (2012 September 27). The SEP source region is enclosed by a white square. The newly observed source region is enclosed by a yellow square.
$\sim 10^{\circ}-20^{\circ}$ from the source region, and no significant eruptions are detected in the newly generated ARs at the time of the flare and the CME eruptions.

### 3.3. Polarity Inversion Lines Near the SEP Source Regions

Figure 8 shows the full-Sun running difference images (STEREO-A, B EUVI $195 \AA$ and SDO AIA $193 \AA$ ) with the $\mathrm{AI}_{\text {HMI }}$-PFSS extrapolations. As seen in the figure, the large
![img-3.jpeg](img-3.jpeg)

Figure 4. From top to bottom: (a) AI $_{\text {HMI }}$-PFSS, (b) HMI-PFSS, (c) GONG-PFSS, and (d) ADAPT-PFSS extrapolations for the N4 event (2013 March 5). The SEP source region is enclosed by a white square.
disturbances of the EUV waves from the source regions are observed except for the N5 event, which is associated with the small and compact source region compared with the others. The waves are thought of as the signatures of the CME-driven shock in the solar corona region. They appear as faint fronts moving with velocities up to $1000 \mathrm{~km} \mathrm{~s}^{-1}$ in the low corona region being associated with solar eruptions (Moses et al. 1997; Thompson et al. 1998). They can be refracted by ARs and streamers, and attenuated through them.
![img-4.jpeg](img-4.jpeg)

Figure 5. From top to bottom: (a) $\mathrm{AI}_{\mathrm{HMI}}$-PFSS, (b) HMI-PFSS, and (c) GONG-PFSS, and (d) ADAPT-PFSS extrapolations for the N5 event (2013 June 21). The SEP source region is enclosed by a white square.

For the six events in this study, the propagation configurations of the EUV waves are mostly comparable to the configurations of the PILs in the $\mathrm{AI}_{\text {HMI }}$-PFSS. In the N4 and N6 events, the PILs near the SEP source regions are different between the $\mathrm{AI}_{\text {HMI }}$-PFSS and the other conventional extrapolations. In Figure 8(d), the EUV waves associated with the N4 event mostly propagate from the source region to the northward, and the path is comparable to the configuration of the inversion line in the $\mathrm{AI}_{\text {HMI }}$-PFSS extrapolation. The source
![img-5.jpeg](img-5.jpeg)

Figure 6. From top to bottom: (a) $\mathrm{AI}_{\mathrm{HMI}}$-PFSS, (b) HMI-PFSS, and (c) GONG-PFSS, and (d) ADAPT-PFSS extrapolations for the N6 event (the 2014 January 6). The SEP source region is enclosed by a white square.
region of the N6 event is located at $\mathrm{W} 110^{\circ}$ enclosed by the white square in Figure 6. The $\mathrm{AI}_{\mathrm{HMI}}$-PFSS plot shows that the source region is surrounded by the inversion line and the sector of the source region has a negative polarity. Meanwhile, the other extrapolations show the inversion line located down to the source region, and the western region including the source AR has mostly a positive polarity. As seen in Figure 7(f) the large and strong magnetic fields of the source region are generated in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetogram compared with the conventional ones, which is may make the negative sector in the western region in the PFSS extrapolations. The EUV wave propagation is typically observed to be oriented nearly perpendicular with respect to the local PIL, suggesting that the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations around the source region are more realistic.

## 4. Summary and Discussion

In this study, we have examined the global magnetic field configurations for the six SEPs accelerated near or behind the limbs. We have examined the events using the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations, which were developed by Jeong et al. (2022), and the conventional extrapolations (HMI, GONG, and ADAPTPFSS extrapolations). The main advantage of the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolation is that it incorporates realistic near real-time farside magnetic field information informed by EUV data, rather than assuming it is frozen in time (as with HMI and GONG synoptic maps) or forward modeling old data (as with ADAPT synchronic maps). For the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolation, the synchronic data, which are constructed by combining frontside SDO/HMI magnetograms and farside AI-generated ones from STEREO/ EUVI observations, are used for the input data. Jeong et al. (2020) shows that the global fields in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms are more consistent with limb and farside observations than the conventional magnetic field synoptic data, especially for rapid changes in the magnetic fields by flux emergence or disappearance. We summarize this study as follows.

1. The locations of the source regions are not much different for the six events between the $\mathrm{AI}_{\mathrm{HMI}}$ and the conventional magnetograms (Figure 7). For the events of the N1, N2, and N6 events, it is seen that the source regions are more elongated within $\sim 10^{\circ}$ east and westward. In the N3 event, the source regions are more elongated within $\sim 10^{\circ}$ southward. The total unsigned magnetic field fluxes of the source ARs are mostly stronger in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetogram than those in the HMI and GONG magnetograms, except for one event. In two of the six events, the total fluxes in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetograms are larger than the ADAPT magnetograms.
2. Newly emerged ARs in the SEP source regions are observed in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS for N3 and N5 events. For the N3 event, the location of the emission features in the running difference images from full-Sun EUV synchronic maps is more consistent with the location of the newly generated AR in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolation. For the N5 event, the source region is detected at S16E73 ${ }^{\circ}$ in the EUV observation and the compact AR is generated at the location in the $\mathrm{AI}_{\mathrm{HMI}}$ magnetogram, but there is no noticeable AR at the location in the HMI and GONG-PFSS magnetogram. The ADAPT magnetogram detects the AR due to the data assimilation method, but it is hard to examine the detailed structure for the AR. This implies that these source regions are likely to continue to strengthen. They can be expected to have an impact on the coronal topology and are likely to host eruptive events.
3. The PILs are changed due to the appearance and/or the disappearance of the ARs. In the running difference EUV images for the six events, the propagation directions of the source eruptions are compared to the configurations of the PILs in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations. For the N4
Table 2
The Number of the ARs in the Range of the Stonyhurst Heliographic Longitude $\left(|\theta| \geqslant 60^{\circ}\right)$, the Polarities of the Source Regions, and the Polarities of the Spacecraft Source Surface Footpoints at $2.5 R_{\odot}\left(S_{f}\right)$ in the, $\mathrm{AI}_{\mathrm{HMI}}$-PFSS, HMI-PFSS, GONG-PFSS, and ADAPT-PFSS Extrapolations for the Six SEP Events

|  |  | $A R_{u}$ |  |  |  | Source Polarity |  |  |  | $S_{f}$ Polarity (B, E, A) |  |  |  |  | Obs <br> (16) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| No. <br> (1) | Data <br> (2) | $\begin{gathered} \mathrm{AI}_{\mathrm{HMI}} \\ (3) \end{gathered}$ | $\begin{gathered} \text { HMI } \\ (4) \end{gathered}$ | $\begin{gathered} \text { GONG } \\ (5) \end{gathered}$ | ADAPT <br> (6) | $\begin{gathered} \mathrm{AI}_{\mathrm{HMI}} \\ (7) \end{gathered}$ | HMI <br> (8) | $\begin{gathered} \text { GONG } \\ (9) \end{gathered}$ | ADAPT <br> (10) | $\begin{gathered} \mathrm{AI}_{\mathrm{HMI}} \\ (11) \end{gathered}$ | $\begin{gathered} \text { HMI } \\ (12) \end{gathered}$ | $\begin{gathered} \text { GONG } \\ (13) \end{gathered}$ | ADAPT <br> (14) | In Situ <br> (15) |  |
| N1 | 20110921 | 13 (4,0) | 9 | 9 | 10 | N | N | N | N | PN N' PN | P' N' N' | PN N' N' | P' N' PN | P N N | S,A |
| N2 | 20110922 | 13 (4,0) | 9 | 9 | 10 | P | P | P | P | PN N' PN | P' N' N' | PN N' N' | P' N' PN | P N N | $\underline{\text { B }}, \mathrm{S}(\mathrm{G}), \mathrm{A}$ |
| N3 | 20120927 | 7 (1,1) | 7 | 7 | 6 | P | P | P | P | PN PN P | N PN P | N PN P | PN PN P | P N PN | $\underline{\text { B }}, \mathrm{S}(\mathrm{G}), \underline{\mathrm{A}}$ |
| N4 | 20130305 | 7 (2,0) | 5 | 5 | 5 | P | N | N | N | P N' P | P N' N' | P N' N' | P N' N' | N N N | B,S,A |
| N5 | 20130621 | 7 (1,0) | 6 | 6 | 6 | P | P | P | P | P' N' P' | P' N' P' | P' N' P' | P' N' P' | P N P | $\underline{\text { B }}, \mathrm{S}(\mathrm{G}), \mathrm{A}$ |
| N6 | 20140106 | 6 (2,2) | 6 | 6 | 6 | N | P | P | P | N P' P | N P' P | N P' P | N P' P | PN P PN | $\underline{\text { B }}, \mathrm{S}(\underline{\mathrm{G}}), \mathrm{A}$ |

Note. The numbers in the parenthesis represent newly generated ARs and degenerated ARs in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations. Spacecraft detecting the SEP events are listed in column 16. G, S, A, and B represent GOES/EPEAD, SOHO/EREN, STEREO-A/LET, and STEREO-B/LET, respectively. Spacecraft that are underlined indicate that the source surface footpoints are closest to the source regions. * The single polarities in columns 11-14 are agree with in situ observation in column 15 .
![img-6.jpeg](img-6.jpeg)

Figure 7. The source regions of the six events in the $\mathrm{AI}_{\mathrm{HMI}}$, HMI, GONG, and ADAPT magnetograms: (a) the N1 event (2011 September 21), (b) the N2 event (2011 September 22), (c) the N3 event (2012 September 27), (d) the N4 event (2013 March 5), (e) the N5 event (2013 June 21), and (f) the N6 event (2014 January 6).
![img-7.jpeg](img-7.jpeg)

Figure 8. The full-Sun running difference images (STEREO-A, B EUVI $195 \AA$ and SDO AIA $193 \AA$ ) with the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations at the times given: (a) the N1 event (2011 September 21), (b) the N2 event (2011 September 22), (c) the N3 event (2012 September 27), (d) the N4 event (2013 March 5), (e) the N5 event (2013 June 21), and (f) the N6 event (2014 January 6). The black arrows indicate EUV wave propagations associated with the SEP events.

Table 3
Total Unsigned Magnetic Fluxes of the Source Regions from $\mathrm{AI}_{\mathrm{HMI}}$, HMI, GONG, and ADAPT Magnetograms for the Six SEP Events

| No. | Data | Longitude | Latitude | $\begin{gathered} \mathrm{AI}_{\mathrm{HMI}} \\ \left(\times 10^{23} \mathrm{Mx}\right) \\ (5) \end{gathered}$ | $\begin{gathered} \text { HMI } \\ \left(\times 10^{23} \mathrm{Mx}\right) \\ (6) \end{gathered}$ | GONG $\left(\times 10^{23} \mathrm{Mx}\right)$ <br> (7) | $\begin{gathered} \text { ADAPT } \\ \left(\times 10^{23} \mathrm{Mx}\right) \\ (8) \end{gathered}$ | ${ }^{a}[\mathrm{AI}, \mathrm{HMI}]$ <br> (9) | ${ }^{b}[\mathrm{AI}$, <br> GONG] <br> (10) | ${ }^{c}[\mathrm{AI}$, <br> ADAPT] <br> (11) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| N1 | 20110921 | $\left[+110^{\circ} \pm 20^{\circ}\right]$ | $\left[+25^{\circ} \pm 20^{\circ}\right]$ | 3.405 | 3.946 | 2.783 | 5.160 | $-13.72 \%$ | $22.34 \%$ |
| N2 | 20110922 | $\left[-75^{\circ} \pm 20^{\circ}\right]$ | $\left[+20^{\circ} \pm 20^{\circ}\right]$ | 5.491 | 3.167 | 2.316 | 2.739 | $73.37 \%$ | $137.05 \%$ |
| N3 | 20120927 | $\left[+152^{\circ} \pm 15^{\circ}\right]$ | $\left[-20^{\circ} \pm 15^{\circ}\right]$ | 3.071 | 1.865 | 1.347 | 2.113 | $64.66 \%$ | $127.98 \%$ |
| N4 | 20130305 | $\left[-140^{\circ} \pm 20^{\circ}\right]$ | $\left[+18^{\circ} \pm 20^{\circ}\right]$ | 3.535 | 2.652 | 1.851 | 5.393 | $33.29 \%$ | $90.99 \%$ |
| N5 | 20130621 | $\left[-62^{\circ} \pm 15^{\circ}\right]$ | $\left[-15^{\circ} \pm 15^{\circ}\right]$ | 2.100 | 1.714 | 1.172 | 2.617 | $22.50 \%$ | $79.14 \%$ |
| N6 | 20140106 | $\left[+108^{\circ} \pm 20^{\circ}\right]$ | $\left[-18^{\circ} \pm 20^{\circ}\right]$ | 5.717 | 3.834 | 2.888 | 6.240 | $49.12 \%$ | $97.98 \%$ |

# Notes. 

${ }^{\mathrm{a}}[\mathrm{AI}, \mathrm{HMI}]=(\mathrm{AI}-\mathrm{HMI}) / \mathrm{HMI}^{\wedge} 100$.
${ }^{\mathrm{b}}[\mathrm{ALGONG}]=\left(\mathrm{AI}-\mathrm{GONG}\right) / \mathrm{GONG}^{\wedge} 100$.
${ }^{\mathrm{c}}[\mathrm{ALADAPT}]=\left(\mathrm{AI}-\mathrm{ADAPT}\right) / \mathrm{ADAPT}^{\wedge} 100$.
and N6 events, the magnetic polarity sectors containing the source ARs are changed.
4. In five out of the six events, when accounting for all footpoints including those near the PILs having mixed polarities, the footpoint polarities in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations remain consistent with the modeled polarities. Furthermore, in these five out of six events,
the polarities of the footpoints in the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS extrapolations correspond to the polarities measured in situ.

In this study, we cannot rule out uncertainties in determining the source surface magnetic footpoints of the spacecraft. Nolte \& Roelof (1973) showed that the longitudes of the source
surface magnetic footpoints could be determined with an accuracy of $10^{\circ}$ using the Parker spiral approximation. Kahler et al. (2016) found that the footpoints on photospheric and $5 R_{\odot}$ source longitudes from Parker spiral field approximation are consistent with those from Wang-Sheeley-Arge within several degrees but not in the slow-fast solar wind interaction regions.

In terms of the space-weather forecast, SEPs accelerated behind the limbs are important for the observers on the SunEarth line. The observers can be magnetically connected to the SEP sources behind the limbs. In particular, there is a high possibility of directly connecting the SEPs near and behind the west limb. When considering the limitation of the farside observation in the conventional models, the $\mathrm{AI}_{\mathrm{HMI}}$-PFSS model is able to give a better understanding of SEP source regions near and behind limbs, and their magnetic field connections.

## Acknowledgments

We appreciate SDO, STEREO, SOHO, GOES, ACE, and WIND teams for their remarkable data. This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF-2020R1I1A1A01074877, NRF2023R1A2C1008051, and RS-2023-00248916), the Korea Astronomy and Space Science Institute under the R\&D program (Project No. 2023-1-850-07) supervised by the Ministry of Science and ICT (MSIT), Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-202300235534, Near-Earth $>10 \mathrm{MeV}$ Solar Proton Event Prediction by Probing into Solar Wind Condition with Automatic CME detection, $15 \%$ and No. RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15\%), and National Meteorological Satellite Center (NMSC)/ Korea Meteorological Administration (KMA) (No. 33233031800). We thank the referee for providing a detailed and constructive report on the manuscript.

## ORCID IDs

Jinhye Park (1) https://orcid.org/0000-0002-0678-2969
Hyun-Jin Jeong (1) https://orcid.org/0000-0003-4616-947X

Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944

## References

Altschuler, M. D., \& Newkirk, G. 1969, SoPh, 9, 131
Arge, C. N., \& Pizzo, V. J. 2000, JGR, 105, 10465
Badman, S. T., Bale, S. D., Martínez Oliveros, J. C., et al. 2020, ApJS, 246, 23
Bučík, R. 2020, SSRv, 216, 24
Cane, H. V., Mewaldt, R. A., Cohen, C. M. S., \& von Rosenvinge, T. T. 2006, JGRA, 111, A06S90
Galvin, A. B., Kistler, L. M., Popecki, M. A., et al. 2008, SSRv, 136, 437
Gopalswamy, N. 2003, GeoRL, 30, 8013
Gopalswamy, N., Yashiro, S., Akiyama, S., et al. 2008, AnGeo, 26, 3033
Hickmann, K. S., Godinez, H. C., Henney, C. J., \& Arge, C. N. 2015, SoPh, 290, 1105
Howard, R. A., Moses, J. D., Vourlidas, A., et al. 2008, SSRv, 136, 67
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Jeong, H.-J., Moon, Y.-J., Park, E., Lee, H., \& Baek, J.-H. 2022, ApJS, 262, 50
Kahler, S. 1994, ApJ, 428, 837
Kahler, S. W. 2001, JGR, 106, 20947
Kahler, S. W., Arge, C. N., \& Smith, D. A. 2016, SoPh, 291, 1829
Kahler, S. W., \& Vourlidas, A. 2013, ApJ, 769, 143
Kallenrode, M.-B. 2003, JPhD, 29, 965
Laker, R., Horbury, T. S., Bale, S. D., et al. 2021, A\&A, 652, A105
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Macneil, A. R., Owens, M. J., Finley, A. J., \& Matt, S. P. 2022, MNRAS, 509, 2390
McComas, D. J., Bame, S. J., Barker, P., et al. 1998, SSRv, 86, 563
Mewaldt, R. A., Cohen, C. M. S., Cook, W. R., et al. 2008, SSRv, 136, 285
Moses, D., Clette, F., Delaboudinière, J.-P., et al. 1997, SoPh, 175, 571
Nolte, J. T., \& Roelof, E. C. 1973, SoPh, 23, 241
Park, J., Innes, D. E., Bucik, R., \& Moon, Y.-J. 2013, ApJ, 779, 184
Park, J., Innes, D. E., Bucik, R., Moon, Y.-J., \& Kahler, S. W. 2015, ApJ, 808, 3
Park, J., Moon, Y.-J., \& Gopalswamy, N. 2012, JGRA, 117, 8108
Reames, D. V. 1998, SSRv, 85, 327
Reames, D. V. 1999, SSRv, 90, 413
Reames, D. V. 2013, SSRv, 175, 53
Reames, D. V. 2020, SSRv, 216, 20
Riley, P., Linker, J. A., Mikić, Z., et al. 2006, ApJ, 653, 1510
Schatten, K. H., Wilcox, J. M., \& Ness, N. F. 1969, SoPh, 6, 442
Stansby, D., Yeates, A., \& Badman, S. T. 2020, JOSS, 5, 2732
Sun, X., Liu, Y., Hoeksema, J., Hayashi, K., \& Zhao, X. 2011, SoPh, 270, 9
Thompson, B. J., Plunkett, S. P., Gurman, J. B., et al. 1998, GeoRL, 25, 2465
Torsti, J., Valtonen, E., Lumme, M., et al. 1995, SoPh, 162, 505
Tylka, A. J., \& Lee, M. A. 2006, ApJ, 646, 1319
vonRosenvinge, T. T., Reames, D. V., Baker, R., et al. 2008, SSRv, 136, 391
Wang, Y.-M., \& Sheeley, N., Jr 1992, ApJ, 392, 310"
Kangwoo Yi et al 2020 - Forecast of Major Solar X-Ray Flare Flux Profiles Using Novel Deep Learning Models.pdf,"# Forecast of Major Solar X-Ray Flare Flux Profiles Using Novel Deep Learning Models 

Kangwoo Yi (1), Yong-Jae Moon (1), Gyungin Shin, and Daye Lim (1)<br>School of Space Research, Kyung Hee university, 1732, Deogyeongdae-ro, Giheung-gu, Yongin-si Gyunggi-do, 17104, Republic of Korea; moonyj@khu.ac.kr Received 2019 December 16; revised 2020 January 22; accepted 2020 January 24; published 2020 February 7


#### Abstract

In this Letter, we present the application of a couple of novel deep learning models to the forecast of major solar X-ray flare flux profiles. These models are based on a sequence-to-sequence framework using long short-term memory cell and an attention mechanism. For this, we use Geostationary Operational Environmental Satellite 10 X-ray flux data from 1998 August to 2006 April. Seven hundred sixty events are used for training and 85 for testing. The models forecast 30 minutes of X-ray flux profiles during the rise phase of the solar flare with a minute time cadence. We evaluate the models using the 10 -fold cross-validation and rms error (RMSE) based on flux profiles and RMSE based on its peak flux. For comparison we consider two simple deep learning models and four conventional regression models. Major results of this study are as follows. First, we successfully apply our deep learning models to the forecast of solar flare X-ray flux profiles, without any preprocessing to extract features from data. Second, our proposed models outperform the other models. Third, our models achieve better performance for forecasting X-ray flux profiles with low-peak fluxes than those with high-peak fluxes. Fourth, our models successfully predict flare duration with high correlations for both all cases and cases at peak times. Our study indicates that our deep learning models can be useful for forecasting time-series data in astronomy and space weather, even for impulsive events such as major flares.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar x-ray flares (1816); Time series analysis (1916)

## 1. Introduction

Solar flare is one of the most energetic activities of the Sun, releasing a huge amount of energy in a broad spectrum of emission and accelerating particles from the Sun to the interplanetary space. An intense X-ray flux from large flare events affects spacecraft anomalies, the safety of astronauts, and radio fade-outs (Huang et al. 2018). These hazards can result in enormous economic losses (Siscoe 2000) but it is very difficult to pre-estimate the characteristics of solar flares such as peak flux or duration because solar radiation travels at the speed of light. National Oceanic and Atmospheric Administration (NOAA) classifies solar flares into five classes (A, B, C, M , and X ) according to its peak soft X-ray flux $(0.1-0.8 \mathrm{~nm})$, and alerts radiation hazards when a major class flare ( M and X ) occurs.

There are several different types of flare forecasts. There have been many studies to predict solar flare occurrence probability within 24 hr : by statistical method (McIntosh 1990; Gallagher et al. 2002; Lee et al. 2012; Lim et al. 2019) and by machine learning methods (Colak \& Qahwaji 2009; Song et al. 2009). Many other studies forecast solar flare occurrence within 24 hr with ""Yes"" or ""No"": by statistical methods (Bloomfield et al. 2012), by machine learning methods (Al-Ghraibah et al. 2015; Bobra \& Couvidat 2015; Nishizuka et al. 2017; Park et al. 2017), and by deep learning (Huang et al. 2018; Nishizuka et al. 2018; Park et al. 2018; Liu et al. 2019). Several studies predict a maximum class of flare within 24 hr : by machine learning methods (Lee et al. 2007; Liu et al. 2017), and by deep learning (Hada-Muranushi et al. 2016). These studies are based on either McIntosh class or extracted magnetic properties of active regions. To the best of our knowledge, there is no study that forecasts the X-ray flux profiles of solar flares. Forecasting flare flux profiles, including the information of flare peak flux and duration, could help various space missions. For example, it would be helpful to
assume the duration of ionospheric electron enhancement which could affect spacecraft operations and HF communication.

In this Letter, for the first time we present a couple of solar flare forecast models that predict the X-ray flux profiles of major solar flares in real-time with one minute cadence. These models are based on novel deep learning methods such as the sequence-to-sequence (seq2seq; Sutskever et al. 2014) framework using the Long Short-Term Memory (LSTM; Hochreiter \& Schmidhuber 1997) and attention mechanism (Bahdanau et al. 2014), which is similar to Google Neural Machine Translation Network (Wu et al. 2016). We apply our models to the Geostationary Operational Environmental Satellite (GOES) X-ray flux profiles of 845 major flares. We compare our results with conventional regression models and different deep learning models. In contrast to the previous research based on active region analysis, we use historical solar flare soft X-ray flux data without any extracting features from data.

This paper is organized as follows. The data are described in Section 2. Our models are explained in Section 3.1. Results and analysis are given in Section 4. A brief conclusion and discussion are presented in Section 5.

## 2. Data

GOES observes solar X-ray fluxes in the two broadband channels of $1-8 \AA$ and $0.5-4 \AA$. NOAA identifies solar flares automatically using one-minute-averaged GOES 1-8 $\AA$ X-ray fluxes. A flare is defined using the following algorithm (Ryan et al. 2016; NOAA GOES X-ray flux ${ }^{1}$ ): (1) the begin time of an X-ray event is defined as the first minute, in a sequence of 4 minutes, of steep monotonic increase in $0.1-0.8 \mathrm{~nm}$ flux, (2) the flux at the end of the fourth minute is at least $40 \%$ greater than the flux in the first minute, (3) the X-ray event peak time is

[^0]
[^0]:    1 https://www.swpc.noaa.gov/products/goes-x-ray-flux
taken as the minute of the peak X-ray flux, and (4) the end time is the time when the flux level decays to a point halfway between the maximum flux and the background level. We apply the algorithm to GOES 10 one-minute X-ray flux data, from 1998 August to 2006 May, to recognize major solar flares. In order to select reliable solar flare events, we remove multiple-peak events and pre-flare phase events by simple automatic process and visual inspection. As a result, we select 845 major solar flare events.

Soft X-ray fluxes from 26 minutes before flare start time to 3 minutes after it are used for input and those of following 30 minutes are used for target. Our data set is constructed using the sliding window method. The window slides from 3 minutes after flare start time to its peak time. We randomly split the data set into training and test sets since flare X-ray profiles can be assumed to be mostly independent one another in that flares have a Poisson distribution in time (Rosner \& Vaiana 1978; Moon et al. 2001; Wheatland 2000; Wheatland \& Litvinenko 2002).

The data set is separated by each flare event to avoid that the data from the same flare event are used for training and test both. It is noted that when we divide the data set chronologically, the number of major flares in the test data set is too small. In order to reduce the effect of test data selection and random initial weights, we consider 10 -fold cross-validation. As a result, 6227-6381 data ( $90 \%$ of events; 69 X -class and 691 M -class) are used for training and 654-808 data ( $10 \%$ of events; 8 X-class and 77 M-class) for test. The difference in data number is due to different lengths of flare events in each cross-validation set.

## 3. Model

### 3.1. Methods

LSTM is one of the advanced recurrent neural networks (Hopfield 1982) that is good to learn time dependence in sequence. LSTM have been widely employed in various fields such as machine translation (Sutskever et al. 2014; Wu et al. 2016), speech recognition (Graves \& Schmidhuber 2005), and time-series prediction (Duan et al. 2018; Tan et al. 2018). LSTM layer has a cell state candidate and three gates, to transfer hidden state from prior time step to next time step. The time step means a sequential order of elements in time-series data. Details of the LSTM calculation are described below.

$$
\begin{gathered}
f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+B_{f}\right) \\
i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+B_{i}\right) \\
\bar{c}_{t}=\tanh \left(W_{c} \cdot\left[h_{t-1}, x_{t}\right]+B_{c}\right) \\
c_{t}=f_{t} \odot C_{t-1}+i_{t} \odot \bar{c}_{t} \\
o_{t}=\sigma\left(W_{o} \cdot\left[h_{t-1}, x_{t}\right]+B_{o}\right) \\
h_{t}=o_{t} \odot \tanh \left(c_{t}\right)
\end{gathered}
$$

where $f_{t}$ represents the forget gate, $i_{t}$ represents the input gate, $\bar{c}_{t}$ represents the cell state candidate, $c_{t}$ represents the cell state, $o_{t}$ represents the output gate, and $h_{t}$ represents the hidden state at time step $t ; X$ is input data; $W$ terms represent weight metrics and $B$ terms are bias, which are trainable; $\sigma$ is a logistic sigmoid function and tanh is a hyperbolic tangent function. Here denotes dot product and $\odot$ denotes element-wise multiplication. The forget gate, input gate, and output gate calculate weights that are applied to past information, new information, and output corresponding to forecasting result, respectively.

The cell state candidate is modified information considering how much input information should be reflected to the cell state. The hidden state is a result of the LSTM layer.

Seq2seq framework relies two LSTM models: one is an encoder for extracting information from input data and the other is a decoder for making a forecast using the information from the encoder. The decoder repeats a process to generate a prediction and then to use the predicted result for the next prediction. This mechanism has been used for generating timeseries data (Wu et al. 2016; Naul et al. 2018).

In addition, we employ an attention mechanism that allows the model to learn and focus on important time steps of input data. The attention computes a context vector $c_{i}$ corresponding to the weighted sum of source state $h$,

$$
c_{i}=\sum_{j=1}^{T_{s}} W_{i j} h_{j}
$$

where $T_{s}$ represents length of input and $W$ represents weight. Here $i$ denotes time step of output and $j$ denotes time step of input. The weight $W$ is computed by

$$
\begin{gathered}
W_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{s}} \exp \left(e_{i k}\right)} \\
e_{i j}=a\left(s_{i-1}, h_{j}\right)
\end{gathered}
$$

where $s$ is a hidden state. Here $a$ denotes an alignment model that calculates a score reflecting how well output at the position $i$ and input at the position $j$ match each other. We adopt the additive attention model (Bahdanau et al. 2014)

$$
a\left(s_{i-1}, h_{j}\right)=v^{T} \tanh \left(W_{i} s_{i-1}+W_{h} h_{j}\right)
$$

where $v$ and $W$ are learnable parameters.

### 3.2. Architecture

Figure 1 shows an overall architecture of our first proposed model. This model takes two LSTM layers and one fully connected layer for the encoder and decoder. We adopt a fully connected layer with the ReLU (Nair \& Hinton 2010) activation function for the encoder and a fully connected layer without an activation function for the decoder. The second model is obtained by removing the attention layer from Figure 1. The mean-squared loss function and Adam optimizer (Kingma \& Ba 2014) with learning rate 0.001 are used for our models.

## 4. Results

To evaluate models, we consider three types of rms errors (RMSEs), one for all forecasting results, another for peak flux, and the other for different forecast start time:

$$
\begin{aligned}
& R M S E_{\text {all }}=\sqrt{\frac{\sum_{e=1}^{E} \sum_{i=3}^{p} \sum_{j=1}^{30} \log \left(y_{e j}^{j}\right)-\log \left(f_{e i}^{j}\right)^{2}}{N_{\text {all }}}}, \\
& R M S E_{\text {peakflux }} \\
& =\sqrt{\frac{\sum_{e=1}^{E} \sum_{i=3}^{p}\left(\max \left(\log \left(y_{e i}^{j}\right)_{j=1}^{30}-\max \left(\log \left(f_{e i}^{j}\right)_{j=1}^{30}\right)^{2}\right.\right.}{N_{\text {peakflux }}}}
\end{aligned}
$$
![img-0.jpeg](img-0.jpeg)

Figure 1. Architecture of our first proposed model. Two LSTM layers and one fully connected layer for the encoder and decoder. The encoder takes 30 minutes of X-ray flux (y) and produces the embedded information constructed by passing the output of LSTM layers into a fully connected layer. The decoder repeats a similar process with the encoder 30 times. In every step, the decoder takes a result of the attention layer and new information to predict X-ray flux (p) of the next 1 minute which is used for next step prediction corresponding to new information. Results of LSTM layer in decoder are sent to a fully connected layer without an activation function. The second proposed model has the same structure without attention.

Table 1
Results of the Models

|  Model | RMSE $_{\text {all }}$ | RMSE $_{\text {peakflux }}$ | RMSE $_{\text {e }}$ |  |  |  | Hyperparameters  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|   |  |  | $t=3$ | $t=4$ | $t=5$ | $t=6$ |   |
|  ARIMA | $0.72 \pm 0.11$ | $0.63 \pm 0.19$ | $0.74 \pm 0.09$ | $1.20 \pm 0.35$ | $0.86 \pm 0.14$ | $0.74 \pm 0.14$ | p: 1-5, d: 1, q: 1-5, Input data length: 1800  |
|  KNNR | $0.39 \pm 0.02$ | $0.31 \pm 0.03$ | $0.45 \pm 0.02$ | $0.44 \pm 0.02$ | $0.44 \pm 0.02$ | $0.43 \pm 0.02$ | K: 58, Metric: Euclidean  |
|  RFR | $0.38 \pm 0.02$ | $0.30 \pm 0.03$ | $0.45 \pm 0.02$ | $0.44 \pm 0.02$ | $0.43 \pm 0.02$ | $0.42 \pm 0.02$ | Trees: 100, M:1.0 , Min sample split: 0.025  |
|  SVMR | $0.37 \pm 0.02$ | $0.29 \pm 0.03$ | $0.46 \pm 0.03$ | $0.45 \pm 0.03$ | $0.42 \pm 0.02$ | $0.39 \pm 0.03$ | Kernel: RBF, C: 1, epsilon: 0.005  |
|  MLP | $0.45 \pm 0.02$ | $0.45 \pm 0.05$ | $0.52 \pm 0.04$ | $0.50 \pm 0.04$ | $0.48 \pm 0.03$ | $0.46 \pm 0.02$ | Fully connected layers: 18 (30-280, 280-280, …, 280-30)  |
|  Simple-LSTM | $0.50 \pm 0.05$ | $0.40 \pm 0.05$ | $0.53 \pm 0.02$ | $0.54 \pm 0.02$ | $0.53 \pm 0.02$ | $0.52 \pm 0.03$ | Hidden units: 48, Fully connected layers: 2 (96-96, 96-1)  |
|  Seq2seq | $0.33 \pm 0.02$ | $0.26 \pm 0.03$ | $0.42 \pm 0.02$ | $0.41 \pm 0.02$ | $0.38 \pm 0.02$ | $0.36 \pm 0.02$ | Hidden units: 192, Embedding size: 8, Fully connected layers: 1(192-1)  |
|  Seq2seq+attention | $0.32 \pm 0.02$ | $0.26 \pm 0.02$ | $0.41 \pm 0.02$ | $0.40 \pm 0.02$ | $0.37 \pm 0.02$ | $0.35 \pm 0.02$ | Hidden units: 256, Embedding size: 1, Fully connected layers: 1(256-1)  |

Note. RMSEs are expressed with standard deviations of 10 -fold cross-validation trials. Fully connected layer is described as input size-output size.

$$ R M S E_{t}=\sqrt{\frac{\sum_{e=1}^{E} \sum_{j=1}^{30}\left(\log \left(y_{e}^{j}\right)\right)_{i=t}-\log \left(f_{e j}^{j}\right)_{i=t}^{i-t}}{N_{t}}} $$

where $y$ represents an observed flux and $f$ represents a forecasted flux. $N_{\text {all }}, N_{\text {peakflux }}$, and $N_{t}$ are the numbers of all cases, at peak times and at times $t$ after flare start, respectively. Here $E, i, j$, and $p$ are the number of flares, forecast start time, each forecasting time, and peak time, respectively. This process was executed 10 times for each cross-validation test data set and the average of the results are shown.

For comparison with our models, we make two simple deep learning models and four conventional regression models. The first deep learning model is based on Multi-Layer-Perceptron (MLP), constructed of several fully connected layers. This model analyzes data as independent features, not time-series features. The second deep learning model is based on LSTM, constructed of two LSTM layers and two fully connected layer. This model can analyze data as time-series features. These models are trained using the same loss function and optimizer with our models. Four conventional models are based on the Auto-Regressive Integrated Moving Average (ARIMA; Luceño \& Peña 2008), the K-Nearest Neighbor Regression (KNNR; Altman 1992), the Support Vector Machine Regression (SVMR; Cortes \& Vapnik 1995), and the Random Forest Regression (RFR; Breiman 2001). For the ARIMA model, we
![img-1.jpeg](img-1.jpeg)

Figure 2. Results of the seq2seq+attention model. Panels (a), (b), (c), (d), (e), and (f) are the X5, X4, X1, M4, M2, and M1 flare forecasts at 7, 7, 7, 6, 3, and 3 minutes after flare start time, respectively. 0–29 minutes are input data and 30–59 minutes are output data together with observations. The black vertical dashed lines denote forecast start times.

use the AUTO-ARIMA process (Hyndman & Khandakar 2008) for all input data to find the best results of the ARIMA model.

For comparing the models fairly, we test many hyperparameters for conventional regression models and deep learning models (48, 96, 128, 256, and 512 hidden units and nodes; 1, 8, 16 embedding and no embedding; and 2–18 fully connected layers with a residual connection; He et al. 2015), and carry out the early stopping (Morgan & Bourlard 1990) process during the deep learning model training to evade the overfitting problem and find the best result of each deep learning model.

Table 1 shows the result of the models. Considering the standard deviation of 10-fold cross-validation, our proposed models (seq2seq, seq2seq+attention) outperform the others in all metrics. The differences between our proposed models are not significant. The seq2seq model could be better than the seq2seq+attention model depending on the data set. For more discussion, we select the seq2seq+attention model for forecast analysis because it produces the best results.

For the seq2seq+attention model, RMSEall and RMSEpeakflux are 0.32 and 0.26, which are much better than those from the four regression models and two simple deep learning models. The RMSEr, which depends on forecast start time (3–6 minutes), is also better for proposed models than those for other models. Our results demonstrate that adopting both seq2seq and attention are effective for forecasting the solar X-ray profiles. In all eight models, the results of the later forecasts (RMSEr with higher *t*) are better than the ones of the earlier forecasts (RMSEr with lower *t*) because of rapid change of the X-ray flux profiles.

In addition, we divide forecasts of the seq2seq+attention model into three groups according to flare class (M1–M4.9, M5–M9.9, and X1-). As a result, RMSEall and RMSEpeakflux are 0.27 and 0.15 for the M1–M4.9 flare, 0.36 and 0.31 for the M5–M9.9 flare, and 0.51 and 0.53 for the X1- flare. This analysis indicates that strong solar flares are harder to predict than weak solar flares.

Figure 2 is the forecast results of the seq2seq+attention model. The model well forecasts flare peak flux, peak time, and overall profiles.

Figure 3 is the forecasts of the seq2seq+attention model from 5 minutes after flare start to peak time for the X5-class flare shown in Figure 2(a). As shown in Figure 3(a), its peak flux is noticeably underestimated. However, when the GOES observation is at the M class, our model forecasts that the flare
![img-2.jpeg](img-2.jpeg)

Figure 3. Prediction of the seq2seq+attention model for an X5-class flare on 2003 November 3. Panels (a)–(h) show the forecasts at 5–12 minutes after the flare start time, in chronological order. 0–29 minutes are the input and 30–59 minutes are the output together with observations. The black vertical dashed lines denote forecast start times.

We also see that the X5-class flare model predicts a peak flux similar to observation. In subsequent forecasts (Figures 3(c)–(h)), the model well predicts its peak flux and X-ray flux profiles in the decay phase.

Figure 4 shows a relationship between predictions and observations of flare duration. To evaluate the flare duration prediction of the seq2seq+attention model, we calculate the correlation coefficient and RMSE in the case that both flare end times for observation and prediction are within 30 minutes. Because of the forecasting period, the data points that have no observed and/or no predicted flare end time within 30 minutes cannot be displayed on the scatter plot. The correlation coefficient and RMSE are 0.78 and 6.5 minutes for all forecasts, while they are 0.9 and 4.8 minutes for the peak time forecast. These results show that our model well forecasts flare duration for both cases. It is also noted that the prediction of flare duration at the peak time is very excellent in view of correlation and RMSE.
![img-3.jpeg](img-3.jpeg)

Figure 4. Predicted flare duration vs. observed flare duration for (a) all forecasts and (b) forecasts at peak times by the seq2seq+attention model. Size of the circles denotes the number of predictions.

## 5. Conclusion and Discussion

In this Letter, we have presented new forecast models to predict X-ray flux profiles of solar major flares. These novel deep learning models are based on seq2seq with/without attention. We take GOES 10 X-ray fluxes of 845 major flare events from 1998 August to 2006 May. We randomly separate our data sets into 90% for training and 10% for test. We train and evaluate our models using 10-fold cross-validation, and compare our results with other deep learning models and conventional regression models. The main results from this study are summarized as follows. First, we successfully apply our models to the forecast of solar flare X-ray flux profiles, without any preprocessing to extract features from the data. Second, in view of RMSE, our models outperform other deep learning models and conventional regression models with 0.32 and 0.33 for RMSEall and 0.26 for RMSEpeakflux. Third, RMSEall and RMSEpeakflux of the seq2seq+attention model are 0.27 and 0.15 for the M1–M4.9 flare, 0.36 and 0.31 for the M5–M9.9 flare, and 0.51 and 0.53 for the X1- flare, respectively. Fourth, in flare duration prediction, the correlation coefficient and RMSE values from the seq2seq+attention model are 0.78 and 6.5 minutes for all forecasts, while they are 0.9 and 4.8 minutes for the peak time forecast.

We build the data set based on the assumption that the flares have a Poisson distribution in time (Rosner & Vaiana 1978; Moon et al. 2001; Wheatland 2000; Wheatland & Litvinenko 2002). On the other hand, there are different opinions such as a power-law distribution (Boffetta et al. 1999) and a Lévy distribution (Lepret et al. 2001). Data set construction could be affected by flare distribution in time.

Our model produces better predictions for weaker flares than stronger flares. This may be mainly caused by two reasons. The first reason is the class imbalance problem: the number of stronger flares is much less than that of weaker flares. In this case, the models are trained to focus on better predicting weaker flares than stronger flares. The second reason is that stronger flares have more dynamic X-ray profiles than weaker flares. This is already noted in Section 4 that RMSEpeakflux rapidly increases for stronger flares.

Recently, several deep learning methods have been applied to time-series data in astronomy and space weather. Muthukrishna et al. (2019) classified the transient class using a gated recurrent unit (Chung et al. 2014). By using the seq2seq model, Naul et al. (2018) reconstructed the light curve of variable stars and classified the class of variable stars better than a conventional algorithm, and Shen et al. (2017) generated denoised gravitational-wave signals.

In this work, we have demonstrated that solar X-ray profiles are well predicted by novel deep learning models that are based on seq2seq with LSTM. It is impressive that our models can well predict rapidly increasing and then decreasing patterns such as solar flares. We expect that our method can be applied to many time-series data in astronomy and space weather, even in other scientific fields. There seems to be several candidates for application of time-series data: variable stars (Chung et al. 2014; Naul et al. 2018), gravitational waves (Shen et al. 2017), and supernova evolution (Suwa 2017). On the other hand, our models can be useful for ionosphere behavior prediction such as D-region enhancement (McRae & Thomson 2004) or an increase in total electron content (Liu et al. 2004) since it can predict X-ray flux profiles in advance.

We appreciate the referee's constructive comments. This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131, NRF-2019R1A2C1002634), the Korea Astronomy and Space Science Institute (KASI) under the R&D program ""Study on the Determination of Coronal Physical
Quantities using Solar Multi-wavelength Images"" (project No. 2019-1-850-02) supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). The GOES X-ray data used here may be accessed through https://satdat.ngdc.noaa.gov/sem/ goes/data/. We thank contributors to Pytorch, Numpy, Scikitlearn, Pmdarima, and Matplotlib open-source packages.

## ORCID iDs

Kangwoo Yi (1) https://orcid.org/0000-0003-4342-9483
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Daye Lim (1) https://orcid.org/0000-0001-9914-9080

## References

Al-Ghraibah, A., Boucheron, L. E., \& McAteer, R. T. J. 2015, A\&A, 579, A64 Altman, N. S. 1992, The American Statistician, 46, 175
Bahdanau, D., Cho, K., \& Bengio, Y. 2014, arXiv:1409.0473
Bloomfield, D. S., Higgins, P. A., McAteer, R. T. J., \& Gallagher, P. T. 2012, ApJL, 747, L41
Bobra, M. G., \& Couvidat, S. 2015, ApJ, 798, 135
Boffetta, G., Carbone, V., Giuliani, P., Veltri, P., \& Vulpiani, A. 1999, PhRvL, 83, 4662
Breiman, L. 2001, Machine Learning, 45, 5
Chung, J., Gulcehre, C., Cho, K., \& Bengio, Y. 2014, arXiv:1412.3555
Colak, T., \& Qahwaji, R. 2009, SpWea, 7, S06001
Cortes, C., \& Vapnik, V. 1995, Machine Learning, 20, 273
Duan, Z., Yang, Y., Zhang, K., Ni, Y., \& Bajgain, S. 2018, IEEE Access, 6, 31820
Gallagher, P. T., Moon, Y. J., \& Wang, H. 2002, SoPh, 209, 171
Graves, A., \& Schmidhuber, J. 2005, NN, 18, 602
Hada-Muranushi, Y., Muranushi, T., Asai, A., et al. 2016, arXiv:1606.01587
He, K., Zhang, X., Ren, S., \& Sun, J. 2015, arXiv:1512.03385
Hochreiter, S., \& Schmidhuber, J. 1997, Neural Computation, 9, 1735
Hopfield, J. J. 1982, PNAS, 79, 2554
Huang, X., Wang, H., Xu, L., et al. 2018, ApJ, 856, 7

Hyndman, R., \& Khandakar, Y. 2008, Journal of Statistical Software, 27, 1
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Lee, J. Y., Moon, Y. J., Kim, K. S., Park, Y. D., \& Fletcher, A. r. 2007, JKAS, 40, 99
Lee, K., Moon, Y. J., Lee, J.-Y., Lee, K.-S., \& Na, H. 2012, SoPh, 281, 639
Lepret, F., Carbone, V., \& Veltri, P. 2001, ApJL, 555, L133
Lim, D., Moon, Y.-J., Park, J., et al. 2019, JKAS, 52, 133
Liu, C., Deng, N., Wang, J. T. L., \& Wang, H. 2017, ApJ, 843, 104
Liu, H., Liu, C., Wang, J. T. L., \& Wang, H. 2019, ApJ, 877, 121
Liu, J. Y., Lin, C. H., Tsai, H. F., \& Liou, Y. A. 2004, JGRA, 109, A01307
Luceño, A., \& Peña, D. 2008, in Encyclopedia of Statistics in Quality and Reliability, ed. F. Ruggieri, R. S. Kennett, \& F. W. Faltin (New York: Wiley),
McIntosh, P. S. 1990, SoPh, 125, 251
McRae, W. M., \& Thomson, N. R. 2004, JASTP, 66, 77
Moon, Y. J., Choe, G. S., Yun, H. S., \& Park, Y. D. 2001, JGR, 106, 29951
Morgan, N., \& Bourlard, H. 1990, in Advances in Neural Information Processing Systems, ed. D. S. Touretzky, Vol. 2 (Burlington, MA: MorganKaufmann), 630
Muthukrishna, D., Narayan, G., Mandel, K. S., Biswas, R., \& Hložek, R. 2019, PASP, 131, 118002
Nair, V., \& Hinton, G. E. 2010, in Proc. 27th Int. Conf. Machine Learning, ICML'10 (Madison, WI: Omnipress), 807, http://dl.acm.org/citation.cfm? id $=3104322.3104425$
Naul, B., Bloom, J. S., Pérez, F., \& van der Walt, S. 2018, NatAs, 2, 151
Nishizuka, N., Sugiura, K., Kubo, Y., et al. 2017, ApJ, 835, 156
Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., \& Ishii, M. 2018, ApJ, 858, 113
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Park, J., Moon, Y.-J., Choi, S., et al. 2017, SpWea, 15, 704
Rosner, R., \& Vaiana, G. S. 1978, ApJ, 222, 1104
Ryan, D. F., Dominique, M., Seaton, D., Stegen, K., \& White, A. 2016, A\&A, 592, A133
Shen, H., George, D., Huerta, E. A., \& Zhao, Z. 2017, arXiv:1711.09919
Siscoe, G. 2000, JASTP, 62, 1223
Song, H., Tan, C., Jing, J., et al. 2009, SoPh, 254, 101
Sutskever, I., Vinyals, O., \& Le, Q. V. 2014, arXiv:1409.3215
Suwa, Y. 2017, MNRAS, 474, 2612
Tan, Y., Hu, Q., Wang, Z., \& Zhong, Q. 2018, SpWea, 16, 406
Wheatland, M. S. 2000, ApJL, 536, L109
Wheatland, M. S., \& Litvinenko, Y. E. 2002, SoPh, 211, 255
Wu, Y., Schuster, M., Chen, Z., et al. 2016, arXiv:1609.08144"
Seungwoo Ahn et al 2025 - Comparison of Empirical and Deep Learning Models for Solar Wind Speed Prediction.pdf,"# Comparison of Empirical and Deep Learning Models for Solar Wind Speed Prediction 

Seungwoo Ahn ${ }^{1}$ (D), Jihyeon Son ${ }^{2}$ (D), Yong-Jae Moon ${ }^{1,3}$ (D), and Hyun-Jin Jeong ${ }^{4,1}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Center for Heliophysics Research, Division of Fundamental Astronomy \& Space Science, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{3}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{4}$ Centre for mathematical Plasma Astrophysics, Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium<br>Received 2025 April 2; revised 2025 May 28; accepted 2025 June 2; published 2025 July 8


#### Abstract

In this study, we compare representative empirical models with a deep learning model for predicting solar wind speed at 1 au . The empirical models are the Wang-Sheeley-Arge-ENLIL model, which combines empirical methods with a magnetohydrodynamic model, and the empirical solar wind forecast model, which uses the relationship between the fractional coronal hole area and solar wind speed. Our deep learning model predicts solar wind speed over 3 days ahead using extreme-ultraviolet images and up to 5 days of solar wind speed before the prediction date. We evaluate the models over the test period (October-December in each year from 2012 to 2020) in view of solar activity phases and the entire period. To validate the model's performance, we use two evaluation methods: a statistical approach and an event-based approach. For statistical verification during the entire period, our model outperforms the other empirical models, with a much lower mean absolute error of $51.4 \mathrm{~km} \mathrm{~s}^{-1}$ and rms error of $68.6 \mathrm{~km} \mathrm{~s}^{-1}$, along with a much higher correlation coefficient of 0.69 . For the event-based verification for high-speed solar wind streams, our model has superior performance in most of the six metrics evaluated within a $\pm 1$ day time window. In particular, it achieves a high success ratio of 0.82 , emphasizing the model's stable performance and ability to minimize false alarms. These results show that our deep learning model has strong potential for practical application as a reliable tool for fast solar wind forecasting with its high accuracy and stability.


Unified Astronomy Thesaurus concepts: Solar wind (1534); Neural networks (1933); Space weather (2037)

## 1. Introduction

Space weather is a field that studies the phenomena in which energy and particles emitted from the Sun interact with the Earth, affecting modern technological infrastructure such as satellites, communication systems, and power grids. Plasma and magnetic fields emitted by the Sun interact with Earth's magnetosphere, triggering phenomena such as substorms and geomagnetic storms. One of the main causes of these phenomena is the solar wind, a plasma flow emitted from the Sun. It is classified into fast solar wind and slow solar wind on the basis of its speed. The fast solar wind is well known to originate from coronal holes ( CHs ) with open magnetic fields (D. McComas et al. 2000) and to travel at speeds exceeding $650 \mathrm{~km} \mathrm{~s}^{-1}$. In contrast, the origin of the slow solar wind, which travels at speeds below $400 \mathrm{~km} \mathrm{~s}^{-1}$ (S. Bravo \& G. A. Stewart 1997), remains an outstanding question (L. Abbo et al. 2016). The interaction caused by the speed difference between these types of solar wind forms a region known as the corotating interaction region (CIR). A CIR is where plasma density increases, magnetic fields are compressed, and plasma is accelerated, affecting the Earth's magnetosphere. As a result, its structure is altered, which affects space-based technologies such as satellites. Therefore, accurate prediction of the solar wind's arrival time at Earth remains a critical challenge in space weather forecasting.

To address this challenge, empirical models have been proposed to predict solar wind speed using several approaches.

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.

These models rely on empirical formulae derived from observational data. Furthermore, comparative studies have been conducted to evaluate the performance of various solar wind speed forecasting models, thus identifying the most effective approach (P. Riley et al. 2015; M. A. Reiss et al. 2016). Representative examples include the Wang-SheeleyArge (WSA)-ENLIL model (Y.-M. Wang \& N. Sheeley 1990a, 1990b, 1992; C. Arge \& V. Pizzo 2000), operated by the National Oceanic and Atmospheric Administration/ Space Weather Prediction Center, and the emperical solar wind forecast (ESWF) model (D. Milošić et al. 2023), operated by the European Space Agency (ESA)/Heliospheric Weather Expert Service Centre (H-ESC), both of which have outperformed other models in the above comparative studies.

Recently, various deep learning (DL)-based models have been developed in the field of space weather. For example, models that use the convolutional layer and Long-Short-Term Memory (LSTM; V. Upendran et al. 2020), the twodimensional attention mechanism (Y. Sun et al. 2021), and the convolution neural network (H. Raju \& S. Das 2021) offer new possibilities to predict solar wind speed. In addition, models using attention mechanisms (E. J. Brown et al. 2022) have been introduced, further expanding the range of techniques used in space weather forecasting. In particular, J. Son et al. (2023) have developed a DL model for solar wind speed prediction. This model combines convolution layers and dense layers in its architecture using extreme-ultraviolet (EUV) solar images and solar wind speed data from the previous 5 days to predict solar wind speed for up to 3 days ahead.

In this study, we make an extensive comparison of our DL model (J. Son et al. 2023) for solar wind speed forecasting
with the following focuses. First, we consider two representative empirical models (WSA-ENLIL and ESWF 3.2) for comparison. Second, our analysis covers a broad range of Carrington rotation (CR) periods: October to December in each year from 2012 to 2020. Third, we perform statistical verification as well as event-based verification to evaluate the model's predictive performance for high-speed solar wind streams (HSS). This paper is organized as follows. In Section 2, we introduce the empirical models and the DL model used for comparison and describe the data preprocessing. In Section 3, we detail two verification methods. In Section 4, we present the verification results. Finally, in Section 5, we summarize the main findings of this study.

## 2. Forecasting Models

To evaluate the performance of the DL model for solar wind speed prediction, we compare it with two empirical models: the WSA-ENLIL and ESWF models. The WSA-ENLIL model is provided by NASA/Community Coordinated Modeling Center (CCMC), and the ESWF model is provided by the ESA/H-ESC. To compare the performance of the DL model with those of empirical models, empirical model results are processed to match the DL model's 6 hr cadence ( $00: 00,06: 00$, 12:00, and 18:00). The comparison is conducted over the test period of October-December for the years 2012-2020. In addition, data corresponding to interplanetary coronal mass ejection (ICME) periods are excluded from the analysis. These periods, as listed by I. G. Richardson \& H. V. Cane (2010), represent the intervals from when a shock reaches Earth to the expected end of the corresponding ICME. Information about ICME periods is taken from the Richardson and Cane ICME catalog. ${ }^{5}$ In this section, we provide an overview of both the empirical models and the DL model.

### 2.1. Wang-Sheeley-Arge (WSA)-ENLIL Model

The WSA-ENLIL model is a well-known solar wind speed prediction model that combines the empirical WSA model and the physics-based ENLIL model into a hybrid structure. The WSA model estimates solar wind speed at 30 solar radii $\left(R_{\odot}\right)$ using the expansion factor and the CH boundary distance of the solar coronal magnetic field. To derive these parameters, we combine the potential-field source surface model (M. D. Altschuler \& G. Newkirk 1969; K. H. Schatten et al. 1969), which extrapolates the coronal magnetic field up to $2.5 R_{\odot}$ from Global Oscillation Network Group (GONG) magnetograms (National Solar Observatory, GONG; http:// gong.nso.edu/), with the Schatten current sheet model (K. H. Schatten 1971), which extends them up to $30 R_{\odot}$. These estimated solar wind speeds are extended into the heliosphere using the ENLIL model, which simulates their propagation through interplanetary space to predict solar wind speed. The ENLIL model is a 3D time-dependent heliospheric model based on ideal magnetohydrodynamic equations, designed to simulate the physical properties of the solar wind. It is often combined with WSA or magnetohydrodynamic algorithm outside a sphere (MAS) coronal models (e.g., D. Odstrcil et al. 2002, 2004; P. Riley et al. 2002) to simulate the propagation and structural evolution of the solar wind. Applying this hybrid approach, the WSA-ENLIL model is a

[^0]large-scale physics-based heliospheric prediction system that provides $1-4$ days advance warnings of solar wind structures and Earth-directed coronal mass ejections (CMEs) that cause geomagnetic storms.

In this study, the results of the WSA-ENLIL model are obtained from CORHEL-MAS_WSA_ENLIL ${ }^{6}$ of NASA/ CCMC. We use GONG synoptic maps as input data because, as shown in a previous study (L. K. Jian et al. 2015), they outperform other data sets. For the CR periods corresponding to the verification period, we select the WSA model as the coronal model and the ENLIL model as the heliospheric model. The default configuration is used for this study, which is provided by CORHEL-MAS_WSA_ENLIL. The WSAENLIL model provides solar wind speed data at Earth from the CR start time to 5 days beyond the CR end time. However, the data for the official start time do not account for the delay in solar wind propagation, which is approximately 3-5 days from the Sun to Earth. Therefore, to account for this delay, we adjust the WSA-ENLIL data to begin 3 days after the CR start time and extend 3 days beyond the CR end time (L. K. Jian et al. 2015). This 3 days delay setting enables an accurate evaluation of the prediction performance specifically for HSS events. For verification, we use model output at a 6 hr cadence by selecting the values corresponding to the times closest to $00: 00,06: 00,12: 00$, and $18: 00$.

### 2.2. Empirical Solar Wind Forecast Model

The ESWF model predicts the solar wind speed at 1 au by calculating the fractional CH areas within a meridional slice (S. Robbins et al. 2006; B. Vršnak et al. 2007) from Solar Dynamics Observatory/Atmospheric Imaging Assembly (SDO/AIA) EUV images at $193 \AA$, based on the empirical relationship between these areas and the measured solar wind speed (D. Milošić et al. 2023). Specifically, it uses an empirical formula to perform predictions while dynamically considering changes in CH areas (T. Rotter et al. 2015). Therefore, the propagation delay depends on the change in the fractional CH area. The ESWF model has been updated to ESWF 3.2 (D. Milošić et al. 2023), and this latest version demonstrates improvements in various aspects compared to previous versions. In the ESWF 3.2 model, the pileup algorithm is adopted to better capture the asymmetric profiles of HSS events. This approach considers interactions and the effects of compression and rarefaction occurring during the propagation of the solar wind between the Sun and Earth, effectively considering the asymmetry in velocity profiles. Additionally, the model uses dynamic thresholding to optimally extract CH areas. Specifically, instead of using a fixed threshold intensity as a constant percentage of the median intensity of the solar disk, a dynamic thresholding method based on statistical analysis is introduced. Moreover, to consider the relationship between CH areas and the maximum solar wind speed observed at Earth, the model divides the meridional slice into 12 latitudinal sectors and assigns colatitude-based weights to the CH area in each sector when calculating the solar wind speed. These improvements enable a more accurate prediction of the speed variations associated with HSS events.

[^1]
[^0]:    5 https://izw1.caltech.edu/ACE/ASC/DATA/level3/icmetable2.htm

[^1]:    6 https://ccmc.gsfc.nasa.gov/models/CORHEL-MAS_WSA_ENLIL-5.0
In this study, the results of the ESWF 3.2 model are obtained from ESA H-ESC. This model provides solar wind speed data at 1 au for the period 2012-2020.

### 2.3. Deep Learning Model

In this study, to evaluate the performance of the solar wind prediction, we use a DL model developed by J. Son et al. (2023). This model predicts solar wind speeds up to 3 days ahead with a 6 hr cadence. The input data consist of $211 \AA$ and $193 \AA$ images from the SDO/AIA along with OMNI solar wind speed data measured at 1 au . Additionally, the input data include up to 5 days, which corresponds to the longest traveling time from the Sun to Earth, of data prior to the prediction time. The architecture of the DL model is designed to process and concatenate two types of data. EUV images are processed through a convolutional block, named Inception, followed by an LSTM layer that captures long-term dependences. Meanwhile, solar wind speed data are processed through dense layers to make them suitable for prediction. These two processed data types are then concatenated, and additional dense layers are applied to predict solar wind speeds up to 3 days ahead.

The data set for the DL model spans from 2010 May to 2020 December and is divided into three subsets: a training set (January-August), a validation set (September), and a test set (October-December), taking into account the solar cycle. The comparisons with empirical models are conducted using the test set for the period 2012-2020.

## 3. Methods

To validate the performance of the DL model for forecasting solar wind speed, two verification methods are applied. First, statistical verification evaluates the model's accuracy in predicting solar wind speed by analyzing prediction errors and provides an overview of its predictive performance. Second, event-based verification evaluates the model's ability to detect HSS events by evaluating its predictions of the arrival time and intensity of fast solar winds. These two verification methods complement each other: statistical verification is useful for evaluating the model's overall prediction accuracy, and event-based verification is essential for ensuring the model's reliability in predicting critical solar wind events. This comprehensive validation process identifies the strengths and weaknesses of each model, providing a basis for improving the accuracy of solar wind forecasting.

### 3.1. Statistical Verification

For statistical verification, the models' performance is evaluated using OMNI data, which indicate solar wind speed at 1 au . The performance of the models is assessed using metrics including mean absolute error (MAE), rms error (RMSE), and the correlation coefficient (CC). These metrics are defined as follows:

$$
\begin{gathered}
\mathrm{MAE}=\frac{1}{N} \sum_{i=1}^{N}\left|f_{i}-y_{i}\right| \\
\mathrm{RMSE}=\sqrt{\frac{\sum_{i=1}^{N}\left(f_{i}-y_{i}\right)^{2}}{N}} \\
\mathrm{CC}=\frac{\sum_{i=1}^{N}\left(f_{i}-\bar{f}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i=1}^{N}\left(f_{i}-\bar{f}\right)^{2}} \sqrt{\sum_{i=1}^{N}\left(y_{i}-\bar{y}\right)^{2}}}
\end{gathered}
$$

Here, $f_{i}, y_{i}, \bar{f}, \bar{y}$, and $N$ denote the $i$ th predicted value, the $i$ th target value, their average values, and the total number of data, respectively. MAE is an intuitive metric that indicates how close the predicted values are to the target values. Since it considers all errors equally, it is less affected by outliers and provides a simple way to evaluate the accuracy of the model. RMSE is more sensitive to large errors, which makes it suitable for assessing how well a model handles significant discrepancies. It emphasizes the overall magnitude of the prediction errors. When used together with MAE, it offers a more comprehensive analysis of the model's performance. CC measures the relationship between the predicted and target values, evaluating how well the model captures the patterns of data variation. A CC value close to 1 indicates that the predicted values closely follow the patterns of the target values, demonstrating the model's ability to accurately capture the characteristics of the data. These three metrics offer diverse perspectives on model performance, enabling a comprehensive assessment of its strengths and weaknesses.

To analyze how the model performance varies with the solar activity cycle, statistical verification is conducted not only for the entire period but also for four distinct phases of the solar activity cycle. As solar activity levels vary across these phases, the model performance is also expected to fluctuate accordingly. Solar activity phases, classified according to the October-December period, include the ascending phase (2012), the maximum phase (2013-2014), the descending phase (2015-2018), and the minimum phase (2019-2020).

We use a Taylor diagram (K. E. Taylor 2001) to visualize the performance of various prediction models for different solar activity phases (P. Riley et al. 2013). This diagram allows us to compare different models and track changes in their performance over a period. In the Taylor diagram, the radial distance from the origin represents the standard deviation, the azimuth (angle from the horizontal axis) represents the CC, and the Euclidean distance from the reference point (OMNI data, placed on the $x$-axis at $\mathrm{CC}=1$ ) represents the RMSE. Consequently, when a model accurately predicts the OMNI data, its standard deviation is similar to that of the reference, its CC is high, and its RMSE is low, positioning the model's results close to the reference star on the diagram.

### 3.2. Event-based Verification

Event-based verification evaluates how accurately solar wind speed prediction models can forecast actual HSS events identified from OMNI data. For this purpose, we use the algorithm for automatic detection (L. K. Jian et al. 2015) based on the methods proposed by M. J. Owens et al. (2005) and P. MacNeice (2009). This algorithm detects peaks in HSS events in both the OMNI data and the model predictions, excluding ICME events. Figure 1 shows the application of the algorithm for automatic detection, highlighting the identified peaks in HSS events with inverted triangle markers in both the observed and predicted solar wind speed data.

The model verification process is conducted by checking whether the predicted events belong to a $\pm 1$ day time window around the peaks in the OMNI data. If a predicted event matches an OMNI peak within the specified time window, it is classified as ""hits."" Conversely, if no predicted event occurs within the time window, it is classified as ""misses.""
![img-0.jpeg](img-0.jpeg)

Figure 1. Solar wind speed at 1 au during the period October-December for the years 2012-2020 by OMNI (black), our work (red), WSA-ENLIL (sky blue), and ESWF 3.2 (green). Here inverted triangles indicate HSS events detected by the algorithm for automatic detection. Intervals corresponding to ICMEs are shaded in gray, with these periods based on the Richardson and Cane ICME catalog.

Additionally, if a model erroneously predicts an event when no actual HSS event has occurred, it is categorized as ""false alarms."" Using this methodology, hits (true positives, TPs), misses (false negatives, FNs), and false alarms (false positives, FPs) are counted to construct a confusion matrix, as shown in Table 1. Since this analysis focuses on predicted and observed events, cases with no predicted or observed events (true negatives, TNs) are not considered.

Using the calculated TP, FN, and FP, we derive the following metrics to evaluate model performance. The
Table 1
Confusion Matrix for Predicted and Observed HSS Events

|  | Observed |
| :-- | :--: |
| Predicted | HSS |
| HSS | TP |
| No HSS | FN |

Table 2
Statistical Verification Results

| CR Period <br> Metrics | Ascending Phase |  |  | Maximum Phase |  |  | Descending Phase |  |  | Minimum Phase |  |  | Entire Period |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | MAE | RMSE | CC | MAE | RMSE | CC | MAE | RMSE | CC | MAE | RMSE | CC | MAE | RMSE | CC |
| DL model (Ours) | 57.6 | 67.3 | 0.49 | 60.5 | 83.3 | 0.35 | 51.0 | 66.2 | 0.77 | 41.7 | 58.6 | 0.65 | 51.4 | 68.6 | 0.69 |
| WSA-ENLIL | 67.7 | 92.9 | 0.35 | 98.0 | 119.2 | 0.37 | 86.2 | 114.8 | 0.49 | 86.8 | 125.7 | 0.24 | 86.9 | 116.3 | 0.41 |
| ESWF 3.2 | 74.5 | 94.9 | 0.12 | 91.5 | 122.2 | 0.19 | 117.4 | 147.8 | $-0.02$ | 86.5 | 116.1 | $-0.09$ | 100.4 | 130.9 | 0.08 |
| Persistence (3 days) | 63.4 | 82.5 | 0.32 | 82.6 | 105.7 | 0.23 | 107.6 | 135.0 | 0.11 | 74.3 | 97.1 | 0.19 | 90.1 | 116.3 | 0.22 |
| Persistence (4 days) | 65.9 | 85.8 | 0.26 | 85.8 | 109.3 | 0.18 | 116.9 | 146.4 | $-0.04$ | 80.3 | 105.8 | 0.06 | 96.7 | 125.0 | 0.11 |
| Persistence (5 days) | 69.9 | 89.7 | 0.2 | 84.6 | 107.9 | 0.22 | 120.4 | 151.4 | $-0.1$ | 85.4 | 111.6 | 0.0 | 99.7 | 128.9 | 0.06 |
| Persistence (27 days) | 67.1 | 86.2 | 0.22 | 64.1 | 88.5 | 0.39 | 68.3 | 90.9 | 0.61 | 59.5 | 77.8 | 0.55 | 65.2 | 87.0 | 0.57 |

Note. The performance of the DL model, WSA-ENLIL model, ESWF 3.2 model, and persistence models is evaluated by statistical metrics such as MAE, RMSE, and CC for different solar activity phases. Bold values indicate the best performance for each metric.
probability of detection (POD), which is the fraction of observed events successfully detected by the model, is given by

$$
\mathrm{POD}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
$$

The false negative rate (FNR), which is the fraction of observed events that the model failed to detect, is given by

$$
\mathrm{FNR}=\frac{\mathrm{FN}}{\mathrm{TP}+\mathrm{FN}}
$$

The success ratio (SR), which is the fraction of predicted events that match the observed events, is given by

$$
\mathrm{SR}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
$$

The false alarm ratio (FAR), which is the fraction of predicted events that fail to occur among all predicted events, is given by

$$
\mathrm{FAR}=\frac{\mathrm{FP}}{\mathrm{TP}+\mathrm{FP}}
$$

The critical success index (CSI), which is the ratio of correctly predicted events to the total number of recorded events, is given by

$$
\mathrm{CSI}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}+\mathrm{FP}}
$$

These metrics range from 0 to 1 , where for POD, SR, and CSI, 0 indicates poor performance and 1 indicates perfect performance. In contrast, for FNR and FAR, the reverse is true. The bias score (BS), which is the ratio of predicted events to observed events, is given by

$$
\mathrm{BS}=\frac{\mathrm{TP}+\mathrm{FP}}{\mathrm{TP}+\mathrm{FN}}
$$

Although this does not measure the correlation between predictions and observations, it shows whether the model underestimates ( $\mathrm{BS}<1$ ) or overestimates ( $\mathrm{BS}>1$ ) observations. These six metrics (POD, FNR, SR, FAR, CSI, and BS) are used to evaluate the performance of solar wind speed prediction models in detecting HSS events. A detailed discussion on the verification matrix is given by D. S. Wilks (2011) and I. T. Jolliffe \& D. B. Stephenson (2012).

## 4. Results and Discussion

The performance of each model-the DL model and the two empirical models-in predicting solar wind speed is evaluated using both statistical and event-based verification. Figure 1 shows the solar wind speed predictions at 1 au from 2012 to 2020. The DL model shows fluctuations similar to the OMNI data, especially during 2016 and 2017, although it generally underestimates the solar wind speed. The WSA-ENLIL model shows relatively smooth and stable predictions, but deviates significantly from the OMNI data during HSS events. The ESWF 3.2 model tends to predict an earlier arrival of HSS events than observations; however, its predictions are overly variable, resulting in large discrepancies with the OMNI data at certain intervals.

Table 2 presents a comparison of the statistical metrics for the DL model, the two empirical models, and the four persistence models for different solar activity phases, as well as over the entire period. Persistence is the naive forecasting model that carries forward the most recently observed solar wind speed unchanged (M. J. Owens et al. 2013). For all four solar activity phases, the DL model consistently shows lower errors than the empirical models and persistence models, both within each phase and over the entire period. Over the entire period, the DL model achieves an MAE of $51.4 \mathrm{~km} \mathrm{~s}^{-1}$ and an RMSE of $68.6 \mathrm{~km} \mathrm{~s}^{-1}$, demonstrating lower errors than the empirical models, with a high CC of 0.69 . In particular, during
![img-1.jpeg](img-1.jpeg)

Figure 2. Variations in the solar sunspot number and the DL model's CC over the entire period (2012–2020). The gray line represents the monthly mean total sunspot number, obtained by taking the simple arithmetic mean of the daily sunspot numbers for each calendar month. The black line indicates the monthly smoothed total sunspot number, derived by applying a 13 months tapered-boxcar running mean. The red line represents the CC of the DL model, the sky blue line represents the CC of WSA–ENLIL, the green line represents the CC of ESWF 3.2, and the orange line represents the CC of the 27 days persistence model.

![img-2.jpeg](img-2.jpeg)

Figure 3. Taylor diagram showing the statistical comparison of RMSE, CC, and standard deviation for solar wind speed prediction models for different solar activity phases and over the entire period. Note that because the CC values for the descending and minimum phases in the ESWF 3.2 model are negative, they are not displayed in the figure.

The descending phase, the DL model records notably lower errors, with an MAE of 51.0 km s⁻¹ and an RMSE of 66.2 km s⁻¹. Moreover, the DL model achieves a higher CC of 0.77, demonstrating a good agreement with the observations. In contrast, the ESWF 3.2 model shows negative CC during the descending and minimum phases, which is probably caused by insufficient detection of the CH regions. The results show that the DL model consistently outperforms the four persistence models.

Figure 2 shows how the CC of the models and the solar sunspot numbers from the Solar Influences Data Analysis Center (SIDC)/Sunspot Index and Long-term Solar
![img-3.jpeg](img-3.jpeg)

Figure 4. Histograms showing the number of HSS events within a ±2 days time window for the DL model (left, red), the WSA-ENLIL model (center, blue), and the ESWF 3.2 model (right, green). The histogram, using a bin size of 6 hr, shows the time difference on the horizontal axis calculated as (the predicted peak times – the observed peak times for HSS events).

Table 3 Event-based Verification Results

|  Model | Totals | Entire Period (2012–2020) |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|   |  | TP | FP | FN | POD | FNR | SR | FAR | CSI | BS  |
|  DL model (ours) | 40 | 33 | 7 | 54 | 0.38 | 0.62 | 0.82 | 0.18 | 0.35 | 0.46  |
|  WSA-ENLIL | 52 | 21 | 31 | 66 | 0.24 | 0.76 | 0.4 | 0.6 | 0.18 | 0.6  |
|  ESWF 3.2 | 116 | 25 | 91 | 62 | 0.29 | 0.71 | 0.22 | 0.78 | 0.14 | 1.33  |
|  Persistence (27 days) | 94 | 41 | 53 | 46 | 0.47 | 0.53 | 0.44 | 0.56 | 0.29 | 1.08  |

Note. Comparison of metrics (POD, FNR, SR, FAR, CSI, and BS) for the DL model, the WSA-ENLIL model, the ESWF 3.2 model, and the 27 days persistence model based on ±1 day time windows over the period of October–December for the years 2012–2020. Bold values indicate the best performance for each metric.

Observations (SILSO) vary over the entire period of solar cycle 24. As seen in Table 2, the DL model outperforms the other models in all phases, suggesting that the DL model captures the CH areas more effectively than the other models. In addition, the DL model has the best CC value (0.86) during the descending phase (around 2016) and maintains good performance during the minimum phase, which may be related to the CH areas that are dominant during these phases as the source of fast solar winds. In contrast, during the maximum phase, when the CH area remains primarily at high latitudes and solar eruptive activities such as CMEs become more frequent, the performance of the DL model is reduced compared to other phases. This result is presumed to arise because the DL model cannot adequately learn information about the magnetic field structure.

Figure 3 shows the performance of the models for different solar activity phases and over the entire period using a Taylor diagram. The diagram shows RMSE, CC, and standard deviation, providing an intuitive comparison of the models' performance. Note that the CCs for the descending and minimum phases in the ESWF 3.2 model are negative; consequently, they are not displayed in the figure. Consistent with the results in Table 2, the DL model (red) has lower RMSE values and higher CC than the empirical models, indicating better predictive performance. The lower RMSE values indicate that the values predicted by the DL model are closer to the observations than those from the empirical models. The higher CC values imply that the pattern of predicted solar wind speeds is more similar to the observations than those of the empirical models. However, the DL model's standard deviations are notably lower than those of the empirical models.

Figure 4 shows the histograms of identified HSS events within a ±2 days time window for the DL model and two empirical models. The WSA-ENLIL and ESWF 3.2 models demonstrate a wide distribution of time differences between their predicted peak times for HSS events and the observed peak times for them. In contrast, the DL model shows a distribution similar to a normal distribution, with time differences concentrated around zero, indicating that the predicted peak times for HSS events align more closely with the observed peak times for them.

Table 3 shows the results of the event-based verification, including the counts for TP, FP, and FN, as well as six metrics: POD, FNR, SR, FAR, CSI, and BS. This method identifies HSS events from the OMNI data using a ±1 day time window to more accurately detect event peaks. Unlike the POD of the WSA-ENLIL model of 0.24 and the POD of the ESWF 3.2 model of 0.29, the DL model records a higher POD of 0.38, thus outperforming these empirical models in accurately predicting HSS events. It also maintains the highest SR of 0.82, underscoring its reliability. Moreover, it attains the highest CSI of 0.35, demonstrating the best overall performance among the models. In comparison, the WSA-ENLIL model records an SR of 0.4 and a CSI of 0.18, and the ESWF 3.2 model records an SR of 0.22 and a CSI of 0.14, further highlighting the advantages of the DL model. However, the DL model shows the lowest BS value, suggesting a tendency to underestimate the occurrence of HSS events compared to the empirical models. Although the 27 days persistence model achieves the highest POD of 0.47, its SR of 0.44 and CSI of 0.29 are below those of the DL model, indicating lower reliability.
## 5. Summary

In this study, we have compared a DL model for solar wind speed prediction with two representative empirical models: the WSA-ENLIL and ESWF 3.2 models. For statistical verification, the DL model outperforms the empirical models throughout the evaluation period (2012-2020), achieving the lowest MAE of $51.4 \mathrm{~km} \mathrm{~s}^{-1}$, RMSE of $68.6 \mathrm{~km} \mathrm{~s}^{-1}$, and the highest CC of 0.69 . In particular, during the descending phase (2015-2018), it records an MAE of $51.0 \mathrm{~km} \mathrm{~s}^{-1}$, an RMSE of $66.2 \mathrm{~km} \mathrm{~s}^{-1}$, and a CC of 0.77 , indicating impressive performance. For event-based verification, histogram analysis reveals that the DL model's identified HSS events within a $\pm 2$ days time window follow a distribution similar to a normal distribution, indicating that the predicted peak times align more closely with the observed peaks. Moreover, the DL model achieves the highest SR of 0.82 and CSI of 0.35 in detecting HSS events within a $\pm 1$ day time window. These findings indicate that the model is effective in reducing false alarms and improving prediction accuracy, although it tends to underestimate the frequency of events.

The DL model, which serves as a practical tool for space weather forecasting, provides highly accurate and reliable predictions. In particular, it offers near-real-time solar wind speed forecasts with an inference time of less than a minute. Our DL model still has the limitation that, although its POD is higher than those of other empirical models, it remains low due to a high FN rate and is lower than that of the persistence models. This fact suggests that the DL model does not fully capture the dynamic variability of solar wind speed. During the solar maximum phase, both the empirical models and the DL model struggle to predict solar wind speed, underscoring the need to incorporate information on magnetic field structure. For example, adding a magnetogram or a squashing factor that quantifies magnetic topology would help the model learn the source regions of the slow solar wind more accurately and thus enhance its predictive performance. In addition, the study focuses on event-based verification of HSS events, as CME data are currently not incorporated into the model. In an effort to address this, attempts to integrate CME data into the model have resulted in only minimal improvements, suggesting that a deeper analysis of the relationship between CMEs and solar wind speed is necessary for future research. Lastly, models that combine DL with empirical methods, such as hybrid frameworks or those that integrate attention mechanisms, are expected to enhance the sensitivity of event detection. This would improve the overall accuracy and reliability of space weather forecasting.

## Acknowledgments

This work was supported by The Korea Astronomy and Space science Institute under the R\&D program (Project No. 2025-1-850-02) and through grant No. (KASI 2025185002) supervised by the Ministry of Science and ICT (MSIT), Institute for Information and Communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (RS-2023-00234488, Development of solar synoptic magnetograms using deep learning, 15\%), BK21 FOUR program through National Research Foundation of Korea (NRF) under Ministry of Education (MoE) (Kyung Hee

University, Human Education Team for the Next Generation of Space Exploration), BK21 FOUR program of Graduate School, Kyung Hee University (GS-1-JO-NON-20242364), and by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-202400346061), and the European Research Council Executive Agency (ERCEA) under the ERC-AdG agreement No. 101141362 (Open SESAME). We gratefully acknowledge the following contributions: the CORHEL-MAS_WSA_ENLIL model by Jon Linker (Predictive Science); the information on the WSA_ENLIL data provided by Dr. Jian (NASA/ GSFC-6720); the ESWF 3.2 data from Daniel Milosic and Assoc. Prof. Dr. Manuela Temmer (HPRG, University of Graz, Austria); OMNI data via NASA/GSFC's Space Physics Data Facility's OMNIWeb service; and the DL model by Dr. Son (KASI).

## ORCID iDs

Seungwoo Ahn (1) https://orcid.org/0009-0007-8498-9985
Jihyeon Son (1) https://orcid.org/0000-0003-2678-5718
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Hyun-Jin Jeong (1) https://orcid.org/0000-0003-4616-947X

## References

Abbo, L., Ofman, L., Antiochos, S., et al. 2016, SSRv, 201, 55
Altschuler, M. D., \& Newkirk, G. 1969, SoPh, 9, 131
Arge, C., \& Pizzo, V. 2000, JGRA, 105, 10465
Bravo, S., \& Stewart, G. A. 1997, ApJ, 489, 992
Brown, E. J., Svoboda, F., Meredith, N. P., Lane, N., \& Horne, R. B. 2022, SpWea, 20, e2021SW002976
Jian, L. K., MacNeice, P. J., Taktakishvili, A., et al. 2015, SpWea, 13, 316
Jolliffe, I. T., \& Stephenson, D. B. 2012, Forecast Verification: A Practitioner's Guide in Atmospheric Science (New York: Wiley)
MacNeice, P. 2009, SpWea, 7, S12002
McComas, D., Barraclough, B., Funsten, H. O., et al. 2000, JGR, 105, 10419
Milošíc, D., Temmer, M., Heinemann, S., et al. 2023, SoPh, 298, 45
Odstrcil, D., Linker, J. A., Lionello, R., et al. 2002, JGRA, 107, 1493
Odstrcil, D., Pizzo, V. J., Linker, J. A., et al. 2004, JASTP, 66, 1311
Owens, M. J., Arge, C. N., Spence, H. E., \& Pembroke, A. 2005, JGRA, 110, A12105
Owens, M. J., Challen, R., Methven, J., Henley, E., \& Jackson, D. 2013, SpWea, 11, 225
Raju, H., \& Das, S. 2021, SoPh, 296, 134
Reiss, M. A., Temmer, M., Veronig, A. M., et al. 2016, SpWea, 14, 495
Richardson, I. G., \& Cane, H. V. 2010, SoPh, 264, 189
Riley, P., Linker, J. A., \& Arge, C. N. 2015, SpWea, 13, 154
Riley, P., Linker, J. A., \& Mikić, Z. 2013, JGRA, 118, 600
Riley, P., Linker, J. A., Mikić, Z., et al. 2002, ApJ, 578, 972
Robbins, S., Henney, C. J., \& Harvey, J. W. 2006, SoPh, 233, 265
Rotter, T., Veronig, A. M., Temmer, M., \& Vršnak, B. 2015, SoPh, 290, 1355
Schatten, K. H. 1971, CosEl, 2, 232
Schatten, K. H., Wilcox, J. M., \& Ness, N. F. 1969, SoPh, 6, 442
Son, J., Sung, S.-K., Moon, Y.-J., Lee, H., \& Jeong, H.-J. 2023, ApJS, 267, 45
Sun, Y., Xie, Z., Chen, Y., Huang, X., \& Hu, Q. 2021, SpWea, 19, e2020SW002707
Taylor, K. E. 2001, JGRD, 106, 7183
Upendran, V., Cheung, M. C., Hanasoge, S., \& Krishnamurthi, G. 2020, SpWea, 18, e2020SW002478
Vršnak, B., Temmer, M., \& Veronig, A. M. 2007, SoPh, 240, 315
Wang, Y.-M., \& Sheeley, N., Jr. 1990a, ApJ, 355, 726
Wang, Y.-M., \& Sheeley, N., Jr. 1990b, ApJ, 365, 372
Wang, Y.-M., \& Sheeley, N., Jr. 1992, ApJ, 392, 310
Wilks, D. S. 2011, Statistical Methods in the Atmospheric Sciences (New York: Academic)"
Eunsu Park et al 2023 - Pixel-to-pixel Translation of Solar Extreme-ultraviolet Images for DEMs by Fully Connected Networks.pdf,"# Pixel-to-pixel Translation of Solar Extreme-ultraviolet Images for DEMs by Fully Connected Networks 

Eunsu Park ${ }^{1}$ (D) Harim Lee ${ }^{2}$ (D) Yong-Jae Moon ${ }^{2,3}$ (D), Jin-Yi Lee ${ }^{2}$ (D), Il-Hyun Cho ${ }^{2}$ (D), Kyoung-Sun Lee ${ }^{4}$ (D), Daye Lim ${ }^{5}$ (D), Hyun-Jin Jeong ${ }^{5}$ (D), and Jae-Ok Lee ${ }^{6}$ (D)<br>${ }^{1}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea; eunsupark@kasi.re.kr<br>${ }^{2}$ Department of Astronomy and Space Science, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{3}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{4}$ Astronomy Program, Department of Physics and Astronomy, Seoul National University, Seoul, 08826, Republic of Korea<br>${ }^{5}$ Centre for mathematical Plasma-Astrophysics, Department of Mathematics, KU Leuven, Leuven, 3001, Belgium<br>${ }^{6}$ Research Institute of Natural Sciences, Chungnam National University, Daejeon, 34134, Republic of Korea<br>Received 2022 September 23; revised 2022 December 2; accepted 2022 December 4; published 2023 January 24


#### Abstract

In this study, we suggest a pixel-to-pixel image translation method among similar types of filtergrams such as solar extreme-ultraviolet (EUV) images. For this, we consider a deep-learning model based on a fully connected network in which all pixels of solar EUV images are independent of one another. We use six-EUV-channel data from the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO), of which three channels (17.1, 19.3, and 21.1 nm ) are used as the input data and the remaining three channels ( $9.4,13.1$, and 33.5 nm ) as the target data. We apply our model to representative solar structures (coronal loops inside of the solar disk and above the limb, coronal bright point, and coronal hole) in SDO/AIA data and then determine differential emission measures (DEMs). Our results from this study are as follows. First, our model generates three EUV channels (9.4, 13.1, and 33.5 nm ) with average correlation coefficient values of $0.78,0.89$, and 0.85 , respectively. Second, our model generates the solar EUV data with no boundary effects and clearer identification of small structures when compared to a convolutional neural network-based deep-learning model. Third, the estimated DEMs from AIgenerated data by our model are consistent with those using only SDO/AIA channel data. Fourth, for a region in the coronal hole, the estimated DEMs from AI-generated data by our model are more consistent with those from the 50 frames stacked SDO/AIA data than those from the single-frame SDO/AIA data.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar instruments (1499); Solar atmosphere (1477); Solar extreme ultraviolet emission (1493); Neural networks (1933)

## 1. Introduction

Over the last several decades, various solar satellite missions such as the Solar and Heliospheric Observatory (SOHO; Domingo et al. 1995, the Transition Region and Coronal Explorer (Handy et al. 1999), Hinode (Kosugi et al. 2007), the Solar Terrestrial Relationship Observatory (STEREO; Kaiser et al. 2008), and the Solar Dynamics Observatory (SDO; Pesnell et al. 2012), have been observing the Sun. In particular, several instruments on board these missions have observed various types of multichannel and simultaneous data. SOHO/ Michelson Doppler Imager (Scherrer et al. 1995) and SDO/ Helioseismic and Magnetic Imager (HMI; Scherrer et al. 2012; Schou et al. 2012) have been observing the intensity, Doppler shift, and photospheric magnetic field. SOHO/Extreme Ultraviolet Imaging Telescope (EIT; Delaboudiniere et al. 1995), STEREO/Extreme Ultraviolet Imager (EUVI; Howard et al. 2008), and SDO/Atmospheric Imaging Assembly (AIA; Boerner et al. 2012; Lemen et al. 2012) have been observing ultraviolet (UV) and extreme-UV (EUV) data in various channels. There is no doubt that these are powerful and excellent data sets for not only scientific research but also machine learning.

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.

Recently, various studies have applied deep-learning algorithms to multichannel solar data. Because most of the solar multichannel data have been provided as a form of image data, these studies have applied convolutional neural networks (CNNs), which are very efficient and suitable for processing image data and extracting meaningful feature maps from the image data. Galvez et al. (2019) presented a curated data set from the SDO mission. They applied a deep-learning model based on CNNs to generate solar UV/EUV images using SDO/HMI vector magnetograms. Szenicer et al. (2019) presented a CNN-based model that learns mapping from EUV narrowband images to spectral irradiance measurements using SDO data. Pineci et al. (2021) presented a CNN-based model to the translation from SOLIS He I images to solar EUV images. Several studies have reported a solar image translation model using a CNN model with a generative adversarial network (GAN; Goodfellow et al. 2014) loss function. Kim et al. (2019) applied a general-purpose image translation algorithm, named Pix2Pix (Isola et al. 2016), based on a conditional GAN (Mirza \& Osindero 2014) and a deep convolutional GAN (Radford et al. 2015), to generate solar magnetograms using SDO/AIA data and then applied the model to STEREO/EUVI images to produce solar farside magnetograms. Park et al. (2019) implemented a deeplearning technique to generate solar UV/EUV images from solar magnetograms. They reported that the generated solar UV/EUV images from CNN models with GAN loss are clearer than those from CNN models without GAN loss in
![img-0.jpeg](img-0.jpeg)

Figure 1. Cartoon of the difference between an FCN and a CNN. The circles represent the pixels of the input, feature map, and output. The green circles represent the pixels needed to determine the yellow circle in the output. The solid lines represent fully connected layers or convolution layers, and the same color has the same weight.

Most channels. Lee et al. (2021) applied pix2pix models to the translation from solar sunspot drawings to SDO/AIA and HMI data, then applied the model to the Galileo sunspot drawings of 1612. Lim et al. (2021) applied Pix2pix to the image translation and then selected the best translation pairs among 170 deep-learning models. They showed the potential for deep learning to provide one additional guideline when a few channels of an UV/EUV imaging instrument need to be selected for subsequent satellite missions. Jeong et al. (2020) applied an improved image translation algorithm, named Pix2PixHD (Wang et al. 2017), to generate solar farside magnetograms. They applied the generated farside magnetograms to the extrapolation of global coronal fields using a potential field source surface model. Son et al. (2021) also applied the Pix2pixHD to image translation from SDO/AIA 19.3 and 30.4 nm data to National Solar Observatory/Synoptic Optical Long-term Investigations of the Sun He I 1083 nm data. They showed it was possible for deep learning to compensate for the limitations of ground observation, such as observation time or weather conditions.

In this work, we propose a pixel-to-pixel translation method for image translation among similar types of filtergrams such as SDO/AIA channel images. As is well known, the data number normalized by exposure time in the *i*th SDO/AIA channel (DN s⁻¹) can be described as

$$DN_i = \int R_i(T) \text{DEM}(T) \, dT,\tag{1}$$

where *R<sub>i</sub>*(T) is the temperature response function of the *i*th SDO/AIA channel (DN cm⁵ K⁻¹) and *DEM*(T) is the differential emission measure (DEM). The DEM can be expressed by the relation of

$$\int \text{DEM}(T) \, dT = \langle n_e^2 \rangle \, dl,\tag{2}$$

which is integrated electron density *ne* over along the line of sight depth *dl*. To satisfy these physical conditions in the deep-learning model, we design a fully connected network (FCN)-based model for the pixel-to-pixel translation. Generally, a FCN-based model consists of one or more fully connected layers, which is generally a function from input *m* nodes to output *n* nodes. We design an FCN model as a function from one pixel of three input channel images (three nodes) to one pixel of output channel images (three nodes, same location as input channel images). The designed model will generate one pixel (three values from three AIA output channel images) using only one pixel (three values from three AIA input channel images) at the same location, and information on other location pixels is not used. This means the model considers all pixels independent of each other as in Equations (1) and (2). In the case of the translation among SDO/AIA channel images, pixel-to-pixel translation can be effective and reasonable. On the other hand, in the case of the translation between two domains that are not similar to each other, such as the translation between solar EUV images and solar magnetograms (Park et al. 2019), applying pixel-to-pixel translation is ineffective, and applying a CNN-based model seems to be reasonable. CNN-based image translation models generally consist of one or more convolution layers. A CNN-based model performs one or more convolution operations in the process of generating an output image, which uses information from several pixels of the input image to determine one pixel of
the output image. Also, many CNN-based image translation models use image-resizing layers such as a max pooling layer, this results in not dealing with all pixels independently. Figure 1 shows a cartoon of the difference between an FCN and a CNN. There is one more advantage of the pixel-to-pixel translation, which is a great gain in terms of the amount of data because all pixels are counted and included individually. It is well noted that a large amount of data is an essential condition for applying and training a deep-learning model.

In this study, we demonstrate the usefulness of the pixel-topixel translation method for the image translation of SDO/AIA three-channel data to other three-channel data. To demonstrate the model performance, we calculate the DEMs using the deeplearning model's generated EUV data, then compare the results to those from SDO/AIA EUV data. This paper is organized as follows: The data and the model will be described in Section 2. Results and discussion are given in Section 3, and a summary and conclusions are presented in Section 4.

## 2. Data and Method

### 2.1. Data

We use SDO/AIA six-EUV-channel data. We consider three channels (17.1, 19.3, and 21.1 nm ) for the input data and the remaining three channels ( $9.4,13.1,33.5 \mathrm{~nm}$ ) for the target data. We select the three input channels for the following reasons: First, we select them because they are relatively less noisy. We think that the less noisy input data would make the model training more stable. We may use a binning process to reduce noise as an alternative, but our goal is to use fullresolution data without binning. Second, we consider the temperature response function of AIA channels. The characteristic emission temperatures for EUV channels are: 17.1 nm $\left(\log T[K] \sim 5.85[\mathrm{Fe} \mathrm{IX}]\right.$ for coronal hole $(\mathrm{CH})$, quiet Sun (QS), active region (AR) and flare (FL)), $19.3 \mathrm{~nm}\left(\log T \sim 6.2\right.$ [Fe XII] for QS and AR, $\log T \sim 7.25$ [Fe XXIV] for FL), 21.1 nm ( $\log T \sim 6.15$ [Fe XI] for CH and QS, $\log T \sim 6.3$ [Fe XIV] for QS, AR and FL), 9.4 nm ( $\log T \sim 6.05[\mathrm{Fe} \mathrm{X}]$ for CH and QS, $\log T \sim 6.85$ [Fe XVIII] for AR and FL), 13.1 nm $\left(\log T \sim 5.6\right.$ [Fe VIII] for CH and QS, $\log T \sim 7.05$ [Fe XXI] for FL), and 33.5 nm ( $\log T \sim 6.05$ [Fe X] for CH and QS, $\log T \sim$ 6.45 [Fe XVI] for AR and FL) (O'Dwyer et al. 2010; Lemen et al. 2012). Since the 17.1, 19.3, and 21.1 nm used as inputs range from low to high temperatures, it is possible to generate $9.4,13.1$, and 33.5 nm from the high-temperature contributions to the cooler channels. Third, they have been commonly adopted for space missions in that SOHO/EIT and STEREO/ EUVI have similar channels. We expect our model to be applicable to data observed in other space missions as well.

We select 358 pairs of SDO/AIA EUV data at every 00:00 UT in 2011 for the training data set. We also select 80 pairs of SDO/AIA EUV data at 00:00 UT on the first day of every month from 2012 to 2018 for the test data set. We process SDO/AIA level 1 data with centering, rotating, normalizing, and correcting degradation. We use standard solar data libraries based on Python (SunPy and AIAPy) for data processing. For training and testing, we use pixels inside of 1.2 solar radii to take into account coronal loops above the limbs. We use the full dynamic range of the original data without limitation.

### 2.2. Deep-learning Model

Figure 2 shows the flowchart and the structure of our FCNbased model (hereafter FCN model). The FCN model consists of the encoder (small gray boxes) and the decoder (small yellow boxes). There are skip connections by concatenation between the encoder and decoder, which is a similar structure to the U-Net model (Ronneberger et al. 2015). The encoder and decoder have eight fully connected layers each. There is a Sigmoid Linear Unit (SiLU or Swish; Ramachandran et al. 2017) activation layer between every two layers, and there is no activation layer after the final output layer. The FCN model does not include a regularization layer. For the comparison, we design an additional deep-learning model, the CNN-based model (hereafter CNN model). The overall structure of the CNN model is identical to the FCN model, but convolution layers replace all fully connected layers. The CNN model has batch normalization layers, whereas the FCN model does not include any regularization layers. To train the models, we use the L2 loss function, which is given by

$$
L_{2}=\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}
$$

where $i$ is a pixel number, $N$ is the total number of pixels, $y$ is the target, and $f(x)$ is the model output. For optimization, we use the Adam solver (Kingma \& Ba 2014) with an initial learning rate of $2 \times 10^{-4}$. We apply the initial learning rate for 50 epochs, and then the running rate decreases linearly to zero for 50 epochs.

### 2.3. Differential Emission Measure

Many previous studies have developed various methods to determine the DEMs of the solar corona. In order to determine DEMs, we apply the method of Morgan \& Pickering (2019) named the Solar Iterative Temperature Emission Solver for Differential Emission Measure (SITES DEM). This solver produces an initial DEM estimate by redistributing observed intensities across temperatures based on the temperature response function of the measurement and subsequently updates this estimation through the calculation of intensity residuals (Morgan \& Pickering 2019). We calculate the DEMs of 31 temperature bins from $\log T=5.0$ to $\log T=8$. and use the same SDO/AIA response function corrected for the degradation of AIA sensitivity over time.

## 3. Result and Discussion

Figure 3 shows an example of the image translation result from the two deep-learning models (FCN and CNN) and the comparison between the result and SDO/AIA EUV data at 2012 March 8 00:00 UT. The two deep-learning models are applied to the $4096 \times 4096$ size of three-input-channel EUV data. The two models successfully generate three channels of EUV data, not only the inside but also the outside of the solar disk. As shown in Figure 3, the overall structures, the coronal loops, and the small-scale structures are generated from both models. However, there are several square boxes around the edge of the EUV image from the CNN model; this is due to the boundary effect. To apply the CNN model to $4096 \times 4096$ images, we crop the input data into two or more patches, translate, and reattach them. The first row in Figure 4 shows the enlarged plots inside the white solid-line box in Figure 3. It is
![img-1.jpeg](img-1.jpeg)

Figure 2. Flowchart and structure of our FCN model. The solid arrows indicate fully connected layers, and the numbers in the small boxes in the model show the number of output nodes after each fully connected layer. The dashed arrows indicate the skip connections from the encoder to the decoder. The gray boxes in the model indicate the encoder, the yellow boxes show the decoder, and the red boxes indicate the output nodes skipped from the encoder. The green box shows how to put three input channels into the model, and the blue box shows how to get three output channels from the model.

Noted that the generated EUV images from the models are less noisy than the target SDO/AIA ones. We plot the intensity profiles on the white lines in the third plot of the first row. The three intensity profiles are very similar to one another in overall tendencies, but those from SDO/AIA show large fluctuations in all cases, which means that the EUV data from our models are less noisy than those from SDO/AIA.

Table 1 shows the average values of the correlation coefficient (CC), the rms error (RMSE), and the signal-to-noise ratio (S/N) from our FCN model result for the test data set. The CC, RMSE, and S/N values for the test data set are calculated using pixels within 1.2 solar radii, and the S/N values are defined as the ratio between the average of the pixel values within 1.2 solar radii and their background noise. In this study, the background noise is determined as a half width at half maximum of a Gaussian fitting to the difference image between two sequential images with 12 s time cadence for the selected region. The average CC values for the 9.4, 13.1, and 33.5 nm channels are 0.78, 0.89, and 0.85, respectively. The average CC value of the 9.4 nm channel is relatively low compared to the other two channels. This is because the dynamic range of this channel is smaller than the other two channels, and our FCN model reduces many hot pixels that look like noise, as shown in Figure 4. The average RMSE values for the 9.4, 13.1, and 33.5 nm channels are 1.18, 4.14, and 3.58, respectively. The average S/N values for the 9.4, 13.1, and 33.5 nm channels of SDO/AIA are 1.37, 3.03, and 2.35, and those of the FCN model results are 23.20, 16.32, and 14.31, respectively. These results show that S/N values of the AI-generated images are noticeably larger than those of the original images. We compare our FCN model results with the corresponding SDO/AIA EUV data for the specific cases and structures.

Figure 5 shows an example for a coronal loop located inside of the solar disk and a comparison between SDO/AIA EUV data and model results. Our FCN model shows the relatively faint coronal loop structures more clearly. We calculate CC, RMSE, and S/N values using all pixels in Figure 5, and define the S/N value as the ratio between the average pixel value in a region of interest (ROI), which is noted as a white solid line, and the background noise of all pixels in the figure. The CC values between SDO/AIA data and our FCN model results for the 9.4, 13.1, and 33.5 nm channels are 0.75, 0.88, and 0.84, and the RMSE values are 2.81, 6.89, and 7.19, respectively. The S/N values for the 9.4, 13.1, and 33.5 nm channels of SDO/AIA are 0.78, 3.43, and 2.53, and those of the FCN model results are 19.04, 19.77, and 10.77, respectively. To calculate DEM, we consider two data sets (hereafter DEMSETs). The first data set is six channels (17.1, 19.3, 21.1, 9.4, 13.1, and 33.5 nm) from SDO/AIA (hereafter DEMSET-AIA). The second data set is three channels (17.1, 19.3, and 21.1 nm) from SDO/AIA and three channels (9.4, 13.1, and 33.5 nm) from our FCN model (hereafter DEMSET-FCN). We calculate DEMs of the selected ROI, then average the calculated DEM. The last plot in Figure 5 shows DEMs from the two DEMSETs. The DEMs from two DEMSETs are consistent with each other, except for relatively high-temperature bins from 6.5 to 7.0 in log T.

Figure 6 shows an example for a coronal bright point on the solar disk. As shown in Figure 6, our FCN model generates the small loop structures in the coronal bright point more clearly. In addition, the background noise in the SDO/AIA EUV data is greatly reduced in the model results. The CC values between
![img-2.jpeg](img-2.jpeg)

Figure 3. A comparison example between three-EUV-channel data from SDO/AIA EUV and those from our models at 2012 March 8 00:00 UT. The first, second, and third rows represent SDO/AIA EUV data, our FCN-model-generated EUV data, and the CNN-model-generated EUV data, respectively.

The SDO/AIA data and our FCN model results for the 9.4, 13.1, and 33.5 nm channels are 0.63, 0.85, and 0.90, and the RMSE values are 0.52, 1.60, and 1.10, respectively. The S/N values for the 9.4, 13.1, and 33.5 nm channels of SDO/AIA are 3.97, 11.56, and 16.17, and those of the FCN model results are 109.14, 53.59, and 94.03, respectively.

Figure 7 shows an example for a coronal loop located outside of the solar disk. We select a coronal loop at 17.1 nm, which is not significantly varied in position during several frames and is clearly observed in the 17.1 and 13.1 nm channels. This is a complex area with several small coronal loops, and it is difficult to distinguish the coronal structures from noise in single-channel images. Our FCN model distinguishes the coronal structures and shows them more clearly. The CC values between the SDO/AIA data and our FCN model results for the 9.4, 13.1, and 33.5 nm channels are 0.92, 0.94, and 0.96, and the RMSE values are 0.95, 6.15, and 5.23, respectively. The S/N values for the 9.4, 13.1, and 33.5 nm channels of SDO/AIA are 8.64, 17.89, and 18.36, and those of the FCN model results are 194.47, 111.90, and 204.30, respectively. The DEMs from DEMSET-FCN are also relatively high at high temperatures like the above example. This is because our FCN model clearly generates faint loops, which is reflected in the DEMs.

Figure 8 shows an example for comparison between SDO/AIA EUV images and model results at 2012 April 9 12:00 UT. A coronal hole is selected for the comparison. The CC values between the SDO/AIA data and our FCN model results for the 9.4, 13.1, and 33.5 nm channels are 0.74, 0.89, and 0.93, and the RMSE values are 0.47, 1.62, and 1.37, respectively. The S/N values for the 9.4, 13.1, and 33.5 nm channels of SDO/AIA are 0.37, 0.85, and 0.47, and those of the FCN model results are
2012-03-08T00:00

![img-3.jpeg](img-3.jpeg)

**Figure 4.** A comparison of 13.1 nm EUV data from SDO/AIA and our models at 2012 March 8 00:00 UT. The first row represents the enlarged plots inside the white solid-line boxes in Figure 3. The second row represents the intensity profiles on the white lines in the first row. The black, red, and green lines represent the AIA, FCN, and CNN, respectively.

**Table 1** The Correlation Coefficient (CC), rms Error (RMSE), and Signal-to-noise Ratio (S/N) Values from our FCN Model Result Using the Test Data Set and Figures 5–8

|   | Test Data Set | Figure 3 | Figure 5 | Figure 6 | Figure 7 | Figure 8  |
| --- | --- | --- | --- | --- | --- | --- |
|   |  | (Full Sun) | (Loop on the Disk) | (Bright Patch) | (Loop on the Limb) | AIA versus FCN  |
|  9.4 nm | CC | 0.78 ± 0.11 | 0.71 | 0.75 | 0.63 | 0.92  |
|   | RMSE | 1.18 ± 1.04 | 1.71 | 2.81 | 0.52 | 0.95  |
|   | S/N_AIA | 1.37 ± 0.26 | 1.29 | 0.73 | 3.97 | 8.64  |
|   | S/N_FCN | 23.20 ± 17.50 | 26.68 | 19.04 | 109.14 | 194.47  |
|  13.1 nm | CC | 0.89 ± 0.08 | 0.90 | 0.88 | 0.85 | 0.94  |
|   | RMSE | 4.14 ± 4.85 | 3.68 | 6.89 | 1.60 | 6.15  |
|   | S/N_AIA | 3.03 ± 0.57 | 3.10 | 3.43 | 11.56 | 17.89  |
|   | S/N_FCN | 16.32 ± 3.37 | 17.46 | 19.77 | 53.59 | 114.90  |
|  33.5 nm | CC | 0.85 ± 0.09 | 0.84 | 0.84 | 0.90 | 0.96  |
|   | RMSE | 3.58 ± 1.24 | 4.92 | 7.19 | 1.10 | 5.23  |
|   | S/N_AIA | 2.35 ± 0.98 | 3.62 | 2.53 | 16.17 | 18.36  |
|   | S/N_FCN | 14.31 ± 5.49 | 16.01 | 10.77 | 94.03 | 204.30  |

**Note.** We consider pixels within 1.2 solar radii for all images and calculate these values in the original data unit.

3.60, 3.92, and 2.06, respectively. As shown in Figure 8, the target channel images from SDO/AIA are very noisy, but our FCN model greatly reduces low signals near noise levels, which are demonstrated by S/N values. As shown in the last plot in Figure 8, there is a noticeable difference between the DEMs from DEMSET-AIA and DEMSET-FCN, especially in the region over 6.0 log *T*. There are two possible explanations for this difference. The first is that noise pixels are included in
![img-4.jpeg](img-4.jpeg)

Figure 5. A comparison example between SDO/AIA EUV images and model results at 2012 March 8 00:00 UT. The first row represents the three channels (17.1, 19.3, and 21.1 nm) from SDO/AIA, the second row represents the three channels (9.4, 13.1, and 33.5 nm) from SDO/AIA, the third row represents the three channels (9.4, 13.1, and 33.5 nm) from our FCN model results, and the white solid line is the ROI selected for the DEM calculation. The last plot represents the DEMs. The red line represents the DEMs from DEMSET-AIA, the blue line represents the DEMs from DEMSET-FCN, and the error bars represent the DEM error defined in Morgan & Pickering (2019).

The DEMs, and the other is the effect of stray lights. In order to demonstrate whether our method is reliable, we compare the results with 50 frames (10 minutes) stacked SDO/AIA EUV images. The CC values between our FCN model results and 50 frames stacked SDO/AIA data for the 9.4, 13.1, and 33.5 nm channels are 0.97, 0.94, and 0.95, and the RMSE values are 0.14, 1.15, and 1.17, respectively. To demonstrate the effect of the noise, we consider an additional DEMSET, three channels (17.1, 19.3, and 21.1 nm) from SDO/AIA and three channels (9.4, 13.1, and 33.5 nm) from 50 frames stacked SDO/AIA (hereafter DEMSET-STACK). It is noted that the differences between the DEMs from DEMSET-STACK and DEMSET-FCN are significantly reduced. Also, the fact that the DEMs from DEMSET-FCN are similar to those from DEMSET-
![img-5.jpeg](img-5.jpeg)

Figure 6. A comparison example between SDO/AIA EUV data and model results at 2012 March 8 00:00 UT. The layout of this figure is same as Figure 5.

STACK shows that our FCN model successfully removes the noise in quiet and dark areas. It is in the same vein that DEM calculations of the equatorial coronal hole using only SDO/ AIA show the hot components caused by scattered lights, not the actual coronal hole emission, and the components can be corrected by deconvolution methods (Wendeln & Landi 2018; Saqri et al. 2020). In addition, our method has the advantage of maintaining a high time cadence without stacking. Our result shows the possibility for the DEM calculations of the dark area without deconvolution or stacking EUV data if we use the generated EUV data from our FCN model.

## 4. Summary and Conclusion

In this study, we have suggested a pixel-to-pixel image translation method among similar types of filtergrams such as solar EUV images. For this, we have considered a deep-learning model based on an FCN in which all pixels of solar EUV images are independent of one another. We have used images from three EUV channels (17.1, 19.3, and 21.1 nm) of SDO/AIA as the input data, and those from the remaining three EUV channels (9.4, 13.1, and 33.5 nm) of SDO/AIA as the target data for the training and test. We have applied our model to representative solar structures in SDO/AIA data and then
![img-6.jpeg](img-6.jpeg)

Figure 7. A comparison example between SDO/AIA EUV images and model results at 2018 May 29 00:00 UT. The layout of this figure is the same as Figure 5.

determined DEMs. Our model generates three EUV channels (9.4, 13.1, and 33.5 nm) with average correlation coefficient values of 0.78, 0.89, 0.85, respectively. Our model generates the solar EUV data with no boundary effects and clearer identification of small structures when compared to a CNN-based deep-learning model. This is a very strong advantage of image translation under the assumption that pixels are independent of one another. The estimated DEMs from AI-generated data by our model are consistent with those from only SDO/AIA channel data. For a region in the coronal hole, the estimated DEMs from AI-generated data by our model are more consistent with those from the 50 frames stacked data than those from single-frame data. This shows the possibility for the DEM calculations of the dark area without deconvolution or stacking EUV data if we use the generated EUV data from our FCN model.

Now we want to discuss the significance and implications of our results in several respects. First, our results show that the data generated by deep learning are scientifically applicable, especially for producing DEMs. Although the development speed of deep learning is very fast and its accuracy is also increasing rapidly, some people still doubt whether the
![img-7.jpeg](img-7.jpeg)

Figure 8. A comparison example between SDO/AIA EUV images and model results at 2012 April 9 12:00 UT. The first row represents the three channels (17.1, 19.3, and 21.1 nm) from SDO/AIA, the second row represents the three channels (9.4, 13.1, and 33.5 nm) from SDO/AIA, the third row represents the three channels (9.4, 13.1, and 33.5 nm) from our FCN model results, and the fourth row represents the three channels (9.4, 13.1, and 33.5 nm) from 50 frames stacked SDO/AIA. The white solid line is the ROI selected for the DEM calculation. The last plot represents the DEMs. The red line represents the DEMs from DEMSET-AIA, the blue line represents the DEMs from DEMSET-FCN, the green line represents the DEMs from DEMSET-STACK, and the error bars represent the DEM error defined in Morgan & Pickering (2019).
application of deep learning to scientific computation (e.g., DEMs) is meaningful or not. In this study, we show, through the results of applying the actual observation data and the data generated by deep learning to scientific techniques, that the deep-learning data did not significantly differ from the real data. In some cases, they are even better because the deeplearning model is able to reduce the noise from several causes. Second, our results present a guideline for selecting a deeplearning model for image translation. Most studies on solar image translation, such as translation between solar magnetograms and solar EUV images, use CNN models. That is because the convolution layer is effective in extracting features from image data. In this study, we have shown that our FCN model, which is much simpler and lighter than the general CNN model, successfully translated solar EUV images, and in particular, it has been shown that the limitations of the CNN model can be partially supplemented. As an additional perspective, our results may be used for selecting a few channels of solar EUV images in small satellite missions (Lim et al. 2021). Our method can be used to generate EUV images of some channels not included in the small solar satellite missions for further analysis such as DEM.

We think that our method is applicable to the entire EUV temperature range and to different instruments, even though further examinations are necessary. In the case of flares and jets, which are high-temperature and transient phenomena, our method is still effective only when flaring parts in the input images are not saturated. For the next project, it will be investigated whether the model can be used for hightemperature material such as flares or jets.

This work was supported by the Basic Science Research Program through the National Research Foundation (NRF) funded by the Ministry of Education (NRF-2020R1C1C1003892, NRF2019R1A2C1002634, NRF-2021R1I1A1A01049615, NRF2020R1A2C2004616, NRF-2021R1A2C1010881, and NRF2022R1A2C1004398), and the Korea Astronomy and Space Science Institute (KASI) under the R\&D program (project No. 2022-1-850-05) supervised by the Ministry of Science and ICT. J.-Y. Lee is supported by the Basic Science Research Program (NRF-2020R1I1A1A01071814) through the National Research Foundation (NRF) funded by the Ministry of Education of Korea and the Air Force Office of Scientific Research under award No. FA2386-20-1-4031. We thank the numerous team members who have contributed to the success of the SDO mission. We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: astropy, sunpy, aiapy, pytorch, and numpy. The SDO data were (partly) provided by the Korea Data Center (KDC) for SDO in cooperation with NASA, Stanford University (JSOC), and KISTI (KREONET), which is supported by the ""Next Generation Space Weather Observation Network"" project of the Korea Astronomy and Space Science Institute (KASI).

Software: astropy (Astropy Collaboration et al. 2013), sunpy (Mumford et al. 2020; The SunPy Community et al. 2020),
aiapy (Barnes et al. 2020), pytorch (Paszke et al. 2019), numpy (Harris et al. 2020).

## ORCID iDs

Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Harim Lee (2) https://orcid.org/0000-0002-9300-8073
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Jin-Yi Lee (2) https://orcid.org/0000-0001-6412-5556
Il-Hyun Cho (2) https://orcid.org/0000-0001-7514-8171
Kyoung-Sun Lee (2) https://orcid.org/0000-0002-4329-9546
Daye Lim (2) https://orcid.org/0000-0001-9914-9080
Hyun-Jin Jeong (2) https://orcid.org/0000-0003-4616-947X
Jae-Ok Lee (2) https://orcid.org/0000-0002-7652-7883

## References

Astropy Collaboration, Robitaille, T. P., Tollerud, E. J., et al. 2013, A\&A, 558, A33
Barnes, W., Cheung, M., Bobra, M., et al. 2020, JOSS, 5, 2801
Boerner, P., Edwards, C., Lemen, J., et al. 2012, SoPh, 275, 41
Delaboudiniere, J.-P., Artzner, G. E., Brunaud, J., et al. 1995, SoPh, 162, 291
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Galvez, R., Fouhey, D. F., Jin, M., et al. 2019, ApJS, 242, 7
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Handy, B. N., Acton, L. W., Kankelborg, C. C., et al. 1999, SoPh, 187, 229
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585, 357
Howard, R. A., Moses, J. D., Vourlidas, A., et al. 2008, SSRv, 136, 67
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2016, arXiv:1611.07004
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Kaiser, M. L., Kucera, T. A., Davila, J. M., et al. 2008, SSRv, 136, 5
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kosugi, T., Matsuzaki, K., Sakao, T., et al. 2007, SoPh, 243, 3
Lee, H., Park, E., \& Moon, Y. J. 2021, ApJ, 907, 118
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Lim, D., Moon, Y.-J., Park, E., \& Lee, J.-Y. 2021, ApJL, 915, L31
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Morgan, H., \& Pickering, J. 2019, SoPh, 294, 135
Mumford, S., Freij, N., Christe, S., et al. 2020, JOSS, 5, 1832
O'Dwyer, B., Del Zanna, G., Mason, H. E., Weber, M. A., \& Tripathi, D. 2010, A\&A, 521, A21
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Paszke, A., Gross, S., Massa, F., et al. 2019, in Advances in Neural Information Processing Systems 32, ed. H. Wallach et al., 8024, https://papers.nips.cc/ paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Pineci, A., Sadowski, P., Gaidos, E., \& Sun, X. 2021, ApJL, 910, L25
Radford, A., Metz, L., \& Chintala, S. 2015, arXiv:1511.06434
Ramachandran, P., Zoph, B., \& Le, Q. V. 2017, arXiv:1710.05941
Ronneberger, O., Fischer, P., \& Brox, T. 2015, arXiv:1505.04597
Saqri, J., Veronig, A. M., Heinemann, S. G., et al. 2020, SoPh, 295, 6
Scherrer, P. H., Bogart, R. S., Bush, R. I., et al. 1995, SoPh, 162, 129
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Son, J., Cha, J., Moon, Y.-J., et al. 2021, ApJ, 920, 101
Szenicer, A., Fouhey, D. F., Munoz-Jaramillo, A., et al. 2019, SciA, 5, eaaw6548
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., et al. 2017, arXiv:1711.11585
Wendeln, C., \& Landi, E. 2018, ApJ, 856, 28"
Daye Lim et al 2021 - Selection of Three (Extreme)Ultraviolet Channels for Solar Satellite Missions by Deep Learning.pdf,"# Selection of Three (Extreme)Ultraviolet Channels for Solar Satellite Missions by Deep Learning 

Daye Lim ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), Eunsu Park ${ }^{1}$ (D), and Jin-Yi Lee ${ }^{1}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, Kyung Hee University, 1732, Deogyeong-daero, Giheung-gu, Yongin-si, Gyeonggi-do 17104, Republic of Korea moonyj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, 1732, Deogyeong-daero, Giheung-gu, Yongin-si, Gyeonggi-do 17104, Republic of Korea Received 2021 March 30; revised 2021 June 17; accepted 2021 June 21; published 2021 July 13


#### Abstract

We address the question of which combination of channels can best translate other channels in ultraviolet (UV) and extreme UV (EUV) observations. For this, we compare the image translations among the nine channels of the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO) using a deep-learning (DL) model based on conditional generative adversarial networks. In this study, we develop 170 DL models: 72 models for single-channel input, 56 models for double-channel input, and 42 models for triple-channel input. All models have a single-channel output. Then we evaluate the model results by pixel-to-pixel correlation coefficients (CCs) within the solar disk. Major results from this study are as follows. First, the model with $131 \AA$ shows the best performance (average $\mathrm{CC}=0.84$ ) among single-channel models. Second, the model with 131 and $1600 \AA$ shows the best translation (average $\mathrm{CC}=0.95$ ) among double-channel models. Third, among the triple-channel models with the highest average $\mathrm{CC}(0.97)$, the model with $131,1600$, and $304 \AA$ is suggested in that the minimum CC $(0.96)$ is the highest. Interestingly, they represent coronal, upper photospheric, and chromospheric channels, respectively. Our results may be used as a secondary perspective in addition to primary scientific purposes in selecting a few channels of an UV/EUV imaging instrument for future solar satellite missions.


Unified Astronomy Thesaurus concepts: The Sun (1693); Solar instruments (1499); Solar ultraviolet emission (1533); Solar extreme ultraviolet emission (1493); Convolutional neural networks (1938)

## 1. Introduction

Over the past 25 yr , ultraviolet (UV) and extreme UV (EUV) phenomena of the Sun have been continuously observed in multi-wavelengths. The Solar and Heliospheric Observatory (SOHO; Domingo et al. 1995), launched in 1995, observed the low corona in four channels of 171, 195, 284, and $304 \AA$ of the Extreme ultraviolet Imaging Telescope (EIT; Delaboudinière et al. 1995). The Transition Region and Coronal Explorer (TRACE; Handy et al. 1999), launched in 1998, provided observations from the solar photosphere to the upper atmosphere in seven channels of $1700,1600,1550,1216,284,195$, and $171 \AA$. The Extreme Ultraviolet Imager (EUVI; Wuelser et al. 2004; Howard et al. 2008) on board the Solar Terrestrial Relationships Observatory (STEREO; Kaiser et al. 2008), launched in 2006, has made it possible to observe the farside of the Sun in 171, 195, 284, and $304 \AA$, similar to SOHO/EIT. Launched in 2010, the Atmospheric Imaging Assembly (AIA; Lemen et al. 2012) on board the Solar Dynamics Observatory (SDO; Pesnell et al. 2012) has provided high-resolution fulldisk images with high time cadences of about 12 s . The AIA has nine channels of $1700,1600,335,304,211,193,171,131$, and $94 \AA$.

Such a large amount of single- or multi-channel paired data allows us to do various applications of deep learning (DL), one of the artificial intelligence (AI) models. First, super-resolution methods that enhance the original resolution of data have been developed (Díaz Baso \& Asensio Ramos 2018; Jia et al. 2019; Rahman et al. 2020). Second, reconstruction and synthesis of data have been considered (Felipe \& Asensio Ramos 2019; Galvez et al. 2019; Kim et al. 2019; Park et al. 2019; Salvatelli et al. 2019; Szenicer et al. 2019; Jeong et al. 2020; Shin et al. 2020; Zhang et al. 2020; Lee et al. 2021a). Third, processes of
reducing noise in data have been proposed (Díaz Baso et al. 2019; Park et al. 2020). Fourth, predictions of future data have been studied (Galvez et al. 2019; Ji et al. 2020; Lee et al. 2021b).

There are two types of approaches in the reconstruction and synthesis of solar data. First, several authors have studied the translation between data from currently operated instruments. Galvez et al. (2019) translated Helioseismic and Magnetic Imager (HMI) vector magnetograms into AIA nine-channel observations. Park et al. (2019) also generated AIA ninechannel images using HMI line-of-sight magnetograms. Szenicer et al. (2019) mapped from AIA images to spectral irradiance measurements. Salvatelli et al. (2019) reconstructed one AIA channel from three other channels among 94, 171, 193, and $211 \AA$. Second, there have been several studies using DL models to compensate for the absence of observational data. Kim et al. (2019) generated farside magnetograms from STEREO/EUVI $304 \AA$. Felipe \& Asensio Ramos (2019) developed a method to improve the quality of farside seismic maps by using HMI magnetograms and farside phase-shift maps. Jeong et al. (2020) constructed extrapolated global magnetic fields using observed frontside and DL-generated farside magnetograms from STEREO/EUVI 171, 195, and $304 \AA$. Shin et al. (2020) generated past HMI-like magnetograms using Ca II K images from the Rome Observatory. Lee et al. (2021a) generated HMI-like magnetograms and AIA-like (E)UV images in 1612 using Galileo sunspot drawings. Zhang et al. (2020) generated Nobeyama Radioheliograph image from SDO/AIA 171, 193, 211, 304, and $335 \AA$.

The above studies presented a sufficient possibility of a ""virtual observatory"" that can generate real-like observational data sets (Salvatelli et al. 2019). In this study, we address a question of what combination of channels can best translate
other channels in UV and EUV observations. For this, we use data from the nine SDO/AIA channels. Here, we consider three types of DL models: (1) 72 models with single-channel input and single-channel output, (2) 56 models with double-channel input and single-channel output, and (3) 42 models with triplechannel input and single-channel output. We also compare the performance of these 170 models in view of a metric. This Letter is organized as follows. In Section 2, we describe the data and method. Section 3 gives the results of the translations with discussion. A brief summary and conclusion are given in Section 4.

## 2. Data and Method

SDO/AIA is so far the most suitable instrument to consider a large number of multi-(E)UV observations. Thus, we use the AIA observations in two UV channels ( 1600 and $1700 \AA$ ) and seven EUV channels ( $94,131,171,193,211,304$, and $335 \AA$ ) with 12 hr cadence from 2011 to 2017. These nine channels observe from the photosphere to the corona and cover the plasma temperature between about 5000 K and 20 MK (Lemen et al. 2012). All data are pre-processed by calibrating, rotating, centering, exposure correction, and degradation correction as mentioned in Park et al. (2019). All nine-channel data are set to have the same solar disk size, scaled down to 1024 by 1024.

Considering the solar cycle phase, we divide the data set into training, validation, and test as follows. Data for September and October of each year are used for the test, and data for July and August of each year for the validation, thereby separating the training and test sets. The remaining data are for training. The number of data per channel for training, validation, and testing are $3,299,825$, and 821 , respectively.

We adjust the dynamic range of each wavelength using the aia_intscale. pro routine to make physically important features of the Sun stand out well at each wavelength. For this reason, the model outputs could be used for quantitative study within the limited dynamic range. Note that it takes much time to set up 170 DL models with full dynamic ranges, which is reserved for a future study. After the dynamic range is adjusted for each channel, all the pixel values are scaled by 255 . Then, they are considered as the input of the model. The output of the model is data with values between 0 and 255 like the input. Between inputs and outputs, the data is scaled between -1 and 1 and processed. The output data is inversely transformed to the dynamic range for each channel before calculating a metric in the test set.

In this study, three types of models are considered as follows. We first consider models that translate from single-channel input to single-channel output. The number of single-channel models for nine channels is 72 , which can be calculated using a permutation that selects two out of nine elements without overlapping, $q P_{2}$. Among the single-channel models, we select one channel, referred to as Channel- $A$, that best translates the other channels on average. Second, we consider models that translate from double-channel input to single-channel output while keeping Channel- $A$ as an input. The number of double-channel models is $56, q P_{2}$. Then, among the double-channel models, we select one combination, referred to as Channel- $A+$ Channel- $B$, showing the best performance. Lastly, we consider models that translate from triple-channel input to single-channel output while keeping Channel- $A$ and Channel- $B$ as an input. The number of triplechannel models is $42,7 P_{2}$. A total of 170 models are trained for this study.

We consider a DL method called ""Model B"" in Park et al. (2019) based on pix2pix of Isola et al. (2017) with two networks ""generator"" and ""discriminator"". A detailed description of this method is presented in GitHub ${ }^{3}$ (Park 2019) and the Appendix in Park et al. (2019). We use the same hyperparameters as in Appendix C of Park et al. (2019). Because this method is for single-channel input, we modify it to be able to input multiple channels for this study.

## 3. Results and Discussion

We compare 170 models using the average pixel-to-pixel correlation coefficient (CC) between DL model outputs and the corresponding real AIA images within the solar disk for the test data set. The first to the ninth rows in Table 1 shows the average CC values for single-channel models. The rms errors between the CC value of each test data and the average CC value are represented as uncertainties. The channels that best translate $94,131,171,193,211$, and $335 \AA$ are $335,171,131$, 211, 193, and 211, respectively. They are all EUV channels. This is not surprising in that the peak temperatures of 131 and $171 \AA$ (about 0.4 MK at lower peak temperature and 0.6 MK , respectively), and 193 and $211 \AA$ (about 1.6 MK and 2 MK , respectively) in the response function are almost similar to each other (Lemen et al. 2012). Likewise, each UV channel has the best performance to generate the other UV channel. The $304 \AA$ has a higher average CC with the EUV channels than the UV channels. Among the nine channels, $131 \AA$ is considered as the Channel- $A$ as it gives the highest average CC. Another interesting result is as follows. The average CC $(0.59)$ for the translation from the higher atmospheric channels ( $94,131,171$, $193,211$, and $335 \AA$ ) to the lower atmospheric channels ( 1600 and $1700 \AA$ ) is lower than that $(0.78)$ for the translation from the lower atmospheric channels to the higher atmospheric channels. The result implies that observations from the lower atmosphere of the Sun may have more information to infer the characteristics of the higher atmosphere. This can be understood from the origin of the solar activity coming from lower atmosphere.

The tenth to the seventeenth rows in Table 1 shows the average CC values with their uncertainties for double-channel models with $131 \AA$ and another channel as inputs. The performances of the models are much better than the singlechannel models. The average CCs of single-channel models with EUV channels are distributed between 0.8 and 0.84 , whereas those of double-channel models are between 0.9 and 0.91 . Since the combination of 131 and $1600 \AA$ best translates other channels on average, $1600 \AA$ is selected as the Channel- $B$. The eighteenth to the last rows in Table 1 show the average CCs for triple-channel models with $131,1600 \AA$ and another channel as inputs. The performances of these models are all similar or better than those of the single- and double-channel models. The triple-channel models with the input including $171,193,211,335$, and $304 \AA$ give the highest average CC of 0.97 . Among these, the model with $131,1600$, and $304 \AA$ is selected as the best model because it has the highest minimum CC $(0.96)$.

Interestingly, the three channels are representative coronal, upper photospheric, and chromospheric channels, respectively. Of the three channels, the $1600 \AA$ has been considered in TRACE and SDO, and $304 \AA$ has been considered in SOHO,

[^0]
[^0]:    3 https://github.com/eunsu-park/solar_euv_generation
Table 1 The Average Pixel-to-pixel CCs between the 170 DL Model Outputs and the Corresponding Real AIA Images, Calculated for the Test Data Set

| Input | Output |  |  |  |  |  |  |  |  | Average |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | 94 | 131 | 171 | 193 | 211 | 335 | 304 | 1600 | 1700 |  |
| 94 | ... | $0.88 \pm 0.03$ | $0.86 \pm 0.02$ | $0.95 \pm 0.02$ | $0.92 \pm 0.03$ | $\mathbf{0 . 9 3} \pm \mathbf{0 . 0 3}$ | $0.76 \pm 0.07$ | $0.45 \pm 0.05$ | $0.65 \pm 0.04$ | 0.80 |
| 131 | $0.93 \pm 0.04$ | ... | $\mathbf{0 . 9 3} \pm \mathbf{0 . 0 2}$ | $0.92 \pm 0.03$ | $0.91 \pm 0.02$ | $0.92 \pm 0.03$ | $\mathbf{0 . 8 6} \pm \mathbf{0 . 0 3}$ | $0.54 \pm 0.04$ | $0.68 \pm 0.02$ | 0.84 |
| 171 | $0.91 \pm 0.04$ | $\mathbf{0 . 9 2} \pm \mathbf{0 . 0 3}$ | ... | $0.94 \pm 0.01$ | $0.89 \pm 0.02$ | $0.88 \pm 0.03$ | $0.84 \pm 0.03$ | $0.51 \pm 0.03$ | $0.68 \pm 0.03$ | 0.82 |
| 193 | $0.94 \pm 0.04$ | $0.89 \pm 0.03$ | $0.86 \pm 0.03$ | ... | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 1}$ | $0.92 \pm 0.03$ | $0.83 \pm 0.05$ | $0.48 \pm 0.05$ | $0.78 \pm 0.03$ | 0.83 |
| 211 | $0.94 \pm 0.03$ | $0.87 \pm 0.03$ | $0.79 \pm 0.03$ | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 1}$ | ... | $0.95 \pm 0.03$ | $0.85 \pm 0.04$ | $0.46 \pm 0.04$ | $0.67 \pm 0.04$ | 0.81 |
| 335 | $\mathbf{0 . 9 5} \pm \mathbf{0 . 0 2}$ | $0.89 \pm 0.02$ | $0.80 \pm 0.03$ | $0.93 \pm 0.02$ | $0.94 \pm 0.02$ | ... | $0.85 \pm 0.03$ | $0.53 \pm 0.04$ | $0.68 \pm 0.03$ | 0.82 |
| 304 | $0.91 \pm 0.03$ | $0.80 \pm 0.04$ | $0.75 \pm 0.03$ | $0.90 \pm 0.02$ | $0.92 \pm 0.02$ | $0.92 \pm 0.03$ | ... | $0.62 \pm 0.03$ | $0.71 \pm 0.02$ | 0.82 |
| 1600 | $0.85 \pm 0.05$ | $0.76 \pm 0.04$ | $0.69 \pm 0.05$ | $0.78 \pm 0.04$ | $0.80 \pm 0.04$ | $0.81 \pm 0.06$ | $0.73 \pm 0.06$ | ... | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 0 4}$ | 0.80 |
| 1700 | $0.84 \pm 0.05$ | $0.75 \pm 0.05$ | $0.68 \pm 0.04$ | $0.77 \pm 0.05$ | $0.79 \pm 0.04$ | $0.81 \pm 0.06$ | $0.71 \pm 0.06$ | $\mathbf{0 . 9 4} \pm \mathbf{0 . 0 1}$ | ... | 0.79 |
| Average | 0.91 | 0.85 | 0.80 | 0.89 | 0.89 | 0.89 | 0.80 | 0.57 | 0.73 |  |
| $131+94$ | ... | ... | $0.96 \pm 0.02$ | $0.98 \pm 0.01$ | $0.96 \pm 0.02$ | $0.96 \pm 0.02$ | $0.91 \pm 0.03$ | $0.72 \pm 0.02$ | $0.82 \pm 0.02$ | 0.90 |
| $131+171$ | $0.96 \pm 0.02$ | ... | ... | $0.97 \pm 0.01$ | $0.97 \pm 0.01$ | $0.96 \pm 0.02$ | $0.92 \pm 0.02$ | $0.72 \pm 0.02$ | $0.82 \pm 0.02$ | 0.90 |
| $131+193$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | ... | $0.98 \pm 0.01$ | $0.96 \pm 0.02$ | $0.92 \pm 0.02$ | $0.72 \pm 0.03$ | $0.81 \pm 0.02$ | 0.90 |
| $131+211$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | $0.97 \pm 0.004$ | ... | $0.97 \pm 0.01$ | $0.93 \pm 0.02$ | $0.72 \pm 0.03$ | $0.81 \pm 0.03$ | 0.91 |
| $131+335$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | $0.97 \pm 0.01$ | $0.97 \pm 0.01$ | ... | $0.93 \pm 0.02$ | $0.72 \pm 0.03$ | $0.81 \pm 0.03$ | 0.91 |
| $131+304$ | $0.96 \pm 0.02$ | ... | $0.97 \pm 0.02$ | $0.96 \pm 0.01$ | $0.96 \pm 0.01$ | $0.97 \pm 0.02$ | ... | $0.75 \pm 0.03$ | $0.83 \pm 0.02$ | 0.91 |
| $131+1600$ | $0.96 \pm 0.02$ | ... | $0.95 \pm 0.02$ | $0.95 \pm 0.02$ | $0.94 \pm 0.02$ | $0.95 \pm 0.02$ | $0.92 \pm 0.02$ | ... | $0.98 \pm 0.003$ | 0.95 |
| $131+1700$ | $0.96 \pm 0.02$ | ... | $0.95 \pm 0.02$ | $0.95 \pm 0.01$ | $0.94 \pm 0.02$ | $0.94 \pm 0.02$ | $0.91 \pm 0.02$ | $0.96 \pm 0.01$ | ... | 0.94 |
| Average | 0.96 | ... | 0.96 | 0.96 | 0.96 | 0.96 | 0.92 | 0.76 | 0.84 |  |
| $131+1600+94$ | ... | ... | $0.96 \pm 0.02$ | $0.98 \pm 0.01$ | $0.96 \pm 0.02$ | $0.96 \pm 0.02$ | $0.93 \pm 0.02$ | ... | $0.98 \pm 0.003$ | 0.96 |
| $131+1600+171$ | $0.97 \pm 0.02$ | ... | ... | $0.97 \pm 0.01$ | $0.97 \pm 0.02$ | $0.97 \pm 0.02$ | $0.93 \pm 0.02$ | ... | $0.98 \pm 0.003$ | 0.97 |
| $131+1600+193$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | ... | $0.98 \pm 0.01$ | $0.96 \pm 0.02$ | $0.93 \pm 0.02$ | ... | $0.98 \pm 0.003$ | 0.97 |
| $131+1600+211$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | $0.99 \pm 0.004$ | ... | $0.97 \pm 0.01$ | $0.94 \pm 0.02$ | ... | $0.98 \pm 0.003$ | 0.97 |
| $131+1600+335$ | $0.97 \pm 0.02$ | ... | $0.97 \pm 0.02$ | $0.97 \pm 0.01$ | $0.97 \pm 0.01$ | ... | $0.93 \pm 0.01$ | ... | $0.98 \pm 0.003$ | 0.97 |
| $131+1600+304$ | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 2}$ | ... | $0.97 \pm 0.02$ | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 1}$ | $\mathbf{0 . 9 6} \pm \mathbf{0 . 0 2}$ | $0.97 \pm 0.02$ | ... | ... | $0.98 \pm 0.003$ | 0.97 |
| $131+1600+1700$ | $0.96 \pm 0.02$ | ... | $0.96 \pm 0.02$ | $0.95 \pm 0.01$ | $0.94 \pm 0.02$ | $0.95 \pm 0.02$ | $0.92 \pm 0.01$ | ... | ... | 0.95 |
| Average | 0.97 | ... | 0.97 | 0.97 | 0.96 | 0.96 | 0.93 | ... | 0.98 |  |

Note. The rms errors between CC value of each test data and the average value are represented as uncertainties. All values are calculated within the solar disk. The bold values are the highest average CC for each wavelength.
![img-0.jpeg](img-0.jpeg)

Figure 1. Comparison of full-disk images between real (AIA observations at 00:00 UT on 2014 October 31) and AI-generated from three models. Each row from the top to the bottom shows 94, 171, 193, 211, and 335 Å, respectively. The first column shows the output results from Model 1 using single-channel (131 Å) input. The second column shows the output results from Model 2 using double-channel (131 and 1600 Å) inputs. The third column shows the output results from Model 3 using triple-channel (131, 1600, and 304 Å) inputs. The fourth column shows the real AIA images.
![img-1.jpeg](img-1.jpeg)

**Figure 2.** Comparison images between real and AI-generated from three models. The first row shows the full-disk images with the boundary for the CH (SPoCA 2453) represented in the red box in AIA 193 Å observation at 00:00 UT on 2011 September 11. The second row shows the magnified corresponding CH region. The third row shows the full-disk images with the boundary for the AR (HARP 3119) represented in the red box in AIA 171 Å observation at 00:00 UT on 2013 September 3. The fourth row shows the magnified corresponding AR. Each column is the same as in Figure 1. The pixel-to-pixel CCs of the CH region between the real image and the three DL model outputs are 0.92, 0.92, and 0.96 from the first column to the third, respectively. The pixel-to-pixel CCs of the AR region between the real image and the three DL model outputs are 0.89, 0.96, and 0.97 from the first column to the third, respectively.

STEREO, and SDO (Delaboudinière et al. 1995; Handy et al. 1999; Howard et al. 2008; Lemen et al. 2012). However, 131 Å is selected first by the SDO (Lemen et al. 2012). The reason why these three channels are selected from the perspective of the metric could be understood as follows. The 131 Å is selected first as it can observe not only the transition region but also the corona, i.e., it has double peaks at low and high temperatures (about 0.4 and 10 MK) in the response function. The 1600 Å is selected second because it can generate the 1700 Å much better than the other channels and is the most
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison cutout images between real (AIA 1700 Å observation at 00:00 UT on 2014 October 31) and AI-generated from three models. Each column is the same as in Figure 1. The pixel-to-pixel CCs between the real image and the three DL model outputs are 0.68, 0.98, and 0.98 from the first column to the third, respectively.

Unpredictable channel among the double-channel models. The 304 Å seems to be considered third because it is also the most difficult channel to be translated from the other channels of AIA among the triple-channel models. There is a possibility that the three prime channels could be selected even with the average CC results of the single-channel models. Because of the substantial difference between UV and EUV channels, one channel in the UV needs to be the second channel. 1600 Å can be better transferred to 1700 Å than the reverse, therefore 1600 Å is selected. Then, in order to choose the third channel among the rest of the channels, one just needs to choose the channel with the lowest average CC (0.8) column-wise (i.e., harder to be reproduced by other channels) in Table 1, which here are 171 and 304 Å. It is difficult to select one out of 171 and 304 Å by CC value alone, but 304 Å could be selected to represent the chromospheric activity. In terms of qualitative scientific viewpoints, another combination could be selected. For example, if a scientific target is coronal holes (CHs), then the 193 Å is preferable as the first selection channel because CHs are well detected in 193 Å (O'Dwyer et al. 2010; Caplan et al. 2016). The remaining two channels can be determined in a similar way as described above.

We compare real images and AI-generated ones by the following three models: the model with single-channel (131 Å) input (hereafter Model 1), the model with double-channel (131 and 1600 Å) input (hereafter Model 2), and the model with triple-channel (131, 1600, and 304 Å) input (hereafter Model 3). Figure 1 shows the full-disk EUV images at 00:00 UT on 2014 October 31. It is found that the EUV channels are well translated in view of the location and shape of the overall structures, even if the single-channel input is used. In addition, we compare local regions of interest such as active regions (ARs) and CHs. For this, we consider 193 and 171 Å, where CHs and coronal loops of ARs are most visible, respectively. We select 82 CHs detected by the spatial possibilistic clustering algorithm (SPoCA; Verbeeck et al. 2014) during the test period. 90 ARs detected as HMI Active Region Patch (HARP; Bobra et al. 2014) are selected during the test period between 2012 and 2014, corresponding to the solar maximum phase. The average CC within the 90 CHs is 0.84 for Model 1, 0.88 for Model 2, and 0.91 for Model 3. For 90 ARs, the average CC is 0.89 for Model 1, 0.93 for Model 2, and 0.95 for Model 3. These results show that Model 3 has performs better translations for the ARs and CHs than Model 1 and Model 2.

The first and second rows in Figure 2 shows the full-disk images and the CH regions (SPoCA 2453) of the AIA 193 Å at 00:00 UT on 2011 September 11. It is shown that Model 3 better translates the overall size and shape of the CH region than Model 1 and Model 2. The average CC within the CH region is 0.92 for Model 1, 0.92 for Model 2, and 0.96 for Model 3. As in Table 1, the single-channel that best translates 193 Å is 211 Å. Thus, 211 Å may give a better reconstruction of CHs. The third and fourth rows in Figure 2 show the full-disk images and the AR regions (HARP 3119) of the AIA 171 Å at 00:00 UT on 2013 September 3. Compared to the results of Model 1 and Model 2, the result of Model 3 gives more similar intensities to the observed intensities and shows more clear coronal loops. The average CC within the AR is 0.89 for Model 1, 0.96 for Model 2, and 0.97 for Model 3. Salvatelli et al. (2019) investigated translations from three channels into one channel among the AIA 94, 171, 193, and 211 Å. They showed that the translation error for 171 Å out of four channels is the lowest. Thus, an input combination of 94, 193, and 211 Å using our DL method may be better for 171 Å translation than Model 1, 2, and 3. Figure 3 shows the comparison between cutout real images and AI-generated ones by the three models for the AIA 1700 Å observations at 00:00 UT on 2014 October 31. The result of Model 1 does not well translate the location and size of a sunspot and faculae, but these features are greatly improved in the results of Models 2 and 3 that consider 1600 Å as the input. The average CC within the region is 0.68 for Model 1, 0.98 for Model 2, and 0.98 for Model 3.

In order to demonstrate the performance of the DL model, we consider a simple base model that is a multiple linear regression of the intensities in log scales for the 131, 1600, and 304 Å as follows.

$$
\begin{aligned}
\log I_{\text{pred, base}} &= a \log I_{\text{obs, 131}} + b \log I_{\text{obs, 1600}} \\
&+ c \log I_{\text{obs, 304}} + d,
\end{aligned}
$$

where $I$ is the intensity of the pixel, $a$, $b$, and $c$ are a slope, and $d$ is a constant of a regression. The data used to make the base model are pre-processed and scaled down to 1024 by 1024 as in the DL models, but have a full dynamic range for each channel. The data for the base model are chosen with 10 days cadence in the training data set since the use of all data are too much time consuming. For comparison with the base model, we develop a DL model using data with the full dynamic range. This model only consider the input combination of 131, 1600, and 304 Å (Model 3 that gives the highest CC among the DL models). Hereafter, Model 3 refers to the DL model considering the data with the full dynamic range.

For the quantitative comparison of the two-dimensional distribution, we consider the joint probability density function (JPDF) of the observed intensities ($I_{\text{obs}}$) and the model predicted intensities ($I_{\text{pred}}$) as suggested in Salvatelli et al. (2019). The JPDF is a function used to characterize the
![img-3.jpeg](img-3.jpeg)

Figure 4. JPDF of the observed intensities and the model predicted intensities for the base model (the first and third rows) and Model 3 (the second and fourth rows). The color bar represents the log JPDF, in which the blue color indicates the higher probability of intensity distribution within the area. If the model predicts perfectly, the JPDF would lie along the diagonal line (solid black line). The sum of JPDFs in all domains for each channel is 1.
probability distribution of a finite domain $\Delta I_{\text {obs }} \Delta I_{\text {pred }}$. The sum of JPDFs in all domains is 1 . If the model predicts perfectly, the JPDF would be lie along the diagonal line.

Figure 4 shows the JPDF of the observed intensities and model predicted intensities in the test set for the base model and Model 3. For all six channels, the results of Model 3 are much more distributed near the diagonal line than those of the base model. This indicates that Model 3 can translate the intensity more precisely within the corresponding range than the base model. To quantify the distribution, we calculate the sum of JPDFs along the diagonal $\left(\sum_{i=1} \operatorname{JPDF}(i, j)\right)$. A higher value means better translation within the observed intensity range. For the base model, $\sum_{i=1} \operatorname{JPDF}(i, j)$ are $0.18,0.2,0.16,0.11$, 0.17 , and 0.42 for $94,171,193,211,335$, and $1700 \AA$, respectively. For Model $3, \sum_{i=1} \operatorname{JPDF}(i, j)$ are $0.2,0.29,0.32$, $0.31,0.23$, and 0.52 for $94,171,193,211,335$, and $1700 \AA$, respectively. It can be found that the performance of the DL model is better than the base model as shown in Figure 4. Among the six channels of Model 3 and the base model, the $1700 \AA$ has the higher sum of JPDFs along the diagonal line. In this case, the average $\mathrm{CC}(0.98)$ is also the highest, as shown in Table 1.

Although the DL model outperforms the base model in reproducing the UV and EUV images, the accuracy is not good enough to fully replace the real measurements based on the quantitative comparison shown in Figure 4. To be useful for reducing the telemetry of future solar missions, the DL model accuracy has to be improved. The current DL model may be further improved by considering the following. The first is to use the data of SDO/AIA with the original size 4096 by 4096. Then the information loss caused by binning could be reduced. The second is to apply the ""pix2pixHD"" method that is specifically devised for high-resolution image translation tasks (Wang et al. 2017). In addition, another DL method that can precisely translate the coronal loop strand should be considered in the future.

## 4. Summary and Conclusion

In this Letter, we have addressed the question of what combination of channels can best translate other channels in UV and EUV observations. For this, we considered the SDO/ AIA observations in two UV channels ( 1600 and $1700 \AA$ ) and seven EUV channels ( $94,131,171,193,211,304$, and $335 \AA$ ) with 12 hr cadence from 2011 to 2017. We developed 170 DL models based on pix2pix: 72 models for single-channel input, 56 models for double-channel input, and 42 models for triplechannel input. All models have a single-channel output. To quantitatively compare the image translations, we evaluated the model results by pixel-to-pixel CCs within the solar disk. The real images and AI-generated ones were also presented to qualitatively compare the image translations.

The major results of this study are as follows. First, the model with $131 \AA$ gives the highest average $\mathrm{CC}(0.84)$ among the singlechannel models. Second, the model with 131 and $1600 \AA$ shows the best translation (average $\mathrm{CC}=0.95$ ) among double-channel models. Third, among the triple-channel models with the highest average $\mathrm{CC}(0.97)$, the model with $131,1600$, and $304 \AA$ has the highest minimum $\mathrm{CC}(0.96)$. The model with $131,1600$, and $304 \AA$ better generates not only the full disk of the Sun but also local regions (e.g., CHs and ARs) than the model with $131 \AA$ and the model with 131 and $1600 \AA$. The average $\mathrm{CC}(0.98)$ for the $1700 \AA$ translation by the model with 131 and $1600 \AA$ is more
significantly improved than that ( 0.68 ) by the model with $131 \AA$. The DL model with 131, 1600, and $304 \AA$ translates the intensity more precisely than the base model using the multiple linear regression with the same combination of channels.

In this study, we compared 170 DL translation models using 24 combinations of input channels. From this, we presented the combination of channels that best translates the other channels. Our results may be used as a secondary perspective in addition to primary scientific purposes in selecting a few channels of an UV/EUV imaging instrument for future solar satellite missions. Our study could help design a DL model for a specific scientific target. We focused on the image translation performance depending on input combinations using one DL method. According to scientific targets, another DL method using multi-channel input may be considered (e.g., a DL method for translation of the coronal loops). Although a large number of models have been developed and compared, we do not consider all input combinations. Thus, future studies of other combinations not dealt with in this study may be needed. As mentioned in Section 3, the current DL model could be improved in the future, and it will show the possibility of quantitative analysis such as differential emission measure.

We thank the reviewer for the helpful comments. This work was supported by the Basic Science Research Program (BSRP) through the National Research Foundation (NRF) funded by the Ministry of Education (NRF-2019R1A2C1002634), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program (project No. 2021-1-850-05) supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). D.L. acknowledges support from the BSRP through the NRF funded by the Ministry of Education (NRF-2021R1I1A1A01040372). E.P. acknowledges support from the BSRP through the NRF funded by the Ministry of Education (NRF-2020R1C1C1003892). J.Y.L. acknowledges support from the BSRP through the NRF funded by the Ministry of Education (NRF-2020R1I1A1A01071814) and the Air Force Office of Scientific Research under award number FA2386-20-14031. We thank the numerous team members who have contributed to the success of the SDO mission. We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: NumPy (numpy.org), Keras (keras.io), and SolarSoftWare.

Facility: SDO/AIA (Lemen et al. 2012).
Software: SolarSoftWare (Freeland \& Handy 1998), Keras, NumPy (van der Walt et al. 2011; Harris et al. 2020), solar_euv_generation (Park et al. 2019).

## ORCID iDs

Daye Lim (2) https://orcid.org/0000-0001-9914-9080
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Jin-Yi Lee (2) https://orcid.org/0000-0001-6412-5556

## References

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SolPh, 289, 3549
Caplan, R. M., Downs, C., \& Linker, J. A. 2016, ApJ, 823, 53
Delaboudinière, J. P., Artzner, G. E., Brunaud, J., et al. 1995, SolPh, 162, 291
Díaz Baso, C. J., \& Asensio Ramos, A. 2018, A\&A, 614, A5
Díaz Baso, C. J., de la Cruz Rodríguez, J., \& Danilovic, S. 2019, A\&A, 629, A99
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Felipe, T., \& Asensio Ramos, A. 2019, A\&A, 632, A82
Freeland, S. L., \& Handy, B. N. 1998, SoPh, 182, 497
Galvez, R., Fouhey, D. F., Jin, M., et al. 2019, ApJS, 242, 7
Handy, B. N., Acton, L. W., Kankelborg, C. C., et al. 1999, SoPh, 187, 229
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585, 357
Howard, R. A., Moses, J. D., Vourlidas, A., et al. 2008, SSRv, 136, 67
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2017, in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (New York: IEEE), 17355135
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Ji, E.-Y., Moon, Y.-J., \& Park, E. 2020, SpWea, 18, e02411
Jia, P., Huang, Y., Cai, B., \& Cai, D. 2019, ApJL, 881, L30
Kaiser, M. L., Kucera, T. A., Davila, J. M., et al. 2008, SSRv, 136, 5
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Lee, H., Park, E., \& Moon, Y.-J. 2021a, ApJ, 907, 118
Lee, S., Ji, E.-Y., Moon, Y.-J., \& Park, E. 2021b, SpWea, 19, e2600

Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
O'Dwyer, B., Del Zanna, G., Mason, H. E., Weber, M. A., \& Tripathi, D. 2010, A\&A, 521, A21
Park, E. 2019, eunsu-park/solar_euv_generation: solar_euv_generation, Zenodo, doi:10.5281/zenodo. 3457777
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, E., Moon, Y.-J., Lim, D., \& Lee, H. 2020, ApJL, 891, L4
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Rahman, S., Moon, Y.-J., Park, E., et al. 2020, ApJL, 897, L32
Salvatelli, V., Bose, S., Neuberg, B., et al. 2019, arXiv:1911.04006
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Szenicer, A., Fouhey, D. F., Munoz-Jaramillo, A., et al. 2019, SciA, 5, eaaw6548
van der Walt, S., Colbert, S. C., \& Varoquaux, G. 2011, CSE, 13, 22
Verbeeck, C., Delouille, V., Mampaey, B., \& De Visscher, R. 2014, A\&A, 561, A29
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., et al. 2017, arXiv:1711.11585
Wuelser, J.-P., Lemen, J. R., Tarbell, T. D., et al. 2004, Proc. SPIE, 5171, 111
Zhang, P.-J., Wang, C.-B., \& Pu, G.-S. 2020, RAA, 20, 204"
Eunsu Park et al 2020 - De-noising SDO HMI Solar Magnetograms by Image Translation Method Based on Deep Learning.pdf,"# De-noising SDO/HMI Solar Magnetograms by Image Translation Method Based on Deep Learning 

Eunsu Park ${ }^{1,2}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D) Daye Lim ${ }^{1,2}$ (D) and Harim Lee ${ }^{1,2}$ (D)<br>${ }^{1}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2019 October 18; revised 2020 February 7; accepted 2020 February 9; published 2020 February 26


#### Abstract

In astronomy, long-exposure observations are one of the important ways to improve signal-to-noise ratios ( $\mathrm{S} / \mathrm{Ns}$ ). In this Letter, we apply a deep-learning model to de-noise solar magnetograms. This model is based on a deep convolutional generative adversarial network with a conditional loss for image-to-image translation from a single magnetogram (input) to a stacked magnetogram (target). For the input magnetogram, we use Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI) line-of-sight magnetograms at the center of the solar disk. For the target magnetogram, we make 21 -frame-stacked magnetograms, taking into account solar rotation at the same position. We train a model using 7004 pairs of the input and target magnetograms from 2013 January to 2013 October. We then validate the model using 707 pairs from 2013 November and test the model using 736 pairs from 2013 December. Our results from this study are as follows. First, our model successfully denoises $S D O /$ HMI magnetograms, and the de-noised magnetograms from our model are mostly consistent with the target magnetograms. Second, the average noise level of the de-noised magnetograms is greatly reduced from 8.66 to 3.21 G , and it is consistent with that of the target magnetograms, 3.21 G . Third, the average pixel-to-pixel correlation coefficient value increases from 0.88 (input) to 0.94 (de-noised), which means that the de-noised magnetograms are more consistent with the target ones than the input ones. Our results can be applied to many scientific fields in which the integration of many frames (or long-exposure observations) are used to improve the $\mathrm{S} / \mathrm{N}$.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); The Sun (1693); Astronomy data analysis (1858)

## 1. Introduction

The simplest way to reduce noise is to take longer exposures (or stack) observations. The signal-to-noise ratio ( $\mathrm{S} / \mathrm{N}$ ) is defined as the ratio of signal to noise. Photon noise increases in proportion to the square root of time, and signal increases in proportion to time. Thus, high $\mathrm{S} / \mathrm{Ns}$ can be obtained by increasing the exposure time or stacking observations (Birney et al. 2006). For example, the Hubble Space Telescope has generated ultra-deep field images by using long exposure times of up to several days (Beckwith et al. 2006). However, this method has the disadvantage of taking a lot of time to reduce noise from observed data.

There have been a few attempts to reduce the noise levels of solar magnetograms. Wang et al. (1995) and Chae et al. (2001) examined weak and small magnetic field structures such as solar intranetwork or small bipoles by integrating 4096 frames of magnetograms taken at Big Bear Solar observatory. Schrijver et al. (1997) stacked Michelson Doppler Imager (MDI) magnetograms to obtain a sequence of high-resolution magnetograms and to determine the polarity pattern on solar quiet region more clearly. DeForest (2017) presented a set of algorithms based on locally adaptive filters to reduce noise in astrophysical images including solar magnetograms.

The Solar Dynamics Observatory (SDO; Pesnell et al. 2012) is a spacecraft mission with three instruments that investigates how solar magnetic field is generated and structured, and how this stored magnetic energy is released into the heliosphere and geospace as solar wind, energetic particles, and variations in the solar irradiance (Pesnell et al. 2012). The Helioseismic and Magnetic Imager (HMI; Scherrer et al. 2012; Schou et al. 2012)
is an onboard instrument of the $S D O$, and is designed to measure the Doppler shift, intensity, and magnetic field at the solar photosphere; it has continuously observed the data near solar surface since 2010 (Scherrer et al. 2012; Schou et al. 2012). The HMI magnetogram is acquired from multiple narrow spectral bands. This instrument has two charge-coupled device (CCD) cameras: one is the ""front camera"" that observes 45 s cadence line-of-sight (LOS) magnetograms, and the other is the ""side camera"" that observes 720 s cadence vector magnetograms. Liu et al. (2012) calculated the average noise level of $S D O /$ HMI magnetograms by assuming that the noise level in a magnetogram is the standard deviation of a best-fitted Gaussian fitting function of the histogram of magnetic flux densities. They reported that the average noise level of $S D O /$ HMI 45 s magnetograms is about 10.2 G and that of 720 s magnetograms is about 6.3 G .

Recently, a deep neural network (DNN; Lecun et al. 2015) and machine-learning algorithm called ""Deep Learning"" has been developed. DNN is a kind of artificial neural network that has been developed to learn how humans think and recognize an object using their deep hierarchical layer structures. The convolutional neural network (CNN; Lecun et al. 1998) is the most popular deep-learning method in the field of image processing and computer vision. In general, CNN models consist of convolution filters that extract features from their data sets. The generative adversarial network (GAN; Goodfellow et al. 2014) is another popular deep-learning method used for several generative tasks. Generally, a GAN consists of two networks: one is generative network (generator) and the other is discriminative network (discriminator). The purpose of the generator is to generate realistic fake data, and
the purpose of the discriminator is to distinguish fake data from real data. Their purposes are adversarial to each other, which gives the appearance of competition between the two networks. Based on this adversarial training, we expect the generator to produce a substantial amount of realistic data that the discriminator cannot distinguish, which is the optimal state for the generator. The deep convolutional generative adversarial network (DCGAN; Radford et al. 2015) is a combined model of CNN and GAN used for stable training and output. Based on DCGAN, several methods have been suggested to solve the various types of image-generation tasks (e.g., Isola et al. 2016; Ledig et al. 2016). Kim et al. (2019) suggested a DCGAN model to generate solar magnetograms using $S D O /$ AIA images, and then applied the model to Solar Terrestrial Relations Observatory/Extreme Ultraviolet Imager (STEREO/ EUVI) images to produce solar farside magnetograms. Park et al. (2019) applied both CNN and DCGAN models to the generation of solar ultraviolet (UV) images from $S D O / \mathrm{HMI}$ LOS magnetograms, then compared the results from two models.

There have been a few attempts to apply deep learning to the de-noising of solar data. Díaz Baso et al. (2019) developed a CNN model to de-noise solar data and applied it to pairs of synthetic magnetograms from simulations with and without noise. They also applied their model to pairs of magnetograms and their deconvolved ones by Swedish 1 meter Solar Telescope. From both applications they obtained de-noised magnetograms with much less noise.

In this Letter, we apply a DCGAN model to the de-noising of solar magnetograms using real observation data sets. For the data sets, we make pairs of original $S D O / \mathrm{HMI}$ LOS magnetograms the input, and 21-frame stacked ones the target; the stacked ones have much lower noise levels. The model outputs are de-noised magnetograms and they are compared with the target magnetograms. This Letter is organized as follows. The data will be described in Section 2, and the model in Section 3. Results are given in Section 4, and a brief summary is presented in Section 5.

## 2. Data

We use $S D O /$ HMI LOS 45 s magnetograms from 2013 January to 2013 December. For the input magnetogram, we select a patch at the center of solar disk with size of 256 by 256 (about $\pm 76 . .^{\prime \prime} 8$ ). For the target magnetogram, we integrate 21 magnetograms that include 10 frames before and 10 frames after the input magnetogram considering solar rotation. The stacked magnetogram has an approximately 15 minute exposure time. We use magnetograms with a range of -100 G for minimum and 100 G for maximum. As a result, we make 8447 pairs of input and target magnetograms with a 1 hr cadence. Then we separate our data sets into training, validation, and test in chronological order. We select 707 pairs from 2013 November for the validation data set, 736 pairs from 2013 December for the test data set, and the remaining 7004 pairs for training data set.

## 3. Method

Our model is based on the model by Park et al. (2019). They modified the model of Isola et al. (2016), who suggested a general purposed solution based on a conditional generative adversarial network (cGAN; Mirza \& Osindero 2014) and

DCGAN to resolve the image-to-image translation problems. Several authors suggested that the results from DCGAN models could be more realistic than those from CNN models (Isola et al. 2016; Ledig et al. 2016). Park et al. (2019) also reported that the generated solar UV images from DCGAN models are clearer than those from CNN models in most passbands, so we follow them. More details about our model and codes are available at our GitHub repository. ${ }^{3}$

Figure 1 shows the main structure of our model based on DCGAN. The purpose of the generator $(G)$ is to generate target-like magnetograms (de-noised, hereafter) using input magnetograms. The purpose of the discriminator $(D)$ is to distinguish pairs of the input magnetograms and target ones, called ""Real Pair,"" and pairs of the input ones and de-noised ones, called ""Fake Pair."" To train our model, we use two loss functions: one is L1 loss (mean absolute error, $L_{1}$ ), which is given by

$$
L_{1}(G)=\frac{1}{N} \sum_{i}^{N}\left|M_{i}^{T}-M_{i}^{D}\right|
$$

where $i$ is a pixel number, $M^{I}, M^{T}$, and $M^{D}$ are the input, target, and de-noised magnetograms, respectively. The generator tries to minimize the $L_{1}$, which means that the generator trains itself to minimize the difference between the $M^{T}$, and $M^{D}$. The other is cGAN loss $\left(L_{\mathrm{cGAN}}\right)$, which is given by

$$
L_{\mathrm{cGAN}}(G, D)=\log \left(D\left(M^{I}, M^{T}\right)\right)+\log \left(1-D\left(M^{I}, M^{D}\right)\right)
$$

where $G$ is the generator, $D$ is the discriminator, $D\left(M^{I}, M^{T}\right)$ is the probability calculated by the discriminator using the Real Pair, and $D\left(M^{I}, M^{D}\right)$ is the probability calculated by the discriminator using the Fake Pair. The discriminator tries to maximize the $L_{\mathrm{cGAN}}$ to well distinguish between the Real Pair and the Fake Pair. On the other hand, the generator tries to minimize the $L_{\mathrm{cGAN}}$ to make the discriminator difficult to distinguish between the Real Pair and the Fake Pair. We expect the $L_{\mathrm{cGAN}}$ contributes to generating realistic de-noised magnetograms. The final loss function is given by

$$
G^{*}=\operatorname{argmin}_{G} \max _{D} L_{\mathrm{cGAN}}(G, D)+\lambda L_{1}(G)
$$

where $\lambda$ is the relative weight of the $L_{\mathrm{cGAN}}$ and the $L_{1}$. In this work, we used 100 for the relative weight, like Isola et al. (2016). To minimize or maximize the losses, we use the adaptive momentum estimation solver (Kingma \& Ba 2014) as an optimizer for both the discriminator and the generator. We save the generator in every 10,000 iterations, so we acquire 50 generator networks while the generator and the discriminator are alternatively trained for 500,000 iterations. Here one iteration refers to when one pair of images is trained in our model. In the validation step, we compare the target magnetograms with the de-noised ones by the 50 generators using the validation data set, and then we select the best model among the saved 50 generators. In the test step, we estimate the model performances of the selected generator in the validation step.

[^0]
[^0]:    3 https://github.com/eunsu-park/solar_magnetogram_denoising
![img-0.jpeg](img-0.jpeg)

Figure 1. Flowchart and structures of our proposed model. $G$ is the generator, $D$ is the discriminator, $M^{I}$ is an input magnetogram, $M^{T}$ is a target magnetogram, and $M^{D}$ is a de-noised magnetogram by the generator. The blue box is a Real Pair $\left(M^{I}, M^{T}\right)$, and the red box is a Fake Pair $\left(M^{I}, M^{D}\right)$.

## 4. Results and Discussion

Figure 2 shows input, target, and de-noised magnetograms (output), and the difference between target and de-noised magnetograms for three specific regions. As shown in Figure 2, the input magnetograms are quite noisy but the target and denoised ones are much less noisy. Impressively, the de-noised magnetograms are quite consistent with the target ones, which is also evident in the difference image between them.

To test our results, we calculate the noise levels of the input, target, and de-noised magnetograms by applying a Gaussian fitting to the histogram of magnetic flux densities. We assume the standard deviation of the Gaussian fitting as the noise level of magnetograms (Liu et al. 2004, 2012). Figure 3 shows histograms of magnetic flux densities for the input, target, and de-noised magnetograms in Figure 2. As shown in Figure 3, the histograms for the de-noised magnetograms are similar to those of the target ones, and their noise levels are almost the same. Table 1 shows the average values of the noise levels for validation and test sets. Our model significantly reduces the average noise level from 8.66 to 3.21 G , which is comparable to that of target magnetograms, 3.21 G .

In addition to the noise levels, we calculate five types of metrics between target magnetograms and de-noised (or input) ones. The first metric is the pixel CC (higher is better). The second is the RE (smaller is better) of the total unsigned
magnetic flux (TUMF, $\Phi_{i}$ ), which is given by

$$
\mathrm{RE}_{i}=\left(\Phi_{i}^{\text {Denoised }}-\Phi_{i}^{\text {Target }}\right) / \Phi_{i}^{\text {Target }}
$$

where $i$ is a serial number of test samples. This value corresponds to the overestimation $\left(\mathrm{RE}_{i}>0\right)$ or underestimation $\left(\mathrm{RE}_{i}<0\right)$ that our method attributes to the TUMF. The third metric is the linear fitting of TUMF between the target magnetograms $\left(\Phi^{T}\right)$ and the de-noised ones $\left(\Phi^{D}\right)$, which is given by $\Phi^{D}=A \Phi^{T}+B$. The fourth metric is the NMSE (smaller is better) of the magnetic field $\left(B_{j}\right)$ given by

$$
\mathrm{NMSE}_{i}=\sum\left(B_{j}^{\text {Denoised }}-B_{j}^{\text {Target }}\right)^{2} / \sum\left(B_{j}^{\text {Target }}\right)^{2}
$$

where $i$ is a serial number of test samples and $j$ is a pixel number. The last metric is the peak $\mathrm{S} / \mathrm{N}$ (higher is less noisy), which is used as a quality measurement between a original image and a compressed image, given by

$$
\operatorname{peakS} / \mathrm{N}_{\mathrm{i}}=20 \log _{10}\left(\frac{\mathrm{MAX}_{I}}{\sqrt{\mathrm{MSE}_{i}}}\right)
$$

where $i$ is a serial number of test samples, $\mathrm{MAX}_{I}$ is the length of data range, and $\mathrm{MSE}_{i}$ is the mean squared error between target and input (or de-noised) magnetograms. The peak $\mathrm{S} / \mathrm{N}$ value becomes higher when an image is less noisy and becomes zero when an image has no noise.
![img-1.jpeg](img-1.jpeg)

Figure 2. Three examples of comparisons between input, target, and de-noised magnetograms. The first column represents input magnetograms, the second column represents target magnetograms, the third column represents de-noised magnetograms from our model, and the last column represents the difference maps between denoised and target magnetograms.

Table 1
The Average Noise Levels, Pixel-to-pixel Correlation Coefficient (Pixel CC), Relative Error (RE) of the Total Unsigned Magnetic Flux (TUMF), Linear Fitting of TUMF, Normalized Mean Squared Error (NMSE), and Peak S/N of Validation and Test Data Sets

|  |   |   |   |   |   |   |   |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  Noise Level | Validation | 8.74 | 3.24 | 3.24 | 4.61 | 4.30 | 4.41  |
|   | Test | 8.66 | 3.21 | 3.21 | 4.57 | 4.27 | 4.36  |
|  Pixel CC | Validation | 0.88 | 1 | 0.94 | 0.93 | 0.95 | 0.94  |
|   | Test | 0.88 | 1 | 0.94 | 0.93 | 0.95 | 0.94  |
|  RE | Validation | 0.515 | 0 | 0.001 | 0.013 | 0.041 | 0.053  |
|   | Test | 0.529 | 0 | 0.001 | 0.012 | 0.043 | 0.053  |
|  Linear Fitting (1e20 Mx) | Validation | $0.92 \Phi^{T}+4.55$ | $\Phi^{T}$ | $0.99 \Phi^{T}+0.08$ | $0.98 \Phi^{T}+0.26$ | $0.97 \Phi^{T}+0.56$ | $0.97 \Phi^{T}+0.60$  |
|   | Test | $0.90 \Phi^{T}+4.66$ | $\Phi^{T}$ | $0.99 \Phi^{T}+0.08$ | $0.96 \Phi^{T}+0.36$ | $0.96 \Phi^{T}+0.58$ | $0.96 \Phi^{T}+0.67$  |
|  NMSE | Validation | 0.31 (0.23) | 0 | 0.12 (0.07) | 0.13 (0.09) | 0.09 (0.06) | 0.11 (0.08)  |
|   | Test | 0.31 (0.23) | 0 | 0.12 (0.07) | 0.13 (0.09) | 0.09 (0.06) | 0.12 (0.08)  |
|  Peak S/N | Validation | 28.49 | 100 | 32.53 | 32.03 | 33.36 | 32.59  |
|   | Test | 28.53 | 100 | 32.62 | 32.17 | 33.53 | 32.72  |

Note. The value in between parenthesizes corresponds to NMSE for pixels larger than the noise level.
![img-2.jpeg](img-2.jpeg)

![img-3.jpeg](img-3.jpeg)

![img-4.jpeg](img-4.jpeg)

**Figure 3.** Three examples of histograms of magnetic flux densities from input, target, and de-noised magnetograms. The red lines represent the histograms of input magnetograms, the green lines represent the histograms of target magnetograms, and the blue lines represent the histograms of de-noised magnetograms from our model. The dates and times are the same as those of Figure 2.

Table 1 shows the average values of the five metrics between target magnetograms and de-noised (input) ones. The average CC value increases from 0.88 (input) to 0.94 (de-noised), which means that the de-noised magnetograms are more consistent with the target ones than the input ones. The average RE value of the de-noised magnetograms greatly decreases from 0.529 to 0.001. In view of RE, our model slightly overestimates the TUMF but the error is quite small, about 0.1%. The linear fitting of TUMF between the target magnetograms and the de-noised ones is very close to the perfect line. The average NMSE value decreases from 0.31 to 0.12. The NMSE is sensitive to pixels having small magnetic flux densities. The average NMSE value, when we consider only the area higher than the noise level, is 0.07, which
![img-5.jpeg](img-5.jpeg)

**Figure 4.** Application of our model to a full-disk *SDO*/HMI magnetogram at 2017 September 5 00:00 UT. The first column represents original *SDO*/HMI magnetograms, and the second column represents de-noised magnetograms from our model. The noise levels of (a), (a'), (b), and (b') are 10.20, 4.05, 11.79, and 4.91 G, respectively.
becomes much smaller. The average Peak $\mathrm{S} / \mathrm{N}$ value between the target magnetograms and the de-noised ones is 32.62 dB , while the value between the target ones and the input ones is 28.53 dB . In conclusion, all four metric values are greatly improved when we consider the de-noised magnetograms generated by our deep-learning model.

We compare our results with conventional smoothing methods such as median, Gaussian, and bilateral methods. ${ }^{4}$ Optional parameters of these methods are determined by looking for the best correlations with target images in the training data sets. More details about these methods and their optional parameters are described in the Appendix. Overall metric scores from these methods are similar to those from our model. However, the average noise level of the de-noised magnetograms is noticeably smaller than those from these methods. The linear-fitting results of our method are better than those of the other methods. A visual comparison among the methods is given in the Appendix.

We have made our model by training the magnetograms taken at the solar center. To look for a possible application of our model, we apply the model to two different regions: the solar center and near the limb. Figure 4 shows an example of the application to full-disk $S D O / \mathrm{HMI}$ magnetogram at 00:00 UT on 2017 September 5. The first region, denoted by (a) and (a') in Figure 4, is located at solar center, but is four times wider area than our data sets. The second region, denoted by (b) and (b') in Figure 4, is located near the limb, but is four times wider than our data sets. A careful comparison between the input magnetogram and the de-noised one for two regions shows that noise signals in the input ones are successfully removed in the de-noised ones. The noise level in the first region decreases from 10.20 to 4.05 G , and in the second region from 11.79 to 4.91 G .

## 5. Conclusion and Summary

In this Letter, we have applied a deep-learning method based on DCGAN to the de-noising of solar magnetograms. We have selected 8447 pairs of $S D O / \mathrm{HMI}$ magnetograms as the input and their corresponding stacked magnetograms as the target. We have separated our data sets into training, validation, and test sets in chronological order. We have trained our model using 7004 pairs from 2013 January to 2013 October. Then we have validated the model using 707 pairs from 2013 November, and tested 736 pairs from 2013 December.

The main results of this study are as follows. First, our model successfully generates the de-noised $S D O / \mathrm{HMI}$ magnetograms, and the de-noised magnetograms are much more consistent with the target magnetograms than the input ones. Second, our model greatly reduces the noise levels of the input magnetograms. The average noise level of the de-noised magnetograms is 3.21 G , which is quite lower than that of the input ones (8.66), and is consistent with that of the target magnetograms, 3.21 G . It is also noted that the average noise level of the de-noised magnetograms is even lower than that of $S D O / \mathrm{HMI} 720 \mathrm{~s}$ magnetograms calculated by Liu et al. (2012), 6.3 G. Third, all five metric values (CC, RE, linear fitting, NMSE, and peak $\mathrm{S} / \mathrm{N}$ ) of the de-noised magnetograms are much better than those of the input ones. Fourth, we applied the trained model to a full-disk $S D O / \mathrm{HMI}$ magnetogram to show a

[^0]possibility of the application of our model from the solar center to solar limb. Then we found that the application is quite successful in that the noise level of the de-noised magnetogram is greatly improved.

In this Letter, we have demonstrated that a deep-learning model based on DCGAN can be used to generate the de-noised magnetograms by training many pairs of single and stacked ones. Our de-noised magnetograms can be used for several studies on small magnetic structures; canceling magnetic features, magnetic flux emergence, solar surface motions, magnetic turbulence at the photosphere, and so on (Livi et al. 1985; Schrijver et al. 1997; Chae et al. 2001; Abramenko 2018). This idea can be applied to many astronomical areas because $\mathrm{S} / \mathrm{Ns}$ are not large due to insufficient photons. There are a few necessary conditions to apply this model to data. First, there should be enough data sets for training and test. Second, the integration of frames (or long-exposure observations) has to be successfully made. Third, there should be little significant motions of features during the integration. As a good example of application, our preliminary results show that this method is successfully applied to make Sloan Digital Sky Survey images de-noised (Park et al. 2019). Furthermore, our method can be applied in many scientific fields in which the integration of many frames are used to improve the $\mathrm{S} / \mathrm{N}$.

This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2013M1A3A3A02042232, NRF-2016R1A2B4013131, NRF-2019R1A2C1002634, NRF-2019R1C1C1004778, NRF2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP; 2018-0-01422, Study on analysis and prediction technique of solar flares). We thank the numerous team members who have contributed to the success of the $S D O$ mission. We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: NumPy (numpy.org), Keras (keras.io), TensorFlow (tensorflow.org), and SunPy (sunpy.org).

## Appendix <br> Smoothing Methods

The result of our median smoothing for a given pixel $p$ is given by

$$
M[I]_{p}=\operatorname{median}\left(I_{q} \in S\right)
$$

where $I_{q}$ is an intensity at a pixel $q$ in window size $S$. The size of the median filter is set to be 3 by comparing the results of training data sets.

The results of our Gaussian smoothing for a given pixel $p$ is given by

$$
G[I]_{p}=\sum_{q \in S} G_{\sigma}(\|p-q\|) I_{q}
$$

where $G_{\sigma}$ is a Gaussian function with $\sigma$, and $I_{q}$ is an intensity at pixel $q$ in window size $S$. The size of the Gaussian filter and $\sigma$


[^0]:    4 https://cs.nyu.edu/fergus/teaching/comp_photo/5_image_proc_noise_ bilateral.pdf
![img-6.jpeg](img-6.jpeg)

Figure 5. Two examples of the results of our model and three smoothing ones. The first column represents the results of our model, the second column represents the results of the median smoothing, the third column represents the results of the Gaussian smoothing and the last column represents the results of the bilateral smoothing. The first row represents the magnetograms from the methods, and the second row represents the difference maps between the results and target magnetograms.

are set to be 3 and 1 by comparing the results of training data sets.

The results of bilateral smoothing for a given pixel *p* are given by

$$B[I]_p = \frac{1}{W_p} \sum_{q \in S} G_{\sigma_r} (\|p - q\|) G_{\sigma_r} \left( |I_p - I_q| \right) I_q, \tag{9}$$

where *W<sub>p</sub>* is a normalized factor, *σ<sub>r</sub>* is spatial extent of the kernel, *σ<sub>r</sub>* is a minimum amplitude of an edge and *I<sub>q</sub>* is an intensity at pixel *q* in window size *S*. The size of the bilateral filter, *σ<sub>s</sub>*, and *σ<sub>r</sub>* are set to be 3, 10, and 10 by comparing the results of training data sets.

Figure 5 shows the results of our model and three smoothing ones for two examples. As shown in Figure 5, difference maps among these methods are similar to one another. Our visual inspection shows that the quality of magnetograms from our method is significant better than those from three smoothing methods. This is consistent with the fact that the noise levels of
the magnetograms from our model are smaller than those of the three smoothing methods.

## ORCID iDs

Eunsu Park (1) https://orcid.org/0000-0003-0969-286X
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Daye Lim (1) https://orcid.org/0000-0001-9914-9080
Harim Lee (1) https://orcid.org/0000-0002-9300-8073

## References

Abramenko, V. I. 2018, in IAU Symp. 340, Long-term Datasets for the Understanding of Solar and Stellar Magnetic Cycles, ed. D. Banerjee et al. (Cambridge: Cambridge Univ. Press), 281
Beckwith, S. V. W., Stiavelli, M., Koekemoer, A. M., et al. 2006, AJ, 132, 1729
Birney, D. S., Gonzalez, G., \& Oesper, D. 2006, Observational Astronomy (2nd ed.; Cambridge: Cambridge Univ. Press)
Chae, J., Martin, S. F., Yun, H. S., et al. 2001, ApJ, 548, 497
DeForest, C. E. 2017, ApJ, 838, 155
Díaz Baso, C. J., de la Cruz Rodríguez, J., \& Danilovic, S. 2019, A\&A, 629, A99
Galvez, R., Fouhey, D. F., Jin, M., et al. 2019, ApJS, 242, 7
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, in Advances in Neural Information Processing Systems 27, ed. Z. Ghahramani et al. (Red Hook, NY: Curran Associates, Inc.), 2672

Isola, P., Zhu, J.-Y., Zhou, T., et al. 2016, in IEEE Conf. on Computer Vision and Pattern Recognition, CVPR 2017, http://openaccess.thecvf.com/ content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_ 2017_paper.html
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, in 3rd International Conf. on Learning Representations, ICLR 2015, https://dblp.org/rec/journals/corr/KingmaB14. bib
Lecun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436
Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, Proc. IEEE, 86, 2278
Ledig, C., Theis, L., Huszar, F., et al. 2016, in IEEE Conf. on Computer Vision and Pattern Recognition, CVPR 2017, http://openaccess.thecvf.com/content_cvpr_ 2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html
Liu, Y., Hoeksema, J. T., Scherrer, P. H., et al. 2012, SoPh, 279, 295
Liu, Y., Zhao, X., \& Hoeksema, J. T. 2004, SoPh, 219, 39
Livi, S. H. B., Wang, J., \& Martin, S. F. 1985, AuJPh, 38, 855
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, Y., Choi, Y.-Y., Moon, Y.-J., et al. 2019, BKoAS, 44, 54
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Radford, A., Metz, L., \& Chintala, S. 2015, in 4th International Conf. on Learning Representations, ICLR 2016, https://dblp.org/rec/journals/corr/ RadfordMC15.bib
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Schrijver, C. J., Title, A. M., van Ballegooijen, A. A., et al. 1997, ApJ, 487, 424
Wang, J., Wang, H., Tang, F., et al. 1995, SoPh, 160, 277"
Sumiaya Rahman et al 2020 - Super-resolution of SDO HMI Magnetograms Using Novel Deep Learning Methods.pdf,"# Super-resolution of SDO/HMI Magnetograms Using Novel Deep Learning Methods 

Sumiaya Rahman ${ }^{1}$, Yong-Jae Moon ${ }^{1,2}$ (D), Eunsu Park ${ }^{2}$ (D), Ashraf Siddique ${ }^{3}$ (D), Il-Hyun Cho ${ }^{2}$ (D), and Daye Lim ${ }^{2}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{3}$ Department of Computer Science Engineering, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2020 February 21; revised 2020 May 20; accepted 2020 June 15; published 2020 July 10


#### Abstract

Image super-resolution is a technique of enhancing the resolution of an image where a high-resolution (HR) image is reconstructed from a low-resolution (LR) image. In this Letter, we apply two novel deep learning models (residual attention model and progressive GAN model) for enhancing Solar Dynamics Observatory (SDO)/ Helioseismic and Magnetic Imager (HMI) magnetograms. For this, we consider line-of-sight (LOS) magnetograms taken by SDO/HMI as output and their degraded ones with $4 \times 4$ binning as input. Deep learning networks try to find internal relationships between LR and HR images from the given input and the corresponding output image. We consider SDO/HMI magnetograms from 2014 May to August for training, from 2014 October to December for validation, and 2015 January to March for test. We find that the deep learning models generate higher-quality results than the bicubic interpolation in terms of visual aspects and metrics. We apply this model to a full-resolution SDO/HMI magnetogram and then compare the generated magnetogram with the corresponding Hinode/The Solar Optical Telescope Narrowband Filtergrams (NFI) magnetogram. This comparison shows that the generated magnetogram is consistent with the Hinode one with a high correlation (CC: 0.94) and a high similarity (SSIM: 0.93 ), which are better than the bicubic method.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); The Sun (1693); Astronomy data analysis (1858); Solar atmosphere (1477)

## 1. Introduction

Solar magnetograms are important tools for studying the distribution of the solar magnetic field, coronal holes, the connection between magnetic fields in active regions, and the interplanetary magnetic field lines (Scherrer et al. 2012). In particular, it is characterized by magnetic field strength, polarity, and its location on the Sun. There are several space missions, such as the Solar Heliospheric Observatory (Domingo et al. 1995), Hinode (Kosugi et al. 2007), and the Solar Dynamics Observatory (SDO; Pesnell et al. 2012), which have observed solar atmosphere and other active phenomena with high spatial and temporal resolution. The Helioseismic and Magnetic Imager (HMI; Schou et al. 2012) is one of SDO's onboard instruments and is designed to provide $0 . .504 \mathrm{pix}^{-1}$ resolution solar image of Doppler shift, continuum intensity, and vector magnetic field. The major activity of the instrument is to investigate the mechanism of solar variability, describe and understand the physical process of the Sun's interior, and observe the magnetic activity of the solar atmosphere. In spite of having such a spatial telescope with polarization selector and image stabilization system, the spatial resolution is not sufficient to observe the small-scale structures of the Sun. A magnetogram with high resolution (HR) can help us to characterize small magnetic features of the solar surface and improve the estimation of the total magnetic flux, which is limited by the resolution of the magnetograms (Baso \& Díaz Ramos 2018).

Super-resolution (SR) is a technique of enhancing the resolution of an image from one or more observed low-resolution (LR) images (Park et al. 2003; Dong et al. 2016; Ledig et al. 2017). In astronomy, SR is a process that provides astronomical images with a higher resolution based on observed astronomical images that are captured with the same astronomical scene by adding highfrequency content and eliminating the degradation due to the image processing of the astronomical telescopes (Guo et al. 2019).

In the literature, several approaches have been used to overcome the resolution enhancement problem, such as pixel-reliabilitybased SR reconstruction algorithm (Li et al. 2018), advanced image restoration techniques (Misra et al. 2018), and image denoising (Yan et al. 2017).

Recently, rapid advancements in deep learning-based image processing have been made using consistent, HR data sets and state-of-the-art algorithms. Deep learning models allow to learn the way humans think and recognize an object using multiple processing layers (Lecun et al. 2015). The depth of the convolutional neural network (CNN; Lecun et al. 1998) is important for SR. However, the deeper networks for image SR are more difficult to train (Zhang et al. 2018) because of the vanishing gradient problem. Because most of the CNN-based SR models are unable to make proper use of the hierarchical features from LR images, relatively low performance was observed (Zhang et al. 2018). To overcome the problem of vanishing gradient, Zhang et al. (2018) considered a residual attention model to get a very deep trainable network that learns important channel-wise features simultaneously. In the literature of image SR, different types of network structure, loss functions, learning principles, and strategies are used. Recently, the residual attention model has proven to be a popular deep learning method that gives a state-of-the-art performance in image SR when compared to the other deep learning models like SRCNN (Dong et al. 2016), EDSR (Lai et al. 2017), and LapSRN (Lim et al. 2017). Furthermore, the generative adversarial network (GAN; Goodfellow et al. 2014) is also a well-known model that enhances the perceptual quality of upsampled images. One of the GAN-based methods is a progressive GAN model (Wang et al. 2018) that follows the progressive multi-scale principle for SR. Direct reconstruction techniques, such as SRCNN and SRGAN (Ledig et al. 2017) can upsample the image in a single step, thus adding blur artifacts. In
contrast, the progressive GAN base model can reduce computational complexity and enlarge the model capability.

To our knowledge, there was one attempt to enhance solar images using deep learning. Baso \& Díaz Ramos (2018) applied a CNN-based deep learning method to synthetic data, which are simulated images of solar active regions with $2 \times 2$ binning, and compared their results with Hinode/The Solar Optical Telescope (SOT; Tsuneta et al. 2008) Broadband Filtergrams continuum images.

In this Letter, we apply two novel deep learning methods (the residual attention model and the progressive GAN model) to SDO/HMI magnetograms. For training, validation, and testing, we consider two data sets: degraded magnetograms with $4 \times 4$ binning for input and the corresponding original ones for output. For evaluation, we compare the results of two deep learning models with that of the bicubic method. In addition, we apply the residual attention model to a fullresolution SDO/HMI magnetogram and compare the generated magnetogram with the corresponding Hinode/SOT NFI magnetogram. We evaluate the results of the models in view of metric and visual inspection. This Letter is organized as follows. The data will be described in Section 2. The methods used for SR are discussed in Section 3. The results are given in Section 4. A brief summary and discussion are given in Section 5.

## 2. Data

### 2.1. SDO/HMI Magnetogram

SDO is a space mission that was launched on 2010 February 11 (Pesnell et al. 2012). The HMI instrument is part of the SDO mission, which provides continuous coverage of full-disk Doppler velocity, line-of-sight (LOS) magnetic flux, and continuum proxy images (Scherrer et al. 2012). For this study, we consider full-disk ( $4096 \times 4096$ ) SDO/HMI LOS magnetograms from 2014 and 2015 for every 00:00 UT. First, we make level 1.5 magnetograms by rotating and centering. To make the training computationally feasible, we randomly crop a region from the areas closer to the disk center (about $\pm 500^{\prime \prime}$ ). In the limb area, the spatial contrast of the magnetograms is very small so that the deep learning networks cannot reconstruct the structures, thus adding artifacts (Lim et al. 2017). We crop 10,785 pairs of HR SDO/HMI magnetograms with $512 \times 512$ pixels and make a 2D Gaussian smoothing with $(5,5)$ kernel and $2 \sigma$. Then we downsample them in size $128 \times 128$ pixels with a bilinear interpolation for LR data. For training, validation, and testing, we consider magnetograms with a range of -2000 G for minimum and 2000 G for maximum. We consider 9000 pairs from 2014 May to August for training and 985 pairs from 2014 October to December for validation and 800 pairs from 2015 January to December for testing. To avoid overfitting, we consider $10 \%$ of the training data for validation.

### 2.2. Hinode/SOT NFI Magnetogram

To validate the output of the residual attention model, we compare the output with an NFI magnetogram taken by the Hinode/SOT on 2013 January 7. The NFI magnetogram is defined by the stokes $V$ divided by the stokes $I$, which are simultaneously observed with the pixel size of $0 . \times 16$ and exposure time of 0.4 s . The NFI magnetogram is aligned to the HMI magnetogram based on visual inspection. The pixel resolution of
the NFI magnetogram is set to be $1 / 4 \times$ the HMI one by using the cubic interpolation. Then, the NFI magnetogram is calibrated by a cross-comparison (Moon et al. 2007) with the corresponding HMI one. For this, the third-order polynomial fitting of the HMI magnetic field strength is made as a function of NFI one. Finally, all magnetograms are scaled to $\pm 500 \mathrm{G}$ for display.

## 3. Method

The network architecture is one of the most important parts of deep learning. In the field of SR, various network design (e.g., residual learning, dense connections, etc.) can be applied. For the SR of the solar magnetogram, we adopt the residual attention model and the progressive GAN model.

### 3.1. Residual Attention Model

To generate the SR magnetogram, we consider a residual attention model that contains a very deep residual network with channel-wise feature attention learning shown in Figure 1(a). In the residual attention model, a deep network over 400 layers is employed as residual-in-residual structure, where residual groups work as a basic module and long skip connection that allows a course level residual learning. Each residual groups contain a stack of residual block with short skip connection that allows recovering the abundant low-frequency information. In order to provide more focus on the informative features, an attention mechanism is applied by exploding the interdependencies among the feature channels. Such a channel attention mechanism enables the network to concentrate the contextual information outside of the local region and enhance the discriminative learning ability.

Such a post-upscaling strategy has been proved to be more efficient in terms of computational complexity, as well as achieve higher performance than pre-upscaling SR methods. We applied several loss functions such as mean square error (MSE) loss, L1 loss, perceptual loss, and adversarial loss to get the optimized results. We find that L1 loss provides better performance for both quantitative and visual evaluation than the others. Here the L1 loss used in this study is given by

$$
L_{1}=\frac{1}{N} \sum_{i=1}^{N}\left|I_{i}^{T}-I_{i}^{\mathrm{SR}}\right|
$$

where $I^{T}$ and $I^{\mathrm{SR}}$ is the target and the generated SR images by the residual attention model, and $N$ is the total number of training data.

For training, we use the ADAM (Kingma \& Ba 2014) optimizer with momentum $\beta_{1}=0.9, \beta_{2}=0.999$. The initial learning rate is $10^{-4}$. In every $2 \times 10^{5}$ iterations of backpropagation, the learning rate is decreased to half from the previous label.

### 3.2. Progressive GAN Model

We also apply a progressive GAN model shown in Figure 1(b) to SDO/HMI magnetograms. In this model, we considered a progressive multi-scale GAN for enhancement of the perceptual quality of the upsampled magnetogram. This network is designed to reconstruct an HR image in intermediate steps by progressively performing a $2 \times$ upsampling of the input from the previous level. For each level, dense compression units are applied after being adapted from the corresponding dense blocks. This network provides residual $R_{i}(X)$ as output that is added to bicubic
![img-0.jpeg](img-0.jpeg)

Figure 1. Two flow charts represent two novel deep learning models: (a) the residual attention model and (b) the progressive GAN model. LR is a degraded low-resolution image, HR is a high-resolution image.

upsampling $I^{\text{Ric}}$ of input image $X$ to generate final output $I^{\text{SR}}$ given by

$$
I_s^{\text{SR}}=I_s^{\text{Ric}}+R_s(X)
$$

In order to enable multi-scale GAN-enhanced SR, a progressive discriminator network similar to the generator network is used. For training, the least-square loss is applied from VGG16 (Simonyan \& Zisserman 2014), which is added to the generator. Expressing the predicted residual and real residual as $R\left(I^{\text{SR}}\right)$ and $R\left(I^T\right)$, the discriminator and generator loss can be expressed as

$$
\begin{gathered}
L\left(D_{s}\right)=\left(D\left(R\left(I_s^{\text{SR}}\right)\right)\right)^2+\left(D\left(R\left(I_s^T\right)\right)-1\right)^2 \\
L\left(G_{s}\right)=\left(D\left(R\left(I_s^{\text{SR}}\right)\right)-1\right)^2+\sum_{k \in 2,4} \| \phi_k (I_s^{\text{SR}}) - \phi_k (I_s^T) \|^2
\end{gathered}
$$

where $L\left(G_{s}\right)$ is the generator loss, $L\left(D_{s}\right)$ is the discriminator loss, $s$ is scaling factor, and $\phi_k$ denotes the $k$th pooling layer input in VGG16.

## 4. Results and Discussion

Figure 2 represents a comparison among the residual attention model, the progressive GAN model, and the bicubic method outputs. The deep learning models show much better results in 4× SR of SDO/HMI magnetograms in both weak and strong fields than the bicubic method. It is very impressive that the magnetograms generated by the deep learning models are consistent with the corresponding target magnetograms.

The results show that deep learning models can reconstruct small-scale magnetic structures much better than the bicubic method. Also, the polarity inversion line is sharper in the deep learning models results than in the bicubic method. It is interesting to note that the deep learning models can successfully alleviate the blurring artifacts when compared to the interpolation methods.

For quantitative evaluation of the results, we calculate four types of metrics for the test data sets. First, we calculate the peak signal-to-noise ratio (S/N) that is commonly applied to measure the reconstruction quality of the model,

$$
\text{Peak } \frac{S}{N} = 10 \log \left( \frac{MAX_I^2}{MSE_I} \right)
$$

where $i$ is the number of testing samples, $MAX_I$ is equal to 4000 maximum value of data, and $MSE$ is the mean square error between the target and the generated magnetogram. Typical values for the peak S/N vary from 30 to 50, where higher is better. The second metric is a pixel-to-pixel Pearson's correlation coefficient (CC) between the target and the generated magnetograms. The third metric is the root mean square error (RMSE) given by

$$
RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (I_i^T - I_i^SR)^2}
$$

where $N$, $I^T$, and $I^SR$ are the total number of testing data, target, and generated SR magnetograms, respectively. The last metric
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparison among input, target, and generated magnetograms by the residual attention model, the progressive GAN model, and the bicubic method. From top to bottom, each row: degraded LR SDO/HMI magnetograms as input data, SDO/HMI magnetograms as target data, generated magnetograms by the residual attention model, the progressive GAN model, and the bicubic method.
![img-2.jpeg](img-2.jpeg)

Figure 3. The top row shows 2D histograms, the middle row shows error bar plots, and the bottom row shows scatter plots of the target and generated magnetograms of the testing data set with range of -2000 to 2000 G. From left to right, each column: the residual attention model, the progressive GAN model, and the bicubic ones. The error bar plot shows the mean pixel value, and the error bars indicate the standard deviation of the pixel values. The 2D histogram illustrates the density of the pixel distribution, and scatter plots illustrate the relationship between targets and outputs.

is a structural similarity (SSIM) index, which measures the perceptual difference between the target and the generated image. Here SSIM is given by

$$ \text{SSIM} = \frac{(2\mu_T \mu_{\text{SR}} + c_1)(2\sigma_T \text{SR} + c_2)}{(\mu_T^2 + \mu_{\text{SR}}^2 + c_1)(\sigma_T^2 + \sigma_{\text{SR}}^2 + c_2)} \tag{7} $$

where $\mu_T$, $\mu_{\text{SR}}$, $\sigma_T$, $\sigma_{\text{SR}}$, $\sigma_T$,SR are the mean, variance, and co-variance of target and generated magnetograms. Here $c_1$ and $c_2$ are constant value. SSIM value varies from 0 to 1, where higher is better.

Table 1 represents the average results of peak S/N, pixel-to-pixel Pearson's CC, RMSE, and SSIM value between SDO/HMI magnetograms and generated magnetograms. As shown in Table 1, the peak S/N value of the residual attention model is 48.35 dB, which is significantly higher than those of the other two models. In view of peak S/N, the residual attention model can reconstruct and recover small-scale features more efficiently than the other models. The pixel-to-pixel correlation value of the residual attention model is 0.93, which is higher than the progressive GAN model and the bicubic method. The average RMSE of the residual attention model is 0.037 G.

Table 1 Average Values of Pearson's CC, Peak S/N, RMSE, and SSIM of the Testing Data Set

|  Method | Peak S/N (dB) | Pixel to Pixel (CC) | RMSE (G) | SSIM  |
| --- | --- | --- | --- | --- |
|  Bicubic | 46.87 | 0.87 | 19.1 | 0.94  |
|  Progressive GAN model | 48.32 | 0.92 | 16.1 | 0.98  |
|  Residual attention model | 48.35 | 0.93 | 15.65 | 0.98  |
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparison of the SR magnetogram generated by the residual attention model with the corresponding Hinode/SOT NFI magnetogram observed on 2013 January 7. From top to the bottom: SDO/HMI magnetogram as input, Hinode/SOT NFI magnetogram, SR magnetogram by the residual attention model, and one by the bicubic method.

which is very small and much better than those of the other methods. The average SSIM indices of three models show that the magnetograms generated by the deep learning models (0.98) are more similar to the target ones than those of the other method.

The first row of Figure 3 shows 2D histograms of target and SR magnetograms generated by the residual attention model, the progressive GAN model, and the bicubic method for the testing data set. The histograms show that the deep learning models can preserve the pixel distributions much better than the bicubic methods. Error bar plots in the second row of Figure 3 show that the standard deviations of the deep learning models are much lower than those of the bicubic method, and the mean pixel values are better correlated with the target ones. The standard deviation indicates the dispersion of generated magnetogram pixel values from the mean pixel value for a given data range. The scatter plot of the deep learning models show a linear relationship between the target and generated magnetograms. The error bar plots show slightly non-linear features for smaller values, especially for the bicubic method. Because these patterns mostly occur at magnetic fields weaker than about 30 Gauss, we think that SR by deep learning models cannot significantly change scientific results.

Figure 4 presents a comparison between the Hinode/SOT NFI magnetogram observed on 2013 January 7 and the SR magnetogram generated by the residual attention model. Visually, the generated magnetogram cannot produce small-scale structures like NFI. The second and third columns show that the features of the generated magnetogram have sharper edges of magnetic flux and show a close resemblance to the target magnetogram in both the positive and the negative magnetic regions. The bicubic one is much more blurred than the generated magnetogram and the magnetic flux is consistently well distributed in the generated one. Other metrics of the residual attention model are as follows: CC = 0.94, peak S/N = 39.11 dB, and SSIM = 0.93. These values are all better than those of the bicubic method that gives: CC = 0.93, peak S/N = 38.92 dB, and SSIM = 0.91.
## 5. Conclusion and Summary

In this Letter, we apply two novel deep learning models for SR to solar magnetograms. For this work, we consider the residual attention and the progressive GAN model. We select 10,785 pairs of degraded LR SDO/HMI magnetograms as input data and corresponding HR magnetograms as target data. We trained our models using 9000 pairs from 2014 May to August, validated our models using 985 pairs from 2015 October to December, and tested using 800 pairs from 2015 January to March.

The major results of this study are as follows. First, all considered models can successfully generate HR solar magnetograms from LR magnetograms. The generated magnetograms are consistent with the target magnetograms. Moreover, we observe that perceptually the deep learning models provide more small-scale and texture-wise information than the bicubic method. From the correlation coefficient values, we show that the residual attention model ( 0.93 ) results are highly correlated with the target magnetograms. The higher peak $\mathrm{S} / \mathrm{N}$ and SSIM values represent that the residual attention model can reconstruct the magnetograms quite efficiently. Furthermore, the metric values (CC, peak S/N, RMSE, and SSIM) of the residual attention model are much better than the other models. In addition, we compare the generated magnetogram with the Hinode/SOT NFI magnetogram and the bicubic method. The generated magnetogram is well correlated with the Hinode one with higher peak $\mathrm{S} / \mathrm{N}$ and SSIM values. Finally, 2D histograms, error bar plots, and scatter plots show better pixel distribution, mean pixel values, and linear relationship in the deep learning models than the other interpolation methods.

SR is an ill-defined problem for solar magnetograms (Yang et al. 2019). Our study shows that deep learning models can extract additional information and features for generating a higher resolution magnetogram. The generated magnetogram will allow us to study small-scale details, magnetic features, sunspot structure, and other physical features of the Sun. We have also tried a larger scale SR like $8 \times 8$ binning; however, the results are not satisfactory. In the future, we look forward to
overcoming this problem using other available techniques. More feasible loss function, optimization method, and model structure may improve the results. In the future, we have a plan to train our model using other types of data such as HR solar continuum images showing detailed granule structures from the Goode Solar Telescope (Cao et al. 2010) and the Daniel K. Inouye Solar Telescope (Tritschler et al. 2016). This methodology can be applied to not only solar images, but also to other astronomical images as well.

We appreciate the referee's constructive comments. We thank the numerous team members who have contributed to the success of the SDO and Hinode mission. This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131, NRF-2019R1A2C1002634, NRF2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Development of a Solar Coronagraph on International Space Station' (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and the Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: SunPy (sunpy.org), pytorch (pytorch. org), TensorFlow (tensorflow.org), and SolarSoft.

## Appendix

Figure A1 shows 2D histogram plots and scatter plots of Figure 4. As shown in Figure A1, the residual attention model and bicubic interpolation method are similar to one another. Visually and the matrices results of the residual attention model are little better than those of the bicubic method.
![img-4.jpeg](img-4.jpeg)

Figure A1. The top row shows 2D histograms and the bottom row shows scatter plot of Hinode/SOT NFI magnetogram and generated magnetogram. From left to right, each column: the residual attention model and the bicubic ones.

## ORCID iDs

Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Eunsu Park (1) https://orcid.org/0000-0003-0969-286X
Ashraf Siddique (1) https://orcid.org/0000-0003-2186-5735
Il-Hyun Cho (1) https://orcid.org/0000-0001-7514-8171
Daye Lim (1) https://orcid.org/0000-0001-9914-9080

## References

Baso, C. J., \& Díaz Ramos, A. A. 2018, A\&A, 614, A5
Cao, W., Gorceix, N., Coulter, R., et al. 2010, AN, 331, 636
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Dong, C., Loy, C. c., He, K., et al. 2016, ITPAM, 38, 295
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Guo, R., Shi, X., \& Wang, Z. 2019, JEL 28, 023032
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kosugi, T., Matsuzaki, K., Sakao, T., et al. 2007, SoPh, 243, 3
Lai, W.-S., Huang, J.-B., Ahuja, N., et al. 2017, in Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (New York: IEEE), 624
Lecun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436

Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, IEEEP, 86, 2278
Ledig, C., Theis, L., Huszár, F., et al. 2017, in Proc. IEEE Conf. on Computer Vision and Pattern Recognition (New York: IEEE), 4681
Li, Z., Peng, Q., Bhanu, B., et al. 2018, Ap\&SS, 363, 92
Lim, B., Son, S., Kim, H., et al. 2017, in Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops (New York: IEEE), 136
Misra, D., Mishra, S., \& Appasani, B. 2018, arXiv:1812.09702
Moon, Y.-J., Kim, Y.-H., Park, Y.-D., et al. 2007, PASJ, 59, S625
Park, S. C., Park, M. k., \& Kang, M. G. 2003, ISPM, 20, 21
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Simonyan, K., \& Zisserman, A. 2014, arXiv:1409.1556
Tritschler, A., Rimmele, T., Berukoff, S., et al. 2016, AN, 337, 1064
Tsuneta, S., Ichimoto, K., Katsukawa, Y., et al. 2008, SoPh, 249, 167
Wang, Y., Perazzi, F., McWilliams, B., et al. 2018, arXiv:1804.02900
Yan, L., Liao, W., Chang, Y., et al. 2017, in IEEE International Geoscience and Remote Sensing Symp. (IGARSS) (New York: IEEE), 3425
Yang, W., Zhang, X., Tian, Y., et al. 2019, IEEE Trans. Multimedia, 21, 3106
Zhang, Y., Li, K., Li, K., et al. 2018, in Proc. European Conference on Computer Vision (ECCV), 286"
Bendict Lawrance et al 2022 - Generation of Solar Coronal White-light Images from SDO AIA EUV Images by Deep Learning.pdf,"# Generation of Solar Coronal White-light Images from SDO/AIA EUV Images by Deep Learning 

Bendict Lawrance ${ }^{1}$ (D) Harim Lee ${ }^{1}$ (D), Eunsu Park ${ }^{2}$ (D), Il-Hyun Cho ${ }^{1}$ (D), Yong-Jae Moon ${ }^{1,3}$ (D), Jin-Yi Lee ${ }^{1}$ (D), Shanmugaraju A ${ }^{4}$ (D), and Sumiaya Rahman ${ }^{2}$<br>${ }^{1}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic Of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Space Science Division, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{3}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic Of Korea<br>${ }^{4}$ Department of Physics, Arul Anandar College, Karumathur 625514, Madurai District, Tamilnadu, India<br>Received 2022 April 8; revised 2022 July 15; accepted 2022 August 11; published 2022 October 4


#### Abstract

Low coronal white-light observations are very important to understand low coronal features of the Sun, but they are rarely made. We generate Mauna Loa Solar Observatory (MLSO) K-coronagraph like white-light images from the Solar Dynamics Observatory/Atmospheric Imaging Assembly (SDO/AIA) EUV images using a deep learning model based on conditional generative adversarial networks. In this study, we used pairs of SDO/AIA EUV (171, 193, and $211 \AA$ ) images and their corresponding MLSO K-coronagraph images between 1.11 and 1.25 solar radii from 2014 to 2019 (January to September) to train the model. For this we made seven (three using single channels and four using multiple channels) deep learning models for image translation. We evaluate the models by comparing the pairs of target white-light images and those of corresponding artificial intelligence (AI)-generated ones in October and November. Our results from the study are summarized as follows. First, the multiple channel AIA 193 and $211 \AA$ model is the best among the seven models in view of the correlation coefficient ( $\mathrm{CC}=0.938$ ). Second, the major low coronal features like helmet streamers, pseudostreamers, and polar coronal holes are well identified in the AI-generated ones by this model. The positions and sizes of the polar coronal holes of the AIgenerated images are very consistent with those of the target ones. Third, from AI-generated images we successfully identified a few interesting solar eruptions such as major coronal mass ejections and jets. We hope that our model provides us with complementary data to study the low coronal features in white light, especially for nonobservable cases (during nighttime, poor atmospheric conditions, and instrumental maintenance).


Unified Astronomy Thesaurus concepts: Solar corona (1483); Astronomy data analysis (1858); Convolutional neural networks (1938); Solar atmosphere (1477); Solar extreme ultraviolet emission (1493); Solar coronal holes (1484); The Sun (1693)

Supporting material: animations

## 1. Introduction

Coronal white-light information helps us understand coronal features of the Sun, like helmet streamers (HSs), pseudostreamers (PSs), coronal mass ejections (CMEs), polar coronal holes, streamer waves, etc. (Gosling et al. 1976; Hundhausen et al. 1994; Yashiro et al. 2004; Schmit et al. 2009; Gopalswamy et al. 2013). In this regard, the white-light data from Mauna Loa Solar Observatory (MLSO), COronal Solar Magnetism Observatory (COSMO), and K-coronagraph (K-Cor) that observe the low corona in white light, which have the field of view (FOV) from 1.05 to 3 Rs , are considered valuable but rare. The K-Cor instrument at the ground-based MLSO (de Wijn et al. 2012) is dedicated to observing white-light low corona from 2013 to the present. This K-Cor was designed to study CMEs specifically and other coronal features in the white-light regime.

Ground-based observations such as the MLSO K-Cor have certain limitations: instrumental maintenance, poor atmospheric conditions, and nighttime nonavailability. While white-light coronagraph data in higher corona are available through the Solar and Heliospheric Observatory's (SOHO) Large Angle

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.

Spectrometric Coronagraph (LASCO; Brueckner et al. 1995), where C2's FOV is from 2 to 6 Rs and C3's FOV is up to 32 Rs , those in lower corona are very rare due to the nonavailability of C1 (FOV 1.1 to 3 Rs ). In this sense, the MLSO K-coronagraph white-light data have been regarded to be very valuable as it observes low corona from 1.05 to 3 Rs .

Deep Learning is a branch of machine learning in artificial intelligence (AI) has been developed to learn the ways humans think based on artificial neural networks (Lecun et al. 1998, 2015). The convolutional neural network given by Lecun et al. (1998) is extensively used as a deep learning method regarding image translation. The deep learning model given by Goodfellow et al. (2014) based on Generative Adversarial Networks (GANs) distinguishes whether the generated image is real or fake by learning the loss. The conditional GANs (cGANs) developed by Isola et al. (2016) are considered to be very successful in resolving the image-to-image translation challenges. As mentioned above, considering the rareness of MLSO K-Cor images, we utilize the space-based Solar Dynamics Observatory/Atmospheric Imaging Assembly (SDO/ AIA; Pesnell et al. 2012) images to generate the white-light images using a deep learning method to replace gaps in MLSO K-Cor data and to cover both the nonobservable time of this instrument and the nighttime from 1.11 to 1.25 Rs. This approach makes it possible to track the white-light coronal
features continuously. There are several deep learning methods reported and used to generate solar images and solar magnetograms (Park et al. 2018; Kim et al. 2019; Shin et al. 2020; Jeong et al. 2020; Lee et al. 2021). Recently, Son et al. (2021) generated ground-based He I 1083 nm images from SDO/AIA images through deep learning. In the present study, we utilize the deep learning method pix2pi to generate the ground-based MLSO K-Cor like white-light images from SDO/ AIA images.

The generation of white-light coronal images by deep learning has several advantages. First, the ground-based MLSO K-Cor like white-light images from 1.11 to 1.25 Rs can be generated around the clock. Second, the generated white-light images will be helpful to fill data gaps and to track coronal features continuously during poor seeing atmospheric conditions and instrumental maintenance. Third, we can generate not only coronal holes and streamers, but also solar eruptions like jets and CMEs. The paper is organized as follows. The data and method are described in Section 2 and Section 3, respectively. The results and discussion are presented in Section 4 and the conclusion in Section 5.

## 2. Data

SDO/AIA is designed and dedicated in providing advanced high-quality EUV data with 12 s temporal resolution to study and understand the physics of the solar activities to the scientific community with an FOV up to 1.28 Rs . It is a spacebased instrument where AIA is specifically designed to provide information in multiple wavelengths in view of the solar corona (Lemen et al. 2012). For this study, we collect three channels such as AIA 171, AIA 193, and AIA $211 \AA$ as input data available from the Joint Science Operation Center (JSOC) database ${ }^{5}$ to represent solar corona. We consider AIA 171, AIA 193, and AIA $211 \AA$ for this study because the coronal features in the low coronal regions beyond the solar limb from 1 to 1.28 Rs are mostly clear and visible at these lines. In the case of other lines, the coronal features are less clear because they correspond to too hot temperatures (flaring) or low temperatures (chromospheric). MLSO COSMO K-Cor is a groundbased observatory operated by the High Altitude Observatory (HAO), a division of the National Center for Atmospheric Research (NCAR). It is dedicated to the study of the formation and the dynamics of low coronal features of the Sun in white light with an FOV from 1.05 to 3 Rs with a nominal cadence of 15 s . We collect MLSO K-Cor white-light data from the MLSO database. ${ }^{6}$

For training, validation, and evaluation data sets, we use SDO/AIA images and MLSO K-Cor white-light images. Though the FOV of MLSO K-Cor is from 1.05 to 3 Rs , the images are clear enough only from 1.11 Rs. Also, regarding SDO/AIA, though its FOV is up to 1.28 Rs , the images are clear only up to 1.25 Rs .

Considering these factors, we consider the FOV of the image pairs from 1.11 to 1.25 Rs for the study. Moreover, the MLSO K-Cor observatory can observe during the day (cadence 15 s ), but SDO/AIA data is available around the clock with a time cadence of 12 s . Hence, considering these factors, we produced 21,071 pairs (from 1.11 to 1.25 Rs ) of input and target images with a 20 minute cadence from 2013 October to 2019

[^0]December by careful visual inspection. We classify the data: January to September for training, October and November for testing, and December for validation.

We apply the data preprocessing to the white-light data and EUV data as follows. First, considering the FOV and the clarity of the images, we select the images from 1.11 to 1.25 Rs for our study. Second, white-light and EUV images are transposed into polar coordinates. Third, regarding the intensity, K-Cor white-light data are expressed in bolometric intensity (log scaled). To calibrate the EUV data, we divide them by the respective median intensity of the solar disk ( $\mathrm{B} / \mathrm{BSun}$ ) to make the EUV data (log scaled) as reported in the K-Cor data. For image translation, we consider the solar coronal images in polar coordinates starting from the western equatorial region. The south western portion $\left(+60^{\circ}\right)$ is added to the left side and the northwestern portion $\left(+60^{\circ}\right)$ to the right side of each so that every transposed image can show coronal features near the western equatorial region properly.

## 3. Method

We utilize the pix2pix model by Isola et al. (2016) based on the GAN (Goodfellow et al. 2014). The pix2pix model that we considered in this study is a combination of the cGAN (Mirza \& Osindero 2014) and the deep convolutional generative adversarial network (DCGAN; Radford et al. 2015). This pix2pix model is included with two networks called generator and discriminator. The generator will generate target-like images from the input images. Then, the generator will minimize the differences between the target images and the generated target-like images. Then, the discriminator part will distinguish the real image pairs from the generated image pairs. Here we consider cGAN as a loss function,

$$
\mathcal{L}_{c G A N}(G, D)=\log D(I, T)+\log (1-D(I, G(I)))
$$

where $G$ is the generator, $D$ is the discriminator, $I$ represents input data, and $T$ represents the target data. Here, $G(I)$ represents an output from the generator for a given input image. While the generator tries to minimize the objective, the discriminator tries to maximize it.

The model proposed by Isola et al. (2016) considered $256 \times 256$ images for training. For this study we use all data as 1024 by 1024 so that we modify the data pipeline and the depth of the generator network (Figure 1). We consider seven deep learning models whose information is given in Table 1. We train the models with 1,500,000 iterations and save the generators for every 15,000 iterations. As a result, we got 100 generators (and the discriminators) for 1,500,000 iterations. Then, we select the best model by comparing the generated white-light images with target images, which gives the highest mean correlation coefficient (CC) value for the respective validation data set of the models considered.

## 4. Results and Discussion

Figure 2 shows the comparison among the target K-Cor white-light image made at 17:33:41 UT on 2019 November 11 and the corresponding AI-generated ones using the seven different models. We compare how well the coronal streamers (HS, PS, and coronal ray-like structures) and the polar coronal holes are generated as they are in the target images. It seems that PSs (possible PS reported in the HAO website), HSs (Abbo et al. 2015), coronal-ray-like structures in the polar regions


[^0]:    5 http://jsoc.stanford.edu/ajax/lookdata.html
    6 https://mlso.hao.ucar.edu/mlso_data_calendar.php
![img-0.jpeg](img-0.jpeg)

Figure 1. Flowchart of our deep learning model (cGAN) used in this study.

Table 1 Comparison of Metrics for the Deep Learning Models Considered for the Validation Data Set in This Study

|  Metrics | Model 1 | Model 2 | Model 3 | Model 4 | Model 5 | Model 6 | Model 7  |
| --- | --- | --- | --- | --- | --- | --- | --- |
|  AIA Input (Å) | 171 | 193 | 211 | 171+193 | 171+211 | 193+211 | 171+193+211  |
|  Normalized RMSE | 0.098 | 0.087 | 0.085 | 0.084 | 0.083 | 0.080 | 0.076  |
|  Pearson's CC (pixel to pixel) | 0.897 | 0.925 | 0.935 | 0.932 | 0.942 | 0.945 | 0.943  |

(Wang et al. 2007), and polar coronal holes are well generated by all of the models except for the first model. For example, the region between the PS and the HS (a region within the red box in Figure 2) on the eastern equator does not seem to be reproduced well by the first model (using AIA 171 Å). These coronal streamers and polar coronal holes seem to be well generated by all other models. Also, it should be noted that, the overall distribution of the target image seems well reproduced in the generated ones of all other models (Figure 2), but there are few differences between these models.

To test our results, we calculate the metrics like CC and normalized root mean square error (NRMSE). CC and NRMSE are calculated from each pair of generated and target white-light images. The results are shown in Table 1. The CC and NRMSE are given by

$$CC = \frac{\sum_{i=1}^{N} (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^{N} (X_i - \bar{X})} \sqrt{\sum_{i=1}^{N} (Y_i - \bar{Y})}} \tag{2}$$

$$NRMSE_i = \sum \frac{RMSE_i}{y \max - y \min} \tag{3}$$

where $X$ represents the pixel values of the generated images, $Y$ represents the pixel values of the real images, $\bar{X}$ represents the average pixel values of generated images, $\bar{Y}$ represents the average pixel values of real images, and $N$ represents the total number of pixels, respectively. ""RMSE"" is the root mean squared error. We estimate these metrics for a considered coronal region with 400 × 624 pixels.

We consider the best model, by comparing the highest mean CC value of the validation data set. Once the deep learning model is trained, the performance of the model will be measured using the validation data set. Based on the comparison, we find that the sixth model AIA 193 and 211 Å gives a CC value of 0.945 with a relatively small NRMSE, which is the best among the seven models. The test data set also confirms that Model 6 is the best with a CC value of 0.938. The results of the validation and the test data set are given in Tables 1 and 2. In addition to that, low coronal eruptions like CMEs/jets seem to be generated well by only Model 6 with AIA 193 and 211 Å. The ejection phenomena show up best in these AIA wavelengths. In this sense, we think that our selection is reasonable.

Next we compare polar coronal hole sizes to check how well our model generates the white-light images. We calculate the polar coronal hole sizes separately in the north and the south polar regions, based on the log-scaled intensity of the target and the AI-generated images. For example, the target K-Cor image observed at 19:38:52 UT on 2018 November 27 and the corresponding AI-generated image is shown in Figures 3(a) and (b).

In this study, we calculate the sizes of polar coronal holes as follows. First, we fix the mean log-scaled intensity separately for the north (horizontal green line) and the south polar regions (horizontal orange line) as shown in Figures 3(c) and (d). As
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparison between the target K-Cor white-light image and the AI-generated images between 1.11 and 1.25 Rs based on single- and multichannel inputs at 17:33:41 UT on 2019 November 11. Note: the solar coronal images are considered in polar coordinates (Azimuth angle) starting from the westward equatorial region for this study. Helmet streamers (HSs) and pseudostreamer (PS). The red box represents the region not well identified by Model 1.
seen in the figure, the intensity is much lower in the polar coronal hole regions than in the equatorial regions. It is important to emphasize here that polar coronal hole regions are open magnetic field regions that are responsible for high-speed solar winds (Krieger et al. 1973). In this perspective, the
northern and the southern polar coronal hole regions look dark in white light, due to low gas density in comparison with the bright structures such as HSs, which have enhanced gas density in the eastern and the western equators (Charbonneau \& Hundhausen 1996). Second, we calculate the distance between
Table 2
Comparison of Metrics for the Deep Learning Models Considered for the Test Data Set in This Study

| Metrics | Model 1 | Model 2 | Model 3 | Model 4 | Model 5 | Model 6 | Model 7 |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| AIA Input (Å) | 171 | 193 | 211 | $171+193$ | $171+211$ | $193+211$ | $171+193+211$ |
| Normalized RMSE | 0.116 | 0.106 | 0.109 | 0.101 | 0.098 | 0.089 | 0.094 |
| Pearson's CC (pixel to pixel) | 0.890 | 0.913 | 0.927 | 0.923 | 0.936 | 0.938 | 0.936 |

![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison of polar coronal hole sizes of the (a) target K-Cor observation on 2018 November 27 at 19:38:52 UT, the (b) corresponding AI-generated image, and the intensity map of (c) target K-Cor and (d) AI-generated images.
Table 3 Comparison of Polar Coronal Hole Sizes of the Target and the AI-generated Images

|  Date | Time (UT) | Polar Coronal Hole Size (deg) |  |  |   |
| --- | --- | --- | --- | --- | --- |
|   |  | Target |  | Generated |   |
|   |  | North Pole | South Pole | North Pole | South Pole  |
|  2016/Oct/11 | 19:19:51 | 79 | 69 | 83 | 73  |
|  2016/Nov/02 | 17:40:18 | 91 | 59 | 91 | 63  |
|  2016/Nov/04 | 22:26:37 | 78 | 71 | 81 | 75  |
|  2016/Nov/05 | 21:15:19 | 76 | 72 | 79 | 75  |
|  2016/Nov/11 | 21:18:35 | 68 | 77 | 75 | 79  |
|  2017/Oct/01 | 17:20:32 | 72 | 75 | 68 | 75  |
|  2017/Oct/02 | 17:37:11 | 79 | 75 | 87 | 70  |
|  2017/Oct/27 | 17:47:52 | 69 | 81 | 68 | 81  |
|  2017/Oct/28 | 20:45:54 | 74 | 75 | 72 | 75  |
|  2017/Oct/29 | 18:46:10 | 75 | 71 | 77 | 70  |
|  2017/Nov/09 | 00:04:22 | 64 | 90 | 62 | 84  |
|  2017/Nov/24 | 21:14:01 | 70 | 76 | 73 | 69  |
|  2018/Oct/24 | 18:21:42 | 64 | 85 | 64 | 80  |
|  2018/Nov/29 | 17:33:41 | 78 | 70 | 78 | 65  |
|  2018/Nov/19 | 17:48:53 | 71 | 83 | 71 | 84  |
|  2018/Nov/21 | 18:44:06 | 73 | 75 | 73 | 79  |
|  2018/Nov/27 | 19:38:52 | 81 | 63 | 80 | 66  |
|  2018/Nov/20 | 19:44:33 | 69 | 83 | 71 | 78  |
|  2019/Oct/17 | 17:48:38 | 74 | 78 | 75 | 80  |
|  2019/Oct/27 | 22:44:21 | 73 | 80 | 75 | 76  |
|  2019/Nov/01 | 19:28:40 | 73 | 84 | 71 | 82  |
|  2019/Nov/02 | 17:32:17 | 72 | 85 | 69 | 86  |
|  2019/Nov/11 | 17:33:47 | 79 | 78 | 76 | 81  |
|  2019/Nov/11 | 19:33:47 | 78 | 83 | 71 | 82  |
|  2019/Nov/18 | 18:43:08 | 79 | 94 | 73 | 93  |

the junctures (marked by red star signs in the horizontal green line of the north polar region) considering from left to right (see Figures 3(c) and (d)).

We consider 25 AI-generated events by careful visual inspection of the test results (October and November). The images that have clear polar coronal holes with a relatively fewer number of plumes or coronal ray-like structures are considered to calculate the polar coronal hole sizes (Table 3) and to compare them with the target events. We found that, most of the polar coronal hole sizes were similar to each other. The average difference and the standard deviation between the polar coronal sizes of the target and the AI-generated ones are $-0.14^{\circ}$ and $3.52^{\circ}$, respectively. The distribution of polar coronal hole sizes of the target and the AI-generated ones are shown in Figure 4. It seems from the distribution that the polar coronal holes sizes of the AI-generated images using this deep learning model are consistent with the target images.

Next, we utilize our model to compare the generation of jet eruptions with the target ones. For this we consider the jet eruption observed by the MLSO K-cor instrument at 18:43:12 UT to 18:53:12 UT on 2016 March 10. The images from this jet eruption have not been included in the training part. We consider the importance of the jet eruption and choose this event for testing our model. It is very impressive that our model successfully generates this jet eruption (Figure 5(b)), which is consistent with the corresponding target images (Figure 5(a)).

This coronal jet started to erupt around 18:46:00 UT and ended around 18:53:00 UT in view of the MLSO K-Cor instrument. Similarly, we notice that the generated images of this jet eruption match with the same starting and ending times

![img-3.jpeg](img-3.jpeg)

Figure 4. Distribution of the target and the AI-generated polar coronal hole sizes. as observed by the MLSO K-Cor instrument. Hence, it seems that the jet eruption in a white-light regime could be made during nonobservable cases of the MLSO K-Cor from 1.11 to 1.25 Rs using the SDO/AIA images and using our model. Due to the occulting disks of the space-borne coronagraphs, the
![img-4.jpeg](img-4.jpeg)

Figure 5. Coronal jet eruption at 18:49:10 UT on 2016 March 10. (a) Jet eruption (yellow arrow) observed by K-Cor instrument. (b) Al-generated image of this jet eruption. An animation of this figure with a 12 s cadence is available. The animation starts at 18:43:11 UT and ends at 18:53:11 UT where the jet eruption can be seen clearly.

(An animation of this figure is available.)

![img-5.jpeg](img-5.jpeg)

Figure 6. Al-generated CME–streamer wave interaction on 2012 May 17 (01:33:37 UT). Note: ST WV—streamer wave (indicated by yellow arrows) and CME—coronal mass ejection (indicated by a blue arrow). An animation of this figure is available. The animation starts at 01:19:37 UT and ends at 01:49:13 UT, in which the CME eruption, the interaction between the CME and streamer wave 1, and the recovery phase of the streamer wave after deflection are seen.

(An animation of this figure is available.)

The solar corona is being observed beyond 2.0 Rs by LASCO C2. Thus it seems difficult to detect the less energetic jets (Hanaoka et al. 2018) by these white-light space-borne coronagraphs. Under those circumstances, scientists could rely on the EUV observations like SDO/AIA and Solar Ultraviolet Imager for the low coronal information on jet eruptions. Considering this issue, we hope that our model can be used to complement white-light data to study jet eruption in low corona from 1.11 to 1.25 Rs during the nonobservable cases.

The CME–streamer wave interactions in the low corona are well generated by our model. For example, we considered the 2012 May 17 CME event that seemed to interact with the coronal streamer wave (Rouillard et al. 2016; Decraemer et al. 2020) as seen clearly in the LASCO C2 and C3 coronagraphs around 02:00 UT. It is noted that these types of interactions can be seen through the EUV observations by SDO/AIA. In fact, the white-light images of this event in the K-Cor instrument are not available. The CME eruption (in white light) seems to be
![img-6.jpeg](img-6.jpeg)

Figure 7. CME eruption on 2014 October 14. Left: CME eruption (yellow arrow) observed by K-Cor instrument. Right: AI-generated images of this CME eruption. An animation of this figure with a 12 s cadence is available.

(An animation of this figure is available.)
![img-7.jpeg](img-7.jpeg)

Figure 8. (a) Poor quality image observed by K-Cor at 20:06:38 UT on 2017 February 2. (b) Replaced K-Cor-like AI-generated image by this model. Note: the yellow arrow in the south polar region indicates the coronal ray-like structure and HS indicated the helmet streamer.

Well generated by using our model in comparison with the SDO/AIA images. As additional data, the generated images would be useful in white-light counterparts as well. We notice the CME-flank and streamer wave (ST WV1) interaction between 01:33:37 to 01:36:15 UT within 1.25 Rs (see Figure 6). The interaction started around 01:34:38 UT and the full deflection can be seen at 01:37:13 UT. The recovery phase of the streamer wave started around 01:39:37 UT. It is noted that these types of interactions can be seen through SOHO LASCO's C2 and C3 at greater heights. It is very impressive that our model could be useful to detect these types of interactions between 1.11 and 1.25 Rs in white light at an earlier time than SOHO LASCO's C2 and C3.

Next, we utilize our model to check how well our model generates the CMEs reported in the MLSO catalog. For this, we consider the test period used in this study of every year from 2013 to 2019 (October and November). As a result, we found 25 CMEs reported in the MLSO CME list. Out of these 25 CMEs, 5 CMEs are found to be major CMEs, 15 are faint CMEs, and the remaining 5 are very faint, in which the starting and ending times are difficult to conclude and not reported in the MLSO catalog. Then we utilize our model to check these remaining 20 CMEs. We found that the major CMEs seem to be generated well by our model. The faint CMEs are not well generated by our model, which is the limitation of this study. The major CMEs seem to be better well generated than the faint CMEs. For example, a major CME on 2014 October 14 event is shown in Figure 7. The starting and ending time of the AI-generated images of this CME eruption match with the target ones of the MLSO K-Cor.

Next, we utilize our model to generate the K-Cor white-light images during the observation time in poor visibility conditions of the MLSO K-Cor instrument. From the HAO website, we can see some reports related to poor quality data; they are classified as noisy, poor, and degraded. Hence, difficulties arise in confirming the coronal features, which lead to interruptions in continuous monitoring of them in white light. Considering this issue, we generate white-light images to replace such kinds of images with poor quality by our model.

Our model is used to replace one such poor quality image of the K-Cor observation (Figure 8(a)) made at 20:06:38 UT on 2017 February 2 by the AI-generated one. It seems that the image is well generated (Figure 8(b)) by our model. For example, the HS (as shown in Figure 8(b)) in the eastern equator is restored well in the generated image. Also the coronal ray-like structure (indicated by a yellow arrow) seems to be restored well at the southern polar region. Hence, this model can be used to restore data during situations like poor visibility/degraded times to confirm coronal features, which makes it possible to have continuous monitoring of solar eruptions in white light, as well as to cover the data gap.

### 5. Conclusion and Summary

In this study, we have applied a deep learning method based on conditional generative adversarial networks to the generation of MLSO K-Cor-like white-light images from SDO/AIA images. We consider 21,071 pairs (between 1.11 and 1.25 Rs) of the MLSO K-Cor and the SDO/AIA (171, 193, and 211 Å) images from 2013 to 2019 for this study. We have made seven deep learning models (three using single channels and four using multiple channels). The major results of this study are summarized as follows. First, among the seven models considered, the multichannel model AIA 193 and 211 Å (Model 6) generates the coronal white-light images successfully with CC = 0.938. The AI-generated images of this model seem consistent with the target images. Second, the white-light images of dominant low coronal features such as HSs, PSs, polar coronal holes, and coronal ray-like structures are successfully generated. The positions and sizes of the polar coronal holes in the AI-generated images are consistent with those in the target ones. Third, using this model, it is impressive to note that we could generate major jet eruptions and CMEs.
The starting and ending times of the AI-generated images of the jet eruption considered in this study match with the target ones of the MLSO K-Cor. Our model could provide the data during the nonobservable cases of the MLSO K-Cor instrument and during the nighttime. This model will be useful to replace poor quality data such as noisy, degraded ones, data observed in poor atmospheric conditions, and during the instrumental maintenance of the MLSO K-Cor.

We would like to stress the advantage of this study is that the major CMEs/jets, polar coronal holes, HSs, and PSs in white light can be generated by our model once the SDO/AIA EUV images are available. In this regard, we hope that our model will be helpful in space weather forecasting of white-light counterparts. Next, by utilizing the AI-generated white-light images by our model, early coronal signatures like major CMEs/jets and other possible features can be detected in the low corona from 1.11 to 1.25 Rs at an earlier time than by SOHO LASCO's C2 and C3 telescopes. In this regard, we hope that our model will complement the nonavailability of the C 1 telescope of the SOHO/LASCO satellite to a certain extent. In the future, we plan to generate SDO/AIA-like EUV images beyond its FOV utilizing the white-light images of the MLSO K-coronagraph.

This research was supported by the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2020R1C1C11003892, NRF-2021R1I1A1A01049615), NRF-2019R1C1C1006033 and the Korea Astronomy and Space Science Institute (KASI) under the R\&D program (project No. 2022-1-850-05) supervised by the Ministry of Science and ICT. Courtesy of the Mauna Loa Solar Observatory, operated by the High Altitude Observatory, as part of the National Center for Atmospheric Research (NCAR). NCAR is supported by the National Science Foundation. We thank the numerous team members who contributed to the success of the SDO/AIA mission. We acknowledge the community effort dedicated to the development of the opensource packages that were used in this work: Numpy (numpy. org), Keras (keras.io), TensorFlow (tensorflow.org), SunPy (sunpy.org), and SolarSoftware.

Software: Numpy (van der Walt et al. 2011; Harris et al. 2020), SolarSoft (Freeland \& Handy 1998), SunPy (SunPy Community et al. 2020; Mumford et al. 2020).

## ORCID iDs

Bendict Lawrance (2) https://orcid.org/0000-0001-6648-0500
Harim Lee (2) https://orcid.org/0000-0002-9300-8073
Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Il-Hyun Cho (2) https://orcid.org/0000-0001-7514-8171
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Jin-Yi Lee (2) https://orcid.org/0000-0001-6412-5556
Shanmugaraju A (2) https://orcid.org/0000-0002-2243-960X

## References

Abbo, L., Lionello, R., Riley, P., et al. 2015, SoPh, 290, 2043
Brueckner, G. E., Howard, R. A., Koomen, M. J., et al. 1995, SoPh, 162, 357
Charbonneau, P., \& Hundhausen, A. J. 1996, SoPh, 165, 237
de Wijn, A. G., Burkepile, J. T., Tomczyk, S., et al. 2012, Proc. SPIE, 8444, 84443 N
Decraemer, B., Zhukov, A. N., \& Van Doorsselaere, T. 2020, ApJ, 893, 78
Freeland, S. L., \& Handy, B. N. 1998, SoPh, 182, 497
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Gopalswamy, N., Xie, H., Akiyama, S., et al. 2013, ApJL, 765, L30
Gosling, J. T., Hildner, E., MacQueen, R. M., et al. 1976, SoPh, 48, 389
Hanaoka, Y., Hasuo, R., Hirose, T., et al. 2018, ApJ, 860, 142
Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Natur, 585, 357
Hundhausen, A. J., Burkepile, J. T., St., \& Cyr, O. C. 1994, JGR, 99, 6543
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2016, arXiv:1611.07004
Jeong, H.-J., Moon, Y.-J., Park, E., et al. 2020, ApJL, 903, L25
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Krieger, A. S., Timothy, A. F., \& Roelof, E. C. 1973, SoPh, 29, 505
Lecun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436
Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, IEEEP, 86, 2278
Lee, H., Park, E., \& Moon, Y.-J. 2021, ApJ, 907, 118
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Mumford, S., Freij, N., Christe, S., et al. 2020, JOSS, 5, 1832
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Radford, A., Metz, L., \& Chintala, S. 2015, arXiv:1511.06434
Rouillard, A. P., Plotnikov, I., Pinto, R. F., et al. 2016, ApJ, 833, 45
Schmit, D. J., Gibson, S., de Toma, G., et al. 2009, JGRA, 114, A06101
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Son, J., Cha, J., Moon, Y.-J., et al. 2021, ApJ, 920, 101
SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
van der Walt, S., Colbert, S. C., \& Varoquaux, G. 2011, CSE, 13, 22
Wang, Y.-M., Biersteker, J. B., Sheeley, N. R., et al. 2007, ApJ, 660, 882
Yashiro, S., Gopalswamy, N., Michalek, G., et al. 2004, JGRA, 109, A07105"
Hyun-Jin Jeong et al 2020 - Solar Coronal Magnetic Field Extrapolation from Synchronic Data with AI-generated Farside.pdf,"# Solar Coronal Magnetic Field Extrapolation from Synchronic Data with AI-generated Farside 

Hyun-Jin Jeong ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1,2}$ (D), Eunsu Park ${ }^{2}$ (D), and Harim Lee ${ }^{2}$ (D)<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2020 August 31; revised 2020 October 14; accepted 2020 October 19; published 2020 November 4


#### Abstract

Solar magnetic fields play a key role in understanding the nature of the coronal phenomena. Global coronal magnetic fields are usually extrapolated from photospheric fields, for which farside data is taken when it was at the frontside, about two weeks earlier. For the first time we have constructed the extrapolations of global magnetic fields using frontside and artificial intelligence (AI)-generated farside magnetic fields at a near-real time basis. We generate the farside magnetograms from three channel farside observations of Solar Terrestrial Relations Observatory (STEREO) Ahead (A) and Behind (B) by our deep learning model trained with frontside Solar Dynamics Observatory extreme ultraviolet images and magnetograms. For frontside testing data sets, we demonstrate that the generated magnetic field distributions are consistent with the real ones; not only active regions (ARs), but also quiet regions of the Sun. We make global magnetic field synchronic maps in which conventional farside data are replaced by farside ones generated by our model. The synchronic maps show much better not only the appearance of ARs but also the disappearance of others on the solar surface than before. We use these synchronized magnetic data to extrapolate the global coronal fields using Potential Field Source Surface (PFSS) model. We show that our results are much more consistent with coronal observations than those of the conventional method in view of solar active regions and coronal holes. We present several positive prospects of our new methodology for the study of solar corona, heliosphere, and space weather.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); Solar corona (1483); Convolutional neural networks (1938); The Sun (1693); Space weather (2037); Astronomical models (86)

## 1. Introduction

Solar magnetic fields dominate the structure and dynamics of the corona and inner heliosphere (Jess et al. 2016). The magnetic field is an energy source of solar flares and their accompanying coronal mass ejections (Amari et al. 2018; Inoue et al. 2018). Photospheric magnetic fields are routinely measured, but direct measurements of coronal magnetic fields are very difficult and rare. Thus, several models of extrapolation or magnetohydrodynamic (MHD) simulation have been developed to derive coronal magnetic fields from photospheric magnetograms (Mikić et al. 2018; Nandy et al. 2018). A synoptic map of solar magnetic fields has been widely used for the input boundary condition of the coronal models. The map constructed by merging together frontside magnetograms, which are daily updated near-central-meridian data, over 27day solar rotation period (Bertello et al. 2014). For the farside of the map, data assimilation techniques with magnetic surface flux transport models and helioseismic farside detections were applied (DeVore et al. 1984; Schrijver \& DeRosa 2003). However, those approaches still have limitations to predict realtime farside magnetic fields, especially for rapid changes in magnetic fields by flux emergence or disappearance near the limb.

Deep learning, a subset of machine learning in artificial intelligence (AI) and also known as deep neural networks, has been developed to find the best mathematical manipulation to turn the input into the output, whether it be a linear or nonlinear relationship. It has made many advances to solve space weather problems like solar flare forecast (Huang et al. 2018; Park et al. 2018), coronal hole detection (Illarionov \& Tlatov 2018), etc. The Pix2Pix (Isola et al. 2017) model, which is based on the
conditional Generative Adversarial Networks (cGAN; Mirza \& Osindero 2014), is a novel deep learning method excellent for image translation tasks that have been well demonstrated in solar astronomy and space weather (Park et al. 2019, 2020; Ji et al. 2020). Kim et al. (2019; hereafter KPL19) proposed an approach to generate solar farside magnetograms from the Solar TErrestrial RElations Observatory (STEREO; Kaiser et al. 2008)/Extreme UltraViolet Imager (EUVI; Howard et al. 2008) $304 \AA$ images. The orbit of STEREO Ahead (A) and Behind (B) is at a distance of about 1 au , and drift away from the Earth at a rate of about $22^{\circ}$ per year in opposite directions. They applied the Pix2Pix and trained and evaluated the model with pairs of frontside Solar Dynamics Observatory (SDO; Pesnell et al. 2011)/Atmospheric Imaging Assembly (AIA; Lemen et al. 2011) $304 \AA$ images and SDO/Helioseismic and Magnetic Imager (HMI; Scherrer et al. 2012) line-of-sight (LOS) magnetograms before the generation of farside data. However, their results were limited to the maximum magnetic field strength of $\pm 100$ Gauss, and showed low correlations in solar quiet regions. It was noted that the Pix2Pix has an issue with generating high-resolution data, and the lack of details and realistic features in the high-resolution results (Chen \& Koltun 2017). Wang et al. (2018) proposed an improved model to solve the issues with a novel adversarial loss and multi-scale architectures of the networks, and they named the model ""Pix2PixHD."" Shin et al. (2020) tried to generate magnetograms from Ca II K images, and their results show that the Pix2PixHD model is useful to generate ones with a large dynamic range ( $\pm 1400$ Gauss).

In this Letter, we make an upgraded model with $\pm 3000$ Gauss dynamic range based on the Pix2PixHD and use
multi-channel images for input. We describe the detailed structure of the model in Section 3.1. We train the model with frontside SDO extreme ultraviolet (EUV) passband images and magnetograms, and then generate farside magnetograms from the corresponding images of STEREO A and B by the model in Section 4.1. Next we generate a synchronic map of photospheric field, which replaces the conventional synoptic map at the farside with the AI-generated ones in Section 4.2. Hereafter, this is called the HMI and AI synchronic map. Finally, we extrapolate coronal magnetic fields from the synchronic map using a Potential Field Source Surface (PFSS) model. Then we compare them with EUV observations as well as those from the conventional method in Section 4.3. We present several prospects of our results in Section 5.

## 2. Data

We use pairs of full-disk SDO/AIA three passband images and SDO/HMI LOS 720 s magnetograms to train our deep learning model. The three EUV passbands are 304, 193, and $171 \AA$, corresponding to the chromosphere, corona, and upper transition region, respectively (Lemen et al. 2011). The data pairs have a cadence of 12 hr (at 00 UT and 12 UT each day) from 2011 January 1 to 2017 December 31. We use 4231 pairs of the multi-channel EUV images for input and magnetograms for output of our model. We construct a training set with 10 months and an evaluation set with two months, and both are selected for each year without any duplication between the two sets. In order to train and evaluate various inclination conditions, which cause different distributions of southern/ northern magnetic field strength for each month (Pastor Yabar et al. 2015), the months are selected randomly. We take 3412 pairs for the training data set and 819 pairs for the evaluation data set.

To generate farside magnetograms, we use STEREO/EUVI A and B (304, 195, and $171 \AA$ ) passband images, which have similar response characteristics to the SDO/AIA images (Downs et al. 2012). These SDO and STEREO passbands are often used together for global solar studies (Su \& Van Ballegooijen 2012; Cairns et al. 2018). The farside EUV images are selected from the closest times (within one hour) to the synoptic data, which is a conventional boundary condition for the coronal magnetic field extrapolation.

The following data pre-processing is applied to the EUV data and magnetograms for effective training and generating. We make Level 1.5 images with the standard SolarSoftWare (SSW) packages (Freeland \& Handy 1998) of aia_prep.pro, hmi_prep.pro and secchi_prep.pro function, which process the images by calibrating, rotating, and centering. We downsample them to be same resolution ( $1024 \times 1024$ pixels), and the solar radius $\left(R_{\odot}\right)$ is fixed at 450 pixels. We mask the area outside $0.98 R_{\odot}$ of disk center to minimize the uncertainty of limb data. For the calibration of the EUV data, all data numbers are scaled by median values of the original data on the solar disk, which are fixed at 100 . Then the logarithms of the scaled data are normalized from -1 to 1 with the saturation values of 0 (lower limit) and $\log (200)$ (upper limit). Then we combine the three passband images from SDO and the STEREOs into the RGB channel dimensions. The magnetograms for training have an upper and lower saturation limit of $\pm 3000$ Gauss for normalization. Finally, we manually exclude a set of SDO data pairs and STEREO data with poor quality; for example, noise images
because of solar flares, those with incorrect header information, those with infrequent events such as eclipses, transits, etc.

We use HMI daily updated radial field synoptic map with polar field correction (Sun et al. 2011) for the conventional magnetic field map at the photosphere. Hereafter, this is called the HMI synoptic map, and is provided by the Joint Science Operation Center (JSOC). The synoptic map well represents the region within $\pm 60^{\circ}$ of longitude with a daily updated frontside magnetogram. However, the map still has several uncertainties at the farside. Here, we improve the farside of the synoptic map in Section 4.2.

## 3. Method

### 3.1. Pix2PixHD Model

We adopt the Pix2PixHD model, which is one of the popular deep-learning methods for image translation of high-resolution images without significant artifacts, to generate solar farside magnetograms. The Pix2PixHD consists of two major networks: one is a generative network (generator) and the other is a discriminative network (discriminator). The generator tries to generate realistic output from input, and the discriminator tries to distinguish the more realistic pair between a real pair and a fake pair. The real pair consists of a real input and a real output. The fake pair consists of a real input and an output from the generator. We construct a real input, a real output, and an output from the generator as a combined image of three EUV passbands, a magnetogram, and an AI-generated magnetogram, respectively.

The generative network consists of several convolutional layers and transposed convolutional layers. A convolutional layer contains a set of filters that extract features automatically from the input data, like a human visual system, and whose parameters are learned or updated during the model training process. A transposed convolutional layer is an inverse process of convolution and tries to reconstruct output from the extracted features. The discriminative network is a classifier that consists of several convolution layers. Features that are passed through the convolution layers are fed into a single sigmoid output in order to produce a probability output in the range of 0 (fake) to 1 (real) so that the discriminator acts like a classifier.

While the model is training, both networks compete with each other and get an update at every step with loss functions. Loss functions are objectives that score the quality of results by the model, and the network automatically learns that they are appropriate for satisfying a goal, i.e., the generation of realistic magnetograms. We train and evaluate our model using SDO frontside data pairs, and generate farside magnetograms from STEREO's EUV observations as an input of the generator.

Networks of the Pix2PixHD model get an update with two loss functions: one is a conditional generative adversarial network (cGAN) loss ( $\mathcal{L}_{\text {cGAN }}$ ), and the other is a featurematching loss $\left(\mathcal{L}_{F M}\right)$. The cGAN loss is a basic function of the models based on cGANs, and aim for the generator and discriminator to compete. In order to clearly discriminate the real and fake pairs, the discriminator tries to maximize the loss. The generator tries to generate realistic data that fools the discriminator, thus minimizing the loss. The $\mathcal{L}_{\text {cGAN }}$ gets a lower value when $D(x, G(x))$ has a value close to 1 , in which the AI-generated image is similar to the real output. The loss of
![img-0.jpeg](img-0.jpeg)

Figure 1. Flowchart and structures of our deep-learning model. $G$ is the generator, $D_{1}$ and $D_{2}$ are the discriminators, $x$ is an EUV image with three passbands, $y$ is a real magnetogram, and $G(x)$ is an AI-generated magnetogram.
cGAN is given by

$$
\mathcal{L}_{\mathrm{cGAN}}(G, D)=\log (D(x, y))+\log (1-D(x, G(x)))
$$

where $x, y$, and $G(x)$ are a real input, a real output, and an output from the generator, respectively. The $\mathcal{L}_{F M}$ is to regularize the fake pair to have more similar distribution and statistics with the real pair from multiple layers of the discriminator. The loss of feature matching is given by

$$
\mathcal{L}_{F M}(G, D)=\sum_{i=1}^{T} \frac{1}{N_{i}}\left[\left\|D^{(i)}(x, y)-D^{(i)}(x, G(x))\right\|\right]
$$

where $T, i$, and $N_{i}$ are the total number of layers, a serial number of the layers, and the number of pixels in output feature maps of each layer, respectively.

Figure 1 shows the main structure of our model, which has one generator and two discriminators ( $D_{1}$ and $D_{2}$ ). One discriminator gets input pairs of the original pixel size, and the other gets input pairs that are downsampled by half. Each discriminator classifies real pairs and generated pairs with different scales, and guides the generative networks to generate globally consistent images and produce finer details. Their full loss function combines both cGAN loss and feature-matching loss. We use 10 for a relative weight $(\lambda)$, which controls the importance of $\mathcal{L}_{\mathrm{cGAN}}$ and $\mathcal{L}_{F M}$ as in Wang et al. (2018). When the model is trained, the generator tries to minimize the full loss and the discriminators try to maximize the cGAN loss (Equation (3)). To minimize or maximize the loss, we use Adam solver (Kingma \& Ba 2015) as an optimizer for both the generator and the discriminator.

$$
\min _{G}\left(\left(\max _{D_{1}, D_{2}} \sum_{k=1,2} \mathcal{L}_{\mathrm{cGAN}}\left(G, D_{k}\right)\right)+\lambda \sum_{k=1,2} \mathcal{L}_{F M}\left(G, D_{k}\right)\right)
$$

In our model, an input image $(x)$ of three EUV channels is given to a generator and it generates an HMI-like magnetogram $(G(x))$. The model calculates $\mathcal{L}_{\mathrm{cGAN}}$ and $\mathcal{L}_{F M}$ from the results of discriminators. Then the networks get an update at every step with the losses and they are iterated until the assigned iteration, which is a sufficient number assuring the convergence of the model. We train the model with 3412 pairs of training data set for 630,000 iterations ( $\sim 150$ epochs), and save AIgenerated magnetograms from the evaluation inputs every 10,000 iterations. Our code is available at https://github.com/ JeongHyunJin/Jeong2020_SolarFarsideMagnetograms. In the readme file, we explain the architecture and selected hyperparameters of our deep-learning model. For basic and extensive information on the deep learning, please refer to Buduma \& Locascio (2017), Goodfellow et al. (2016), and Subramanian (2018).

### 3.2. PFSS Model

We use PFSS extrapolation, which is a well-established method for estimating large scale structure of global corona (Riley et al. 2006). PFSS is much more widely used for the space weather forecast (Hakamada et al. 2005; Pomoell \& Poedts 2018) than the Nonlinear Force-Free Field (NLFFF) extrapolation and the magnetohydrodynamic (MHD) approach. The extrapolation calculates current-free field of the corona from the bottom boundary radius to the source surface radius $\left(R_{S S}\right)$ by solving the Laplace equation, which is given by

$$
\nabla^{2} \Phi_{B}(r)=0
$$

where $\Phi_{B}$ is a scalar potential. At the source surface, the magnetic field is assumed to be radial, because the outflowing solar wind drags the field out into the heliosphere. Open field lines arriving at the source surface are associated with coronal holes (CHs; Lowder et al. 2014). The CHs are regions of lowintensity emission in EUV and X-rays due to their low density and temperature compared to the surrounding quiet corona. The solar winds and interplanetary magnetic fields are known to originate from these regions (Wang et al. 1996). The
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparisons of a real SDO/HMI line-of-sight magnetogram and AI-generated magnetograms. (a) AI-generated data by KPL19 model. (b) Real magnetogram on 2014 April 1 at 12:00 UT. (c) AI-generated one by our model. Full-disk magnetograms are displayed as white for positive polarity and black for negative one. A solar AR on the center of the solar disk (red box) and a QR on the limb (blue box) are zoomed and represented with other color maps showing the large dynamic range values in Gauss.

The heliocentric height of *RSS* has been conventionally assumed to be 2.5 solar radius (*R⊙*). However, the lower *RSS* produces better results near solar minimum (Lee et al. 2011). Lowering the *RSS* in the PFSS model results in more open fluxes and more coronal hole areas. In this Letter we use two values: 2.0*R⊙* and 2.5*R⊙*. The input boundary condition is the measured radial magnetic fields in the photosphere-like HMI synoptic maps. We compute the PFSS model on a uniform grid of 155 × 240 × 480 (*r* × *θ* × *φ*).

## 4. Results

### 4.1. Generation of Solar Farside Magnetograms

We train and evaluate our deep-learning model using pairs of SDO/AIA three EUV passband images and SDO/HMI LOS magnetograms. Table 1 shows results of three objective measures for full disk, active regions (ARs), and quiet regions (QRs) between real and AI-generated magnetograms for the evaluation data set. First, we calculate correlation coefficients (CCs) between the total unsigned magnetic flux (TUMF) of the SDO/HMI magnetograms with a full dynamic range and that of AI-generated ones. Our model shows that the CCs are 0.99, 0.95, and 0.98 for 819 full disk, 1,281 ARs, and 819 QRs. These values demonstrate that our model can successfully generate TUMF over all regions. Second, we calculate CCs between the net magnetic flux (NMF) of SDO/HMI magnetograms and that of AI-generated ones, and those are 0.86, 0.93, and 0.97 for the same data sets, respectively. Because discrete magnetic field polarity at the limb of solar disk, NMF CC for full disk is lower than those for ARs and QRs. Third, mean pixel-to-pixel CCs between SDO/HMI magnetograms and that of AI-generated ones after 8 × 8 binning are 0.81, 0.79, and 0.62. These imply that our model greatly improves the generation of magnetograms for not only ARs but also QRs when compared with results of KPL19. In particular, it is noticeable that the mean pixel-to-pixel CCs of the QRs has greatly increased.

Figure 2 shows a comparison of magnetograms on 2014 April 1: a real one, one by KPL19, and ours. It is taken from the evaluation data set. AI-generated magnetograms from KPL19 and our model show overall magnetic field distributions well. However, in detailed magnetic structures, our AI-generated magnetogram is much more consistent with the real one than one by KPL19 with a couple of strong points. First, the NOAA AR 12021 (red box), which shows strong (higher than 1,000 Gauss) magnetic fields, are well generated by our model. Second, the networks of magnetic fields (blue box) are well

Table 1 Three Objective Measures of Comparison between SDO/HMI Magnetograms and AI-generated Ones for Full Disk, ARs, and QRs

|   | Full Disk |  | AR |  | QR |   |
| --- | --- | --- | --- | --- | --- | --- |
|   | 825 images |  | 1,033 patches |  | 825 patches |   |
|   | (1,024 × 1,024 pixels) |  | (128 × 128 pixels) |  | (128 × 128 pixels) |   |
|   | Ours | KPL19 | Ours | KPL19 | Ours | KPL19  |
|  Total unsigned magnetic flux CC | 0.99 | 0.97 | 0.95 | 0.95 | 0.98 | 0.74  |
|  Net magnetic flux CC | 0.86 | … | 0.93 | … | 0.97 | …  |
|  Mean pixel-to-pixel CC (8 × 8 binning) | 0.81 | 0.77 | 0.79 | 0.66 | 0.62 | 0.21  |

Note. For comparison with the previous research, the results of KPL19 are shown together.
![img-2.jpeg](img-2.jpeg)

Figure 3. A series of full-disk EUV images and magnetograms. (a) The first and fourth EUV images are taken from SDO/AIA 304, 193, and 171 $\AA$ passbands. The second and third EUV images are taken from STEREO/EUVI A and B 304, 195, and 171 $\AA$ passbands. (b) The first and fourth magnetograms are taken from SDO/ HMI. The second and third magnetograms are AI-generated farside ones by the model. The yellow boxes show the tracking of solar ARs over a solar rotation. The positions of the boxes are slightly different due to the inclination angle between the ecliptic plane and orbit of the spacecraft. (c) EUV images and magnetograms in the yellow box area are zoomed. Full-disk magnetograms are displayed as white for positive polarity and black for negative one. The color map of zoomed magnetograms shows large dynamic range values in kG .
generated by our model, and the distributions of magnetic polarity look like real one. Based on those results, our model generates more reliable magnetograms than KPL19.

We generate more realistic farside magnetograms from the corresponding three EUV passband images of STEREOs by the model. The images of STEREOs are scaled to SDO ones with a correction factor that is estimated from the ratio of the median on-disk brightness of those images (Ugarte-Urra et al. 2015; Liewer et al. 2017). It is not meant as an absolute calibration correction, but it makes consistent conditions with training data for the deep-learning model. Thus the farside magnetograms generated make it possible to monitor the continuous evolution of solar magnetic field distribution over the solar surface. Our farside magnetograms generate ARs with realistic magnetic field strength (Figure 3).

### 4.2. Generation of HMI and AI Synchronic Maps

We generate the HMI and AI synchronic map using the farside AI-generated magnetograms. The AI-generated ones are converted from full disk images to Carrington heliographic coordinated maps, and from the line of sight to the radial magnetic field by applying the radial-acute method (Wang \& Sheeley 1992) based on their coordinates. A farside part (from $60^{\circ}$ to $300^{\circ}$ Carrington longitude) of the HMI synoptic map is replaced by the AI-generated magnetograms within $\pm 60^{\circ}$ if the farside EUV data are available. Our synchronic map greatly improves farside magnetic fields, which can be generated at almost the same time as near-real-time EUV observations.

We compare farside photospheric field maps and an EUV $304 \AA$ synchronic map, which is reconstructed with multiviewpoints observations of SDO/AIA and STEREO/EUVI A
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparisons of solar farside magnetic field maps of the photosphere with the EUV $304 \AA$ synchronic map. (a) Conventional HMI synoptic map on 2011 June 1 at 12:00 UT. (b) HMI and AI synchronic map. (c) ADAPT GONG map. (d) EUV $304 \AA$ synchronic map. Each map is shows $60^{\circ}$ to $300^{\circ}$ Carrington longitude and within $\pm 60^{\circ}$ latitude. "" $A$ "" indicates appearance or shift case of solar ARs and "" $D$ "" indicates a disappearance case. The polarity of the magnetic fields are shown as a color map of full-disk magnetograms in Figures 2 and 3.
and B on 2011 June 1 (Figure 4). Each map is interpolated to an uniform, $240 \times 480$ grid in latitude and Carrington longitude for the comparisons of input conditions to compute the coronal field extrapolation. The farside of the HMI synoptic map (Figure 4(a)) and the EUV synchronic map (Figure 4(d)) show noticeable differences, because the farside of the synoptic map was taken several days previously. There were several flux appearance (cases A1, A2, and A3 in Figure 4) and flux disappearance (case D1 in Figure 4) of ARs. There was also a flux emergence and a shift of the location of an AR moving west (case A4 in Figure 4). Each location of A1-4 and D1 in Figure 4 was set according to the EUV map. The Air Force Data Assimilative Photospheric flux Transport (ADATP) Global Oscillation Network Group (GONG) map (Hickmann et al. 2015) provided by the National Solar Observatory (NSO) is based on a magnetic surface flux transport model, and can predict the changes of sequential magnetic flux that showed at the solar frontside (cases A1 and D1 case in Figure 4(c)). As shown in cases A2 and A3 in Figure 4(c), rapid changes associated with the emergence of new magnetic regions at the limb or farside are not properly reproduced. Our synchronic map shows not only the appearance and disappearance of ARs (cases A1-2, A4, and D1 in Figure 4(b)) but also a shift of an AR (case A4 in Figure 4(b)). In the case of STEREO data unavailability (e.g., case A3 in Figure 4(b)), our result cannot predict the appearance of the AR.

### 4.3. Extrapolation of Coronal Magnetic Fields

We use the HMI and AI synchronic map as a bottom boundary condition to extrapolate global coronal magnetic fields. Then we predict open field areas ( CHs ) from the results of extrapolation, and compare those with CHs observed in EUV emissions. Figure 5 shows the results of extrapolations
calculated from the HMI synoptic and HMI and AI synchronic data, and EUV observations of STEREO/EUVI $284 \AA$ and SDO/AIA $211 \AA$ from 2011 June 1 to 21 . Those EUV passbands are not used for training and generation, and ARs and CHs are well identified in those images. There was a continuous magnetic flux emergence of the NOAA AR 11236 over a solar rotation. We select the data when the AR was near the center of the solar disk in each spacecraft observation, and indicate them with green arrows. The first and second rows in Figure 5 show STEREO EUV images at the solar farside and the extrapolation results at the corresponding positions. These positions are $94^{\circ}$ Carrington longitude near the west limb and $267^{\circ}$ Carrington longitude near the east limb of the solar frontside. There were appearances of two ARs including NOAA AR 11236 near the center of the solar disk observed by STEREO A (Figure 5(b)). The PFSS extrapolation from HMI synoptic data cannot represent those ARs, and depict long CH structures along the latitudinal direction near the meridian (Figure 5(a)). On the other hand, the extrapolation from HMI and AI synchronic data well represents the ARs and CHs under the NOAA AR 11236 (Figure 5(c)). 23 days later, flux emergences of the NOAA AR 11236 and another AR on its southeast region were observed by STEREO-B on 2011 June 14 (Figure 5(e)). The PFSS extrapolation from HMI synoptic data show similar CH distributions as the extrapolations from several days previous (Figure 5(d)). The extrapolation from our synchronic data well represents the distributions of the ARs and CHs consistent with the observed images (Figure 5(f)). The third row in Figure 5 shows SDO observations at the frontside and PFSS results at the corresponding position on 2011 June 21. The results of these calculations are mostly same, because both extrapolations are computed by the observed frontside magnetograms (Figures 5(g) and (i)). The
![img-4.jpeg](img-4.jpeg)

Figure 5. Comparisons between EUV observations and results of PFSS extrapolations from conventional and HMI and AI synchronic data in view of full-disk observations. (a)–(c) Result of PFSS extrapolation from HMI synoptic data, EUV observation, and PFSS extrapolation from HMI and AI synchronic data at the position of STEREO A on 2011 June 1. (d)–(f) Those at the position of STEREO-B on 2011 June 14. (g)–(i) Those at the position of SDO on 2011 June 21. Positive and negative polarities of the open fields are indicated with blue and red colors. Closed field lines are indicated with dark yellow. Field lines at the limb and open field area on the surface are only displayed for comparison. The PFSS is computed with R25 = 2.0R25 and R25 = 2.5R25, and those open field areas are displayed with hatched pattern and filled area, respectively. The solar surface is filled with the bottom boundary data to show the distribution of ARs. The green arrow represents appearances of the NOAA AR 11236.

PFSS results of our synchronic data (Figures 5(c), (f) and (i)) show continuous sequences of ARs and CHs from the farside to the frontside of the Sun.

Figure 6 shows the EUV observations and CHs identified from the extrapolations in view of Carrington maps from 2011 December 25 to 2012 January 21. The first row in Figure 6 shows synchronized EUV maps from STEREO/EUVI A and B 195 Å and SDO/AIA 193 Å. The second row in Figure 6 shows the results from our HMI and AI synchronic data, and the third row in Figure 6 corresponds to the results from conventional HMI synoptic data. There are two major magnetic flux emergence of the ARs. They are linked with equatorial open field regions (CHs), which are indicated with green and pink arrows in Figure 6. Those ARs and CHs are more consistent with our results than those of the conventional method. Our extrapolations show overall consistent distributions of global magnetic field polarities over one solar rotation.

We calculate the magnetic fluxes for each area (green box in Figure 6) including an equatorial coronal hole. On 2011 December 25, when the coronal hole is on the farside of the Sun, the total unsigned magnetic flux, positive magnetic flux, and negative magnetic flux from our HMI and AI synchronic data are 2.1 × 10^21, 6.2 × 10^20, and 1.5 × 10^21 Mx, respectively. In HMI synoptic data they are 2.3 × 10^21, 1.0 × 10^21,
![img-5.jpeg](img-5.jpeg)

Figure 6. Comparisons between EUV synchronic maps and results of PFSS extrapolations from conventional and HMI and AI synchronic data in view of Carrington maps over a solar rotation. (a)–(c) EUV synchronic maps from STEREO/EUVI A and B 195 Å and SDO/AIA 193 Å on 2011 December 25, and those on 2012 January 11 and 21. (d)–(f) Results of PFSS extrapolation from HMI and AI synchronic data corresponding to the EUV maps. (g)–(i) Those from HMI synoptic data corresponding to the EUV maps. Green and pink arrows indicate two solar ARs linked with equatorial open field regions. The boxes with green represent the areas, which include a coronal hole, where we compute the magnetic fluxes. Other features are described in Figure 5.

and 1.3 × 10<sup>21</sup> Mx, respectively. In our case the ratio of the negative magnetic flux to the total unsigned magnetic flux is about 71%, while the ratio from the synoptic data is about 55%. For 2012 January 11 data, where the equatorial coronal hole is well predicted, the ratio of our result is about 70%, and 71% from the synoptic data. Our results show more consistent magnetic fluxes of the unipolar region than those from the HMI synoptic data. These results represent that our AI-generated magnetograms well generate distributions of magnetic fluxes in coronal holes.

### 5. Summary and Prospects

We want to stress on that our new methodology has several positive prospects for the study of solar corona, heliosphere, and space weather. First, AI-generated farside magnetograms, together with frontside magnetograms, can be used for the long-term evolution of sunspots and solar magnetic fluxes. In this Letter, we have applied a deep learning model to the generation of solar farside magnetograms in order to extrapolate solar coronal magnetic fields. We show that the AI-generated magnetograms in our model can generate strong magnetic fields, which show correlations with real magnetograms of full disk, ARs, and QRs that are higher than those of KPL19. The improvements provide new opportunities for global magnetic flux studies, such as tracking solar ARs and studying their evolution (Ugarte-Urra et al. 2015) or studying the time evolution of open and total magnetic fluxes at the solar surface (Solanki et al. 2002). As deep learning technology advances, our AI-generated data will become more realistic and the applications from the AI-generated ones will show more promising results.

Second, our synchronic map can better input data for not only the PFSS global field extrapolations but also MHD approaches such as the Magnetohydrodynamic Algorithm outside a Sphere (MAS) model (Mikić et al. 2018). We show that our maps are more consistent with EUV observations than the conventional photospheric data. The MAS model has been used to be computed for higher heights of corona than the source surface radius of the PFSS and constructs the physical parameters of corona, which are not only magnetic field vectors but also plasma properties such as mass density, gas pressure, and velocity. If our results are used for the input of the MAS model, we expect that the model can produce more reasonable solar coronal and heliospheric physical parameters.

Third, the HMI and AI synchronic map may be improved with data assimilation methods and photospheric flux transport models, which include the effects of differential rotation, meridional flow, super-granulation, and random background flux. In our study, we simply replace a farside HMI synoptic map by the AI-generated farside magnetograms. There are several techniques that assimilate magnetograms into the flux transport model, e.g., the Schrijver and DeRosa model (Schrijver & DeRosa 2003) and the ADAPT model (Hickmann et al. 2015). As shown in their methods, our AI-generated farside magnetograms can be assimilated into the flux transport model, and as a result we expect that the model may show better results than before. This model has the advantage that the map has a better stable balance in magnetic flux on the boundary of the generated data and on the map close to the boundary.

Fourth, extrapolated coronal magnetic field data can be used for initial boundary conditions of several space weather prediction models. We have shown that the PFSS extrapolations with the synchronic data are also more consistent with
EUV observations than the conventional methods and show continuous sequences of coronal structure changes over several solar rotations. Our improved PFSS extrapolation data will be useful as better input conditions for the solar wind forecasting (Hakamada et al. 2005; Pomoell \& Poedts 2018), which is a major component for space weather. For example, the Wang-Sheeley-Arge (WSA) solar wind model (Arge \& Pizzo 2000) has been widely used to forecast the solar wind at 1 au from the coronal extrapolation. It provides improved solar wind conditions to heliospheric MHD models, such as an ENLIL model for forecast of corona mass ejection arrivals (Steenburgh et al. 2013). Our method will provide more accurate solar wind predictions, especially for the farside of the Sun and heliosphere.

Fifth, our PFSS extrapolation can be help us study the global environment from the Sun to the interplanetary space. The extrapolation has been widely used for decades to study interplanetary fields (Schatten et al. 1969; Rust et al. 2008), photospheric sources of the solar wind (Wang \& Sheeley 2003), and solar energetic particle events (Nitta et al. 2006; Park et al. 2013). Recently, the Parker Solar Probe (PSP; Fox et al. 2016), the first spacecraft to fly into the low solar corona, has been collecting data on the magnetic fields, which has then been compared with the time series predictions of radial magnetic fields from the PFSS (Bale et al. 2019; Panasenco et al. 2020). When the training data for our deep-learning model have been prepared for this solar cycle, we hope to compare the results with the PSP observations. Solar Orbiter, which was launched in 2020, is equipped with a wide range of not only in situ but also remote-sensing instruments (Mueller et al. 2013), which can be better examined with our method. Moreover, our study may give us new insight on the global solar research techniques and on how to construct a set of instruments on board future spaceborne solar imaging missions such as the L4 and L5 missions (Vourlidas 2015).

We really appreciate the referee's constructive comments. We thank the numerous team members who have contributed to the success of the SDO mission, as well as the STEREO mission. We also thank the Solar Physics Group at Stanford University for their support in providing timely access to HMI data and synoptic maps. We acknowledge the community efforts devoted to developing the open-source packages that were used in this work. This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2016R1A2B4013131, NRF-2019R1A2C1002634, NRF-2019R1C1C1004778, NRF-2020R1C1C1003892), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP; 2018-0-01422, Study on analysis and prediction technique of solar flares).

## ORCID iDs

Hyun-Jin Jeong (2) https://orcid.org/0000-0003-4616-947X
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944

Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Harim Lee (2) https://orcid.org/0000-0002-9300-8073

## References

Amari, T., Canou, A., Aly, J.-J., Delyon, F., \& Alauzet, F. 2018, Natur, 554, 211
Arge, C., \& Pizzo, V. 2000, IGR, 105, 10465
Bale, S., Badman, S., Bonnell, J., et al. 2019, Natur, 576, 237
Bertello, L., Pevtsov, A., Petrie, G., \& Keys, D. 2014, SoPh, 289, 2419
Buduma, N., \& Locascio, N. 2017, Fundamentals of Deep Learning: Designing Next-generation Machine Intelligence Algorithms (Sebastopol, CA: O'Reilly Media, Inc.)
Cairns, I. H., Lobzin, V., Donea, A., et al. 2018, NatSR, 8, 1676
Chen, Q., \& Koltun, V. 2017, Proc. IEEE ICCV, 1520, 1
DeVore, C. R., Sheeley, N., \& Boris, J. 1984, SoPh, 92, 1
Downs, C., Roussev, J. I., van der Holst, B., Lugaz, N., \& Sokolov, I. V. 2012, ApJ, 750, 134
Fox, N., Velli, M., Bale, S., et al. 2016, SSRv, 204, 7
Fresland, S. L., \& Handy, B. 1998, SoPh, 182, 497
Goodfellow, I., Bengio, Y., Courville, A., \& Bengio, Y. 2016, Deep Learning, Vol. 1 (Cambridge: MIT Press)
Hakamada, K., Kojima, M., Ohmi, T., Tokumaru, M., \& Fujiki, K. 2005, SoPh, 227, 387
Hickmann, K. S., Godinez, H. C., Henney, C. J., \& Arge, C. N. 2015, SoPh, 290, 1105
Howard, R. A., Moses, J., Vourlidas, A., et al. 2008, SSRv, 136, 67
Huang, X., Wang, H., Xu, L., et al. 2018, ApJ, 856, 7
Illarionov, E. A., \& Tlatov, A. G. 2018, MNRAS, 481, 5014
Inoue, S., Kusano, K., Büchner, J., \& Skála, J. 2018, NatCo, 9, 1
Isola, P., Zhu, J.-Y., Zhou, T., \& Efros, A. A. 2017, Proc. IEEE, 1125, 1
Jess, D. B., Reznikova, V. E., Ryans, R. S., et al. 2016, NatPh, 12, 179
Ji, E.-Y., Moon, Y.-J., \& Park, E. 2020, SpWea, 18, e02411
Kaiser, M. L., Kucera, T., Davila, J., et al. 2008, SSRv, 136, 5
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 5, 397
Kingma, D. P., \& Ba, J. 2015, arXiv:1412.6980
Lee, C., Luhmann, J., Hoeksema, J., et al. 2011, SoPh, 269, 367
Lemen, J. R., Akín, D. J., Boerner, P. F., et al. 2011, SoPh, 275, 17
Liewer, P., Qiu, J., \& Lindsey, C. 2017, SoPh, 292, 146
Lowder, C., Qiu, J., Leamon, R., \& Liu, Y. 2014, ApJ, 783, 142
Mikić, Z., Downs, C., Linker, J. A., et al. 2018, NatAs, 2, 913
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Mueller, D., Marsden, R. G., Cyr, O. S., et al. 2013, SoPh, 285, 25
Nandy, D., Bhowmik, P., Yeates, A. R., et al. 2018, ApJ, 853, 72
Nitta, N. V., Reames, D. V., DeRosa, M. L., et al. 2006, ApJ, 650, 438
Panasenco, O., Velli, M., Dmicis, R., et al. 2020, ApJS, 246, 54
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Park, E., Moon, Y.-J., Lim, D., \& Lee, H. 2020, ApJL, 891, L4
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Park, J., Innes, D., Bucik, R., \& Moon, Y.-J. 2013, ApJ, 779, 184
Pastor Yabar, A., Martínez González, M., \& Collados, M. 2015, MNRAS, 453, L69
Pessell, W. D., Thompson, B. J., \& Chamberlin, P. 2011, SoPh, 275, 3
Pomoell, J., \& Poedts, S. 2018, JSWSC, 8, A35
Riley, P., Linker, J., Mikić, Z., et al. 2006, ApJ, 653, 1510
Rust, D. M., Haggerty, D. K., Georgoulis, M. K., et al. 2008, ApJ, 687, 635
Schatten, K. H., Wilcox, J. M., \& Ness, N. F. 1969, SoPh, 6, 442
Scherrer, P. H., Schou, J., Bush, R., et al. 2012, SoPh, 275, 207
Schrijver, C. J., \& DeRosa, M. L. 2003, SoPh, 212, 165
Shin, G., Moon, Y.-J., Park, E., et al. 2020, ApJL, 895, L16
Solanki, S., Schüssler, M., \& Fligge, M. 2002, A\&A, 383, 706
Steenburgh, R., Biesecker, D., \& Millward, G. 2013, SoPh, 289, 675
Su, Y., \& Van Ballegooijen, A. 2012, ApJ, 757, 168
Subramanian, V. 2018, Deep Learning with PyTorch: A Practical Approach to Building Neural Network Models using PyTorch (Packt Publishing Ltd)
Sun, X., Liu, Y., Hoeksema, J., Hayashi, K., \& Zhao, X. 2011, SoPh, 270, 9
Ugarte-Urra, I., Upton, L., Warren, H. P., \& Hathaway, D. H. 2015, ApJ, 815, 90
Vourlidas, A. 2015, SpWea, 13, 197
Wang, T.-C., Liu, M.-Y., Zhu, J.-Y., et al. 2018, Proc. IEEE, 8798, 1
Wang, Y.-M., Hawley, S. H., \& Sheeley, N. R. 1996, Sci, 271, 464
Wang, Y.-M., \& Sheeley, N., Jr 1992, ApJ, 392, 310
Wang, Y.-M., \& Sheeley, N., Jr 2003, ApJ, 587, 818"
Eunsu Park et al 2019 - Generation of Solar UV and EUV Images from SDO HMI Magnetograms by Deep Learning.pdf,"# Generation of Solar UV and EUV Images from SDO/HMI Magnetograms by Deep Learning 

Eunsu Park ${ }^{1}$ (D) Yong-Jae Moon ${ }^{1}$ (D) Jin-Yi Lee ${ }^{2}$ (D), Rok-Soon Kim ${ }^{3,4}$ (D), Harim Lee ${ }^{2}$ (D), Daye Lim ${ }^{1}$ (D), Gyungin Shin ${ }^{1}$, and Taeyoung Kim ${ }^{1,5}$<br>${ }^{1}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{2}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>${ }^{3}$ Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{4}$ University of Science and Technology, Daejeon, 34113, Republic of Korea<br>${ }^{5}$ InSpace Co., Ltd., Daejeon, 34111, Republic of Korea

Received 2019 June 8; revised 2019 September 11; accepted 2019 September 21; published 2019 October 11


#### Abstract

In this Letter, we apply deep-learning methods to the image-to-image translation from solar magnetograms to solar ultraviolet (UV) and extreme UV (EUV) images. For this, We consider two convolutional neural network models with different loss functions, one (Model A) is with L1 loss $\left(L_{1}\right)$, and the other (Model B) is with $L_{1}$ and cGAN loss $\left(L_{\mathrm{cGAN}}\right)$. We train the models using pairs of Solar Dynamics Observatory (SDO)/Atmospheric Imaging Assembly (AIA) nine-passband ( $94,131,171,193,211,304,335,1600$, and $1700 \AA$ ) UV/EUV images and their corresponding $S D O /$ Helioseismic and Magnetic Imager (HMI) line-of-sight (LOS) magnetograms from 2011 to 2016. We evaluate the models by comparing pairs of $S D O /$ AIA images and the corresponding ones generated in 2017. Our main results from this study are as follows. First, the models successfully generate $S D O /$ AIA-like solar UV and EUV images from SDO/HMI LOS magnetograms. Second, in view of three metrics (pixel-to-pixel correlation coefficient, relative error, and the percentage of pixels having errors less than $10 \%$ ), the results from Model A are mostly comparable or slightly better than those from Model B. Third, in view of the rms contrast measure, the generated images by Model A are much more blurred than those by Model B because of $L_{\mathrm{cGAN}}$ specialized for generating realistic images.


Key words: methods: data analysis - Sun: atmosphere - techniques: image processing

## 1. Introduction

Over the past several decades, many astronomical observations have been made through multiwavelength observations. In the case of the Sun, several space missions, such as Solar Heliospheric Observatory (SOHO; Domingo et al. 1995), Hinode (Kosugi et al. 2007), Solar Terrestrial Relations Observatory (STEREO; Kaiser et al. 2008), and Solar Dynamics Observatory (SDO; Pesnell et al. 2012), have observed solar atmosphere, and they have captured various types of solar multiwavelength observations, such as ultraviolet (UV) images, extreme UV (EUV) images, and magnetograms. In particular, the UV and EUV observations by several instruments, including $S O H O /$ Extreme-ultraviolet Imaging Telescope (EIT; Delaboudinière et al. 1995), Transition Region And Coronal Explorer (TRACE; Handy et al. 1999), STEREO/ Extreme UltraViolet Imager (EUVI; Howard et al. 2008), and SDO/Atmospheric Imaging Assembly (AIA; Lemen et al. 2012), have allowed us to identify several solar atmospheric structures such as coronal holes, filaments, coronal loops, and active regions. In addition, they have provided us with detailed measurements of plasma parameters such as electron densities and temperature (e.g., Del Zanna \& Mason 2018). The magnetic field of the Sun, which is another important solar observational parameter, is the underlying cause of many diverse phenomena as a part of solar activity (Solanki et al. 2006; Wiegelmann et al. 2014). There have been several instruments for the measurements of solar photospheric magnetic fields such as $S O H O /$ Michelson Doppler Imager (MDI; Scherrer et al. 1995) and SDO/Helioseismic and

Magnetic Imager (HMI; Scherrer et al. 2012; Schou et al. 2012).

The deep neural network (DNN; Lecun et al. 2015), which is called ""Deep Learning,"" is a kind of artificial neural network, and is developed to learn the way humans think and recognize an object using their deep hierarchical layer structures. The DNN has become very popular and is applied to several fields due to a large amount of data (big data), advanced hardware, and improvements in machine-learning algorithms. The convolutional neural network (CNN; Lecun et al. 1998) is the most popular deep-learning method in the area of image processing and computer vision. In general, the CNN models consist of convolution filters, and the filters extract features from their data sets. The CNN models train the filters to select and extract features automatically without human intervention, while traditional machine-learning algorithms require researchers to manually select and construct feature extractors. Recently, the generative adversarial network (GAN; Goodfellow et al. 2014), which is one of the popular deep-learning methods, has been widely examined in several generations of tasks. In particular, Isola et al. (2016) suggested a general purposed solution based on a conditional generative adversarial network (cGAN; Mirza \& Osindero 2014) and a deep convolutional generative adversarial network (DCGAN; Radford et al. 2015) to resolve the image-to-image translation problems. The model by Isola et al. (2016) very successfully works for various types of image-to-image translations: labels to the street scenes, label to facade, black and white image to color one, satellite view to map, day view to night view, and even sketch image to photo.

There have been a few attempts to translate between solar images using deep-learning methods. Galvez et al. (2019)
applied a deep-learning model based on CNNs to generate solar UV/EUV images using $S D O / \mathrm{HMI}$ vector magnetograms. Kim et al. (2019) suggested a deep-learning model based on cGAN to generate solar magnetograms using $S D O /$ AIA images and then applied the model to STEREO/EUVI images to produce solar farside magnetograms.

In this Letter, we apply a deep-learning method to the image-to-image translation from solar magnetograms to solar UV and EUV images. For training the model, we use $S D O / \mathrm{HMI}$ and $S D O /$ AIA all-passband data. Then we quantitatively evaluate the results of the model and discuss them in view of underlying physics. This Letter is organized as follows. The data will be described in Section 2, and the image-to-image translation method in Section 3. Results are given in Section 4, and a brief summary is presented in Section 5.

## 2. Data

SDO is a spacecraft mission that investigates how a solar magnetic field is generated and structured and how this stored magnetic energy is released into the heliosphere and geospace as solar wind, energetic particles, and variations in the solar irradiance with three instruments (Pesnell et al. 2012). HMI is an instrument of the $S D O$, which is designed to measure the Doppler shift, intensity, and vector magnetic field at the solar photosphere (Scherrer et al. 2012; Schou et al. 2012). AIA, which is another instrument of the $S D O$, is designed to provide multiple simultaneous images with views of the entire solar corona and transition region (Lemen et al. 2012). We use pairs of $S D O / \mathrm{HMI}$ line-of-sight (LOS) magnetograms and $S D O /$ AIA nine-passband UV and EUV images. We select pairs of full-disk $S D O / \mathrm{HMI}$ magnetograms and $S D O /$ AIA images with 6 hr cadence (four pairs per day) from 2011 to 2017 for nine passbands ( $94,131,171,193,211,304,335,1600$, and 1700 $\AA$ ). We first make level 1.5 images by calibrating, rotating, and centering the images. We divide all AIA images by exposure time to make all AIA images have a homogeneous exposure condition ( $\mathrm{DN} \mathrm{s}^{-1}$ ). To compensate for the instrument degradation over time for seven AIA EUV passbands (Boerner et al. 2014), we find a degrading factor for each passband and date using a SolarSoft routine (aia_get_response.pro) with a reference date of 2011 January 1. Then, we apply each factor to the corresponding image. Also, we coalign the AIA and HMI images by fixing the solar disk size, and downsample the images to 1024 by 1024 for computational capability. We use an AIA image with the range of $0 \mathrm{DN} \mathrm{s}^{-1}$ for minimum and $2^{14}-1 \mathrm{DN} \mathrm{s}^{-1}$ for maximum. To input the AIA data to our models, we convert the data to the log scale ( $0-14$ ), then rescale to ( -1 to 1 ). Detail data pipeline code is available at our GitHub repository. ${ }^{6}$ We exclude images with poor quality, including too noisy images due to solar flares, incorrect header information, and untypical images for reasons such as the transit of a planet. As a result, we make a total of 9985 pairs of $S D O / \mathrm{HMI}$ magnetograms and $S D O /$ AIA images for each passband (total of 99,850 images). We separate our data sets into training, validation, and test in chronological order. We select 714 pairs for each passband from 2017 January to 2017 June for the validation data set, 727 pairs from 2017 July to 2017 December for the test data set, and the remaining 8544 pairs for the training data set.

[^0]
## 3. Method

We consider two CNN models, one is Model A with L1 loss $\left(L_{1}\right)$, and the other is Model B with $L_{1}$ and cGAN loss ( $L_{\mathrm{cGAN}}$ ). Details about functions of the two losses are described in Appendix A. The two models have a generative network, called ""generator."" The generator has an objective, which is to generate $S D O /$ AIA-like images using $S D O / \mathrm{HMI}$ magnetograms. The generators of the two models have the same structure, but we train them with different losses.

In the case of Model A, we train its generator by minimizing $L_{1}$ to satisfy the objective of the generator. To minimize the loss, we use the adaptive momentum estimation solver (ADAM; Kingma \& Ba 2014) as an optimizer for the generator; detailed hyperparameters are described in Appendix C. The training process of Model A is described in Appendix B. 1

In the case of Model B, we use both $L_{1}$ and $L_{\mathrm{cGAN}}$. To apply $L_{\mathrm{cGAN}}$, this model includes a discriminative network, called ""discriminator."" More details about the architectures of the two networks and codes are available at https://github.com/eunsupark/solar_euv_generation. Figure 1 shows the main structure of Model B. The discriminator has an objective, which is to distinguish pairs of $S D O /$ AIA images and $S D O / \mathrm{HMI}$ magnetograms, as denoted ""Real pair,"" from pairs of generated images and $S D O / \mathrm{HMI}$ magnetograms, as denoted ""Fake pair."" This process is based on a competition between the generator and the discriminator in that they have adversarial objectives to each other. We expect $L_{1}$ contributes to minimizing the difference between $S D O /$ AIA images and generated ones, and $L_{\mathrm{cGAN}}$ contributes to generating realistic $S D O /$ AIA-like images. To minimize or maximize the losses, we use the ADAM optimizer, the same as Model A, for the generator and the discriminator. The training process of Model B is described in Appendix B. 2

We train the models with 500,000 iterations, and we save the generators every 10,000 iterations. As a result, we acquire 50 generators while the generators (and the discriminator) are trained for 500,000 iterations ( $\sim 59$ epochs). Here one iteration is when one pair of images is trained in our model, and one epoch is when an entire training data set of 8544 pairs is done in our model. In the validation step, we compare the $S D O /$ AIA images with the generated images by the 50 generators using the validation data set, then we select the best model among the 50 saved generators. In the test step, we estimate the model performances of selected generator networks in the validation step. We repeat the training, validation, and test steps nine times for each passband, so we acquire a total of 450 generators and the 9 best generators for each passband.

## 4. Results and Discussion

To test our results, we calculate three types of metrics between $S D O /$ AIA images and the generated ones for the entire test data sets. The first metric is the pixel-to-pixel correlation coefficient (CC; higher is better). We can get high CC when our model generates well not only pixel values but also locations of brightening. The second metric is relative error (RE; closer to 0 is better) of total pixel value $\left(\Phi_{i}\right)$, which is given by

$$
\mathrm{RE}_{i}=\left(\Phi_{i}^{\text {generated }}-\Phi_{i}^{\text {AIA }}\right) / \Phi_{i}^{\text {AIA }}
$$


[^0]:    ${ }^{6}$ https://github.com/eunsu-park/solar_euv_generation
![img-0.jpeg](img-0.jpeg)

Figure 1. A flowchart and structures of Model B. G is the generator network, D is the discriminator network, H is an SDO/HMI magnetogram, A<sub>R</sub> is an SDO/AIA image, and A<sub>F</sub> is a Fake image by the generator. The blue box is a Real pair (H, A<sub>R</sub>), and the red box is a Fake pair (H, A<sub>F</sub>).

where i is a serial number of 727 test samples. The third metric is the percentage of pixels having errors less than 10% (PPE10; higher is better). For three metrics, we perform 4 × 4 binning of output images to make ones that are 256 by 256, and we consider pixels on the solar disk. In addition to the metrics, we calculate the rms contract measure (CM; higher is more clear) to measure the blurriness of the generated images, which is given by

$$
\text{CM}_i = \sqrt{\frac{1}{N} \sum (I_j - \bar{I})^2}, \tag{2}
$$

where i is a serial number of 727 test samples, j is a pixel number, I<sub>j</sub> is a pixel value of the jth pixel, and $\bar{I}$ is the average pixel values on the solar disk.

Table 1 shows the average CC, RE, PPE10, and CM values of the entire test data sets for each passband. For the three metrics and CM in Table 1, we unlog the output data and compare in the linear scale with the range of (0 DN s<sup>−1</sup>–2<sup>14</sup> − 1 DN s<sup>−1</sup>) as described in Section 2. CC values of Model A and Model B are higher than 0.69 and 0.66, and the average CC values of the two models are 0.84 and 0.83, respectively. The absolute values of RE of Model A and Model B are less than 0.18 and 0.17, and the average RE values of the two models are 0.07 and 0.06, respectively. PPE10 values of Model A range from 24.6% to 96.6% with an average value of 46.2%, and those of Model B range from 21.8% to 92.7% with an average value of 43.5%. The percentage of pixels having errors less than 50% of values of Model A range from 80.9% to 99.9% with an average value of 93.4%, and those of Model B range from 77.2% to 99.9% with an average value of 92.1%. In view of these three metrics, the results from Model A are mostly comparable or slightly better than those from Model B for most passbands. However, Model B has better CM values than Model A, which means that the results from Model A are more blurred than those from Model B. Figure 2 shows a comparison between real SDO/AIA images and the generated ones from the two models for a data set (nine passbands) at 18:00 UT on 2017 July 11. It was already noted that the results from the image translation models using L<sub>cGAN</sub> could be more realistic than those from the models using only L<sub>1</sub>, because that L<sub>1</sub> only contributes to minimizing the difference between the generated images and the target images, but L<sub>cGAN</sub> also contributes to generating realistic images (Isola et al. 2016; Ledig et al. 2016).

SDO/AIA observes the specific types of solar features depending on the passbands, which represent the characteristic temperatures of the primary ions for each passband (Lemen et al. 2012; see Table 1). Now we discuss our results according to the three groups classified by the temperature responses of the passbands with the results from Model B.

The first group corresponds to photospheric UV passbands, which are 1600 and 1700 Å. This group has CC values larger than 0.92 and absolute values of RE smaller than 0.11. In particular, the average correlation of 1700 Å is 0.95 (see Table 1), which is the best among all passbands. Galvez et al. (2019) also find that the 1600 and 1700 Å observations are easier to predict than other passbands. Figure 3 shows a comparison between generated 1600 and 1700 Å images and their corresponding AIA ones at 12:00 UT on 2017 August 7. As shown in the figure, the generated one is quite consistent with the real one, even the small-scale structures. This group has a high CC value (0.44 for 1700 Å and 0.46 for 1600 Å) with the corresponding SDO/HMI magnetograms compared to
Table 1 The Average Pixel-to-pixel Correlation Coefficient (CC), Relative Error (RE), the Percentage of Pixels Having Errors Less than 10\% (PPE10), and Root Mean Square Contrast Measure (CM) for Each Passband

|  Passband ( $\AA$ ) | Temperature (K) | Region (Lemen et al. 2012) | Remark | Step | CC |  | RE |  | PPE10 |  | CM |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|   |  |  |  |  | Model A | Model B | Model A | Model B | Model A | Model B | Model A | Model B  |
|  1700 | 4500 | temperature minimum, photosphere | Group 1 | validation | 0.97 | 0.95 | $-0.01$ | 0.01 | 98.2 | 93.8 | 193.18 | 208.91  |
|  1600 | 10,000 | transition region, upper photosphere |  | validation | 0.97 | 0.95 | 0.01 | 0.02 | 96.6 | 92.7 | 197.44 | 209.87  |
|   |  |  |  | validation | 0.94 | 0.92 | 0.04 | 0.06 | 81.0 | 73.3 | 6.75 | 8.57  |
|   |  |  |  | test | 0.94 | 0.92 | 0.11 | 0.11 | 52.3 | 50.4 | 6.88 | 9.23  |
|  304 | 50,000 | chromosphere, transition region | Group 2 | validation | 0.86 | 0.84 | $-0.06$ | $-0.05$ | 37.1 | 35.2 | 16.47 | 20.38  |
|  171 | 600,000 | quiet corona, upper transition region |  | test | 0.84 | 0.83 | $-0.18$ | $-0.17$ | 25.5 | 25.3 | 15.96 | 19.75  |
|   |  |  |  | validation | 0.72 | 0.68 | $-0.06$ | $-0.04$ | 28.0 | 25.7 | 69.46 | 86.13  |
|   |  |  |  | test | 0.69 | 0.66 | $-0.05$ | $-0.04$ | 27.3 | 25.3 | 66.26 | 80.73  |
|  193 | 1000,000 | corona, hot flare plasma | Group 3 | validation | 0.82 | 0.78 | 0.05 | 0.06 | 25.8 | 23.9 | 100.28 | 106.14  |
|  211 | 2000,000 | active region corona |  | test | 0.78 | 0.74 | 0.07 | 0.07 | 24.6 | 23.3 | 95.54 | 102.50  |
|   |  |  |  | validation | 0.88 | 0.81 | 0.07 | 0.07 | 25.7 | 22.5 | 43.49 | 44.77  |
|   |  |  |  | test | 0.86 | 0.78 | 0.08 | 0.08 | 25.2 | 21.8 | 40.45 | 43.02  |
|  335 | 2500,000 | active region corona |  | validation | 0.89 | 0.88 | $-0.03$ | 0.04 | 49.3 | 46.1 | 2.57 | 3.30  |
|   |  |  |  | test | 0.86 | 0.85 | $-0.03$ | 0.04 | 49.2 | 46.3 | 2.47 | 3.10  |
|  94 | 6000,000 | flaring corona |  | validation | 0.83 | 0.79 | $-0.02$ | $-0.02$ | 71.5 | 67.2 | 0.61 | 0.70  |
|   |  |  |  | test | 0.79 | 0.75 | $-0.03$ | $-0.02$ | 72.0 | 67.4 | 0.60 | 0.72  |
|  131 | 10,000,000 | transition region, flaring corona |  | validation | 0.84 | 0.81 | $-0.04$ | $-0.02$ | 43.7 | 40.0 | 2.94 | 3.42  |
|   |  |  |  | test | 0.81 | 0.78 | $-0.04$ | $-0.03$ | 42.9 | 39.4 | 2.77 | 3.29  |
|   |  | Test Average |  |  | 0.84 | 0.83 | 0.07 | 0.06 | 46.2 | 43.5 | 47.60 | 52.47  |
![img-1.jpeg](img-1.jpeg)

Figure 2. Comparison between real $S D O /$ AIA images and the generated ones for a data set (nine passbands) at 12:00 UT on 2017 July 11. The first column represents generated images by Model A, the second column represents generated images by Model B, and the third column represents $S D O /$ AIA images.
other passbands (average value of 0.36 ). This pronounced similarity between brightness in $1600 \AA$ and the underlying photospheric magnetic field is reported by Loukitcheva et al. (2009) and Barczynski et al. (2018). These results imply that these similar structures have caused the high CC and RE scores.
The second group corresponds to chromospheric, transition region, and quiet coronal EUV passbands, which are 171 and $304 \AA$. Figure 4 shows a comparison between generated 171 and $304 \AA$ images and their corresponding AIA ones at 12:00 UT on 2017 August 11. As shown in the $171 \AA$ images, the
morphology of generated loops does not well match that of the AIA ones. These detailed structures observed with high resolution are not clearly observed in other passbands, which means that our model suffers from greater difficulty in generating in local structures than in global structures. In the case of $304 \AA$ images, the average CC value is 0.83 with -0.17 RE, which is not so good compared to that of group 1. The reason may be due to complex chromospheric structures such as filaments and the instrument degradation of this passband. In fact, it is well known that the instrument degradation over time for the $304 \AA$ EUV passband is the largest among all passbands
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison between generated 1600 and $1700 \AA$ images by Model B and $S D O /$ AIA ones at 12:00 UT on 2017 August 7. The first column represents generated images by Model B, the second column represents $S D O /$ AIA ones, and the last column represents the difference maps between generated images and $S D O /$ AIA ones.
(Boerner et al. 2014). Even though we have made a correction using the degrading factor, there is a possibility that the instrument degradation effects still have remained.

The last group corresponds to active region coronal and flaring coronal EUV passbands, which are all the remaining ones as shown in Table 1. This group has CC values greater than 0.74 and absolute values of RE less than 0.08 . These results seem to be related to the fact that strong magnetic fields in HMI magnetograms are mostly located in solar active regions, and EUV brightenings are mostly found in active regions and flaring sites. We understand that our model learns well the relationship between magnetic fields and EUV brightness. Our results are also supported by that the EUV brightenings are due to the heating by small-scale magnetic field reconnections such as nanoflare heating and/or by MHD waves (Parker 1972; Sturrock \& Uchida 1981; Parker 1983; van Ballegooijen 1986; Heyvaerts \& Priest 1992).

## 5. Conclusion and Summary

In this Letter, we have applied a deep-learning method to the image-to-image translation from solar magnetograms to solar UV and EUV images. We have trained two CNN models, one is with $L_{1}$, and the other is with $L_{1}$ and $L_{\mathrm{cGAN}}$. We selected 9985 pairs of the nine-passband $S D O /$ AIA UV and EUV images and the corresponding $S D O / \mathrm{HMI}$ magnetograms. We separated our data sets into training, validation, and test sets in
chronological order. We trained our models using 8544 pairs from 2011 January to 2016 December, validated our models using 714 pairs from 2017 January to 2017 June, and tested our models using 727 pairs from 2017 July to 2017 December.

The main results of this study are as follows. First, the models successfully generated $S D O /$ AIA-like solar UV and EUV images from $S D O /$ HMI magnetograms. Second, CC values from Model A range from 0.69 to 0.97 , and those from Model B range from 0.66 to 0.95 . Third, RE values from Model A and Model B are within -0.18 and -0.17 , respectively. Fourth, $46.2 \%$ and $43.5 \%$ of pixels of the generated images from Model A and Model B have a pixel value error within $10 \%$, respectively. Fifth, in view of the three metrics (CC, RE, and PPE10), the results from Model A are mostly comparable or slightly better than those from Model B. Sixth, in view of CM, the generated images by Model A are much more blurred than those by Model B because of $L_{\mathrm{cGAN}}$ being specialized for generating realistic images. Then we have briefly discussed our results in view of the physical connection between photospheric magnetic fields and the formation of UV/EUV passbands. Further detailed discussions are beyond the scope of this Letter and remain as open questions.

In this study we have trained, validated, and tested the models using chronologically separated data sets. This approach is suitable in view of the space weather operator (Nishizuka et al. 2017; Park et al. 2018), but the results from this approach could not fully consider the solar cycle phase
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparison between generated 171 and $304 \AA$ images by Model B and SDO/AIA ones at 12:00 UT on 2017 August 7. The first column represents generated images by Model B, the second column represents $S D O /$ AIA ones, and the last column represents the difference maps between generated images and $S D O /$ AIA ones.
effect and the time-varied AIA degradation effects. Thus, there is a possibility that the results may be improved if we use other data sets such as randomly or nonchronologically separated sets.

Recently, UV/EUV observations have become more important in space weather. For example, coronal holes, which are observed from the UV/EUV observations, have become a major cause of space weather disturbances, while solar activities are very quiet. However, due to the absorption by the Earth's atmosphere, these observations have been possible since the late 1990s, the so-called SOHO era. Before the SOHO era, there were several observational blanks such as the ""EUV Hole"" (Tobiska et al. 2000) because of the limitations of EUV as seen through satellite observations. On the other hand, the magnetic field of the Sun has been continuously observed at various ground stations using vector magnetograms since the 1970s (Livingston et al. 1976; Jones et al. 1992). If we can generate EUV images from the magnetograms, we can extend our study on solar activity and space weather effects using more extended data sets.

Our results could be improved by considering other data sets such as vector magnetic fields to complement the morphology of generated structures. Also, we are trying to improve the results by developing other models such as image translation models for high-resolution images to generate small-scale structures. In the future, video translation models would be more promising by considering temporal and spatial evolution
together. Our results have demonstrated a sufficient possibility that this methodology can be applied to many scientific fields that use several multiwavelength images, in not only astronomy but also other scientific fields.

This work was supported by the BK21 plus program through the National Research Foundation (NRF) funded by the Ministry of Education of Korea, the Basic Science Research Program through the NRF funded by the Ministry of Education (NRF-2013M1A3A3A02042232, NRF-2016R1A2B4013131, NRF-2019R1A2C1002634), the Korea Astronomy and Space Science Institute (KASI) under the R\&D program ""Study on the Determination of Coronal Physical Quantities using Solar Multi-wavelength Images (project No. 2019-1-850-02)"" supervised by the Ministry of Science and ICT, and Institute for Information \& communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01422, Study on analysis and prediction technique of solar flares). We thank the numerous team members who have contributed to the success of the SDO mission. We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: NumPy (numpy.org), Keras (keras.io), TensorFlow (tensorflow.org), SunPy (sunpy. org), and SolarSoft.
## Appendix A Loss function

The function of $L_{\mathrm{cGAN}}$ is described as follows:

$$
\begin{aligned}
L_{\mathrm{cGAN}}(G, D)= & E_{x, y}[\log D(x, y)] \\
& +E_{x}\{\log (1-D(x, G(x))]
\end{aligned}
$$

where $G$ is the generator, $D$ is the discriminator, $x, y$, and $G(x)$ are the real input, real output, and fake output, respectively. $D$ $(x, y)$ is the probability calculated by the discriminator using Real pair, and $D(x, G(x))$ is the probability calculated by the discriminator using Fake pair. $G$ tries to minimize the $L_{\mathrm{cGAN}}$ loss function against an adversarial $D$ that tries to maximize the $L_{\mathrm{cGAN}}$ loss function. The function of $L_{1}$ is described as follows:

$$
L_{1}=E_{x, y}(\|y-G(x)\|)
$$

$G$ tries to minimize this the $L_{1}$ loss. The final loss function for this work can be found with $G^{*}$ given by

$$
G^{*}=\operatorname{argmin}_{G} \max _{D} L_{\mathrm{cGAN}}(G, D)+\lambda L_{1}(G)
$$

where $\lambda$ is the relative weight of $L_{\mathrm{cGAN}}$ loss and $L_{1}$ loss. In this work, we used 100 for the relative weight like Isola et al. (2016).

## Appendix B Training Process

## B.1. Model A

We train Model A as follows:

1. We prepare an $S D O /$ AIA image and a corresponding $S D O /$ HMI magnetogram.
2. The generator generates an $S D O /$ AIA-like image from the $S D O /$ HMI magnetogram.
3. The model calculates $L_{1}$ between the $S D O /$ AIA image and the generated one, then back-propagates the value of $L_{1}$ to the generator.
4. The generator updates itself by minimizing $L_{1}$ to generate more realistic images.
5. We iterate the above steps.

## B.2. Model B

We train Model B as follows:

1. We prepare an $S D O /$ AIA image and a corresponding $S D O /$ HMI magnetogram.
2. The generator generates an $S D O /$ AIA-like image from the $S D O /$ HMI magnetogram.
3. The model calculates $L_{1}$ between the $S D O /$ AIA image and the generated one, then back-propagates the value of $L_{1}$ to the generator.
4. The discriminator distinguishes the Real pair from the Fake pair, then returns the result as a percentage value ( 0 for Fake pair, 1 for Real pair).
5. The model calculates $L_{\mathrm{cGAN}}$ using the result, then backpropagates $L_{\mathrm{cGAN}}$ to the generator and the discriminator.
6. The generator updates itself by minimizing both $L_{\mathrm{cGAN}}$ and $L_{1}$ to generate more realistic images.
7. The discriminator updates itself by maximizing $L_{\mathrm{cGAN}}$ to distinguish well both the Real pair and Fake pair.
8. We iterate the above steps.

## Appendix C Hyperparameter

The initializer for the Convolution layers and the Convolu-tion-Transpose layers is a normal distribution with 0.0 mean and 0.02 standard deviation. The initializer for the BatchNormalization layers is a normal distribution with 1.0 mean and 0.02 standard deviation. We use the ADAM solver as the optimizer with a learning rate of $2 \times 10^{-4}$, momentum $\beta 1$ of 0.5 , and momentum $\beta 2$ of 0.999 .

## ORCID iDs

Eunsu Park (2) https://orcid.org/0000-0003-0969-286X
Yong-Jae Moon (2) https://orcid.org/0000-0001-6216-6944
Jin-Yi Lee (2) https://orcid.org/0000-0001-6412-5556
Rok-Soon Kim (2) https://orcid.org/0000-0002-9012-399X
Harim Lee (2) https://orcid.org/0000-0002-9300-8073
Daye Lim (2) https://orcid.org/0000-0001-9914-9080

## References

Barczynski, K., Peter, H., Chitta, L. P., et al. 2018, A\&A, 619, A5
Boerner, P. F., Testa, P., Warren, H., et al. 2014, SoPh, 289, 2377
Del Zanna, G., \& Mason, H. E. 2018, LRSP, 15, 5
Delaboudinière, J.-P., Artzner, G. E., Brunaud, J., et al. 1995, SoPh, 162, 291
Domingo, V., Fleck, B., \& Poland, A. I. 1995, SoPh, 162, 1
Galvez, R., Fouhey, D. F., Jin, M., et al. 2019, ApJS, 242, 7
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., et al. 2014, arXiv:1406.2661
Handy, B. N., Acton, L. W., Kankelborg, C. C., et al. 1999, SoPh, 187, 229
Heyvaerts, J., \& Priest, E. R. 1992, ApJ, 390, 297
Howard, R. A., Moses, J. D., Vourlidas, A., et al. 2008, SSRv, 136, 67
Isola, P., Zhu, J.-Y., Zhou, T., et al. 2016, arXiv:1611.07004
Jones, H. P., Duval, T. L., Harvey, J. W., et al. 1992, SoPh, 139, 211
Kaiser, M. L., Kucera, T. A., Davila, J. M., et al. 2008, SSRv, 136, 5
Kim, T., Park, E., Lee, H., et al. 2019, NatAs, 3, 397
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kosugi, T., Matsuzaki, K., Sakao, T., et al. 2007, SoPh, 243, 3
Lecun, Y., Bengio, Y., \& Hinton, G. 2015, Natur, 521, 436
Lecun, Y., Bottou, L., Bengio, Y., \& Haffner, P. 1998, Proc. IEEE, 86, 2278
Ledig, C., Theis, L., Huszar, F., et al. 2016, arXiv:1609.04802
Lemen, J. R., Title, A. M., Akin, D. J., et al. 2012, SoPh, 275, 17
Livingston, W. C., Harvey, J., Pierce, A. K., et al. 1976, ApOpt, 15, 33
Loukitcheva, M., Solanki, S. K., \& White, S. M. 2009, A\&A, 497, 273
Mirza, M., \& Osindero, S. 2014, arXiv:1411.1784
Nishizuka, N., Sugiura, K., Kubo, Y., et al. 2017, ApJ, 835, 156
Park, E., Moon, Y.-J., Shin, S., et al. 2018, ApJ, 869, 91
Parker, E. N. 1972, ApJ, 174, 499
Parker, E. N. 1983, ApJ, 264, 642
Pesnell, W. D., Thompson, B. J., \& Chamberlin, P. C. 2012, SoPh, 275, 3
Radford, A., Metz, L., \& Chintala, S. 2015, arXiv:1511.06434
Scherrer, P. H., Bogart, R. S., Bush, R. I., et al. 1995, SoPh, 162, 129
Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, SoPh, 275, 207
Schou, J., Scherrer, P. H., Bush, R. I., et al. 2012, SoPh, 275, 229
Solanki, S. K., Inhester, B., \& Schüssler, M. 2006, RPPh, 69, 563
Sturrock, P. A., \& Uchida, Y. 1981, ApJ, 246, 331
Tobiska, W. K., Woods, T., Eparvier, F., et al. 2000, JASTP, 62, 1233
van Ballegooijen, A. A. 1986, ApJ, 311, 1001
Wiegelmann, T., Thalmann, J. K., \& Solanki, S. K. 2014, A\&ARv, 22, 78"
Hyun-Jin Jeong et al 2025 - Prediction of the Next Solar Rotation Synoptic Maps Using an Artificial Intelligence–based Surface Flux Transport Model.pdf,"# Prediction of the Next Solar Rotation Synoptic Maps Using an Artificial Intelligencebased Surface Flux Transport Model 

Hyun-Jin Jeong ${ }^{1,2}$ (D) Mingyu Jeon ${ }^{2}$ (D) Daeil Kim ${ }^{2}$ (D) Youngjae Kim ${ }^{2}$ (D) Ji-Hye Baek ${ }^{3,4}$ (D) Yong-Jae Moon ${ }^{2,3}$ (D), and Seonghwan Choi ${ }^{3,4}$ (D)<br>${ }^{1}$ Centre for mathematical Plasma Astrophysics, Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium; jeong_hj@khu.ac.kr<br>${ }^{2}$ School of Space Research, Kyung Hee University, Yongin, 17104, Republic of Korea; moonyj@khu.ac.kr<br>${ }^{3}$ Technology Center for Astronomy and Space Science, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{4}$ Space Science Division, Korea Astronomy and Space Science Institute, Korea Astronomy and Space Science Institute, Daejeon, 34055, Republic of Korea<br>${ }^{5}$ Department of Astronomy and Space Science, College of Applied Science, Kyung Hee University, Yongin, 17104, Republic of Korea<br>Received 2024 December 6; revised 2025 March 12; accepted 2025 March 21; published 2025 April 16


#### Abstract

In this study, we develop an artificial intelligence (AI)-based solar surface flux transport (SFT) model. We predict synoptic maps for the next solar rotation ( 27.2753 days) using deep learning. Our model takes the latest synoptic maps and their sine-latitude grid data as inputs. Synoptic maps, which represent global magnetic field distributions on the solar surface, have been widely used as initial boundary conditions in the Sun and space-weather prediction models. Here we train and evaluate our deep-learning model, based on the Pix2PixCC architecture, using data sets of Solar Dynamics Observatory/Helioseismic and Magnetic Imager, Solar and Heliospheric Observatory/ Michelson Doppler Imager, and National Solar Observatory/Global Oscillation Network Group synoptic maps with a resolution of 360 by 180 (longitude and sine latitude) from 1996 to 2023. We present results of our model and compare them with those from the persistent model and the conventional SFT model, including the effects of differential rotation, meridional flow, and diffusion on the solar surface. The average pixel-to-pixel correlation coefficient between the target and our AI-generated data, after 10 by 10 binning with a $10^{6}$ resolution in longitude, is 0.71 . This result is qualitatively similar to the results of the conventional SFT model $(0.65-0.68)$ and better than the results of the persistent model ( 0.56 ). Our model successfully generates magnetic features, such as the diffusion of solar active regions and the motions of supergranules. Using synthetic input data with bipolar structures, we confirm that our model successfully reproduces differential rotation and meridional flow. Finally, we discuss the advantages and limitations of our model in view of magnetic field evolution and its potential applications.


Unified Astronomy Thesaurus concepts: Solar magnetic fields (1503); The Sun (1693); Astronomy data analysis (1858); Convolutional neural networks (1938)

## 1. Introduction

Magnetic fields on the solar surface play a crucial role in shaping the large-scale structure of the Sun's atmosphere and driving space-weather disturbances that affect Earth's environment (D. H. Mackay \& A. R. Yeates 2012; D. Nandy et al. 2023). Various numerical schemes for predicting the solar corona and space weather have used global solar photospheric magnetic field data as model inputs (M. J. Owens et al. 2014). The global photospheric field data are routinely provided as synoptic maps, produced by merging a series of line-of-sight full-disk magnetograms over a solar synodic rotation period of 27.2753 days (referred to as a Carrington rotation or CR). For each magnetogram, the data within a certain longitudinal range of the central meridian are used to construct the synoptic maps (R. Howard et al. 1969; J. T. Hoeksema \& P. H. Scherrer 1986). The synoptic maps, available over several decades, have facilitated understanding of the evolution and transport of solar magnetic flux on timescales ranging from a solar rotation to solar cycles (L. E. A. Vieira \& S. K. Solanki 2010; S. Gosain et al. 2013).

The surface flux transport (SFT) model has been used to predict the distribution of the Sun's magnetic fields and has

Original content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI.
been shown to provide a good description of the global evolution after magnetic field emergence (R. H. Cameron et al. 2012). The conventional SFT model is based on the idea that radial magnetic flux on the solar surface is transported by horizontal plasma flow but without any back-reaction on these flows (R. B. Leighton 1964). Accurate modeling of SFT requires observationally constrained descriptions of large-scale flow parameters, such as differential rotation, meridional circulation, and supergranular diffusion. On the solar surface, equatorial regions rotate faster than polar regions. Poleward meridional circulation drives the polarity reversal of the solar magnetic fields. Magnetic field diffusion is generated by the random motions of supergranules on the solar surface. Each SFT parameter has been empirically determined based on longterm photospheric observations, and the model results are highly dependent on their values (T. Whitbread et al. 2017).

Deep learning, also broadly known as artificial intelligence (AI), has been used to predict magnetic fields on the solar surface. L. Bai et al. (2021) predicted the evolution of a solar active region's magnetic fields over the next 6 hr using a 12 hr sequence of Spaceweather HMI Active Region Patch data sets with a spatiotemporal long short-term memory network. F. P. Ramunno et al. (2024) predicted the next 24 hr of solar full-disk magnetograms using Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager (HMI) data sets with a denoising diffusion probabilistic model. However, it has been challenging to compare global magnetic flux transport
![img-0.jpeg](img-0.jpeg)

Figure 1. Overview of the conventional synoptic map and SFT model parameters, and our proposed approaches to predict the next solar rotation magnetic fields. (a) A synoptic map is constructed from magnetic field data near the Sun's central meridian over a solar rotation. (b) The conventional surface flux transport model, describing the long-term evolution of the photospheric magnetic fields, includes differential rotation, meridional flow, and supergranular diffusion. (c) Our deep neural network generates the next solar rotation photospheric field data from the latest synoptic map and its sine-latitude grid data.
between the results of the conventional SFT model and AIbased prediction models when using local or full-disk magnetic field predictions with several-hour-ahead results. J. J. Athalathil et al. (2024) used a physics-informed neural network-based model to study the evolution of bipolar magnetic regions, aiming to enhance computational accuracy in simulating SFT equations on the solar surface by overcoming the challenges of conventional grid-based methods. However, their model did not incorporate observed data for training, and they planed to use real magnetic field data in future studies.

In the present study, we propose a data-driven AI-based SFT model to predict the evolution of global magnetic fields on the solar surface during a solar rotation. We use synoptic maps and their sine-latitude grid data as the model input, and the next solar rotation synoptic maps as the model output. Our AI-based SFT model is based on the Pix2PixCC architecture, which can generate realistic magnetic field data without any artificial saturation limits from the model inputs (H.-J. Jeong et al. 2022). Figure 1 shows the construction of a synoptic map, three main parameters of the conventional SFT model, and our AIbased SFT model's input and output data configurations. We describe our data configurations in Section 2 and model structures in Section 3. We show the evaluation metric results of our AI-based SFT model and compare our model results with those from the persistent model and the conventional SFT models in Section 4. We summarize and conclude our study in Section 5.

## 2. Data

Here we use Solar and Heliospheric Observatory (SOHO)/ Michelson Doppler Imager (MDI) synoptic maps from 1996 June to 2010 November, SDO/HMI synoptic maps from 2010

May to 2023 December, and National Solar Observatory (NSO)/Global Oscillation Network Group (GONG) synoptic maps from 2006 August to 2023 December to train and evaluate our deep-learning model. Polar fields that are missing or of poor quality from HMI and MDI synoptic maps are filled using a two-dimensional spatial-temporal interpolation correction scheme from X. Sun et al. (2011) and those from GONG synoptic maps are filled using a cubic polynomial surface fit to the currently observed magnetic fields at neighboring latitudes. The full CR HMI, MDI, and GONG synoptic maps have resolutions of $3600 \times 1440,3600 \times 1080$, and $360 \times 180$, equally spaced in longitude and sine latitude, respectively. We interpolate all maps to a uniform $360 \times 180$ grid in longitude and sine latitude without applying cross calibrations. We make them $512 \times 256$ data for computation by adding proper padding (same data for the left and right sides, and reflected data for the upper and lower sides), considering the spherical surface. For the model evaluation, we use $360 \times 180$ data extracted from the $512 \times 256$ model input and output corresponding to the synoptic maps. The resolution of our model input and output is approximately $11^{\prime \prime}$ per pixel.

For this study, we collected 592 synoptic maps from CR 1910 to CR 2279, including data from solar cycles 23 and 24 and data from the ascending phase of solar cycle 25 , to train, validate, and test our model. Since the number of synoptic maps is insufficient, we augment the training data set approximately 360 times by shifting $1^{\prime \prime}$ in longitude to provide a more diverse data set during training (C. Shorten \& T. M. Khoshgoftaar 2019; S. Rahman et al. 2023). Data augmentation is not applied to the validation and test data sets. We use pairs of synoptic maps and their sine-latitude grid data as the input and the next solar rotation synoptic maps as the
output of our model, ensuring no duplication between the training, validation, and test data sets. We consider two consecutive years for training, six consecutive months for validation, and six consecutive months for testing from 1996 June to 2023 December. To ensure that our model's performance is generalized and not limited to specific data sets, we employ K-fold cross validation with six folds $(K=6)$, enabling the entire set of synoptic maps to be used for testing. We train our model six times with different data splits and report the averaged metric results from the six trained models. For each fold iteration, we take approximately 90,000 (multiplied by 360), 100, and 100 pairs for the training, validation, and test data sets, respectively.

## 3. Methods

### 3.1. Conventional SFT Model

The conventional SFT model describes the passive transport of radial magnetic fields $\left(B_{r}\right)$ on the solar surface under the effects of differential rotation, meridional flow, and supergranular diffusion as shown in Figure 1(b). In this study, we use empirical parameters and the equation of the conventional SFT model (A. R. Yeates et al. 2023):

$$
\begin{aligned}
\frac{\partial B_{r}}{\partial t}= & -\Omega(\theta) \frac{\partial B_{r}}{\partial \phi}-\frac{1}{R_{\odot} \sin \theta} \frac{\partial}{\partial \theta}\left[u(\theta) B_{r} \sin \theta\right]+\eta_{\mathrm{H}} \\
& \times\left[\frac{1}{R_{\odot}^{2} \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \frac{\partial B_{r}}{\partial \theta}\right)+\frac{1}{R_{\odot}^{2} \sin ^{2} \theta} \frac{\partial^{2} B_{r}}{\partial \phi^{2}}\right]
\end{aligned}
$$

where $\phi, \theta, R_{\odot}, \Omega(\theta), u(\theta)$, and $\eta_{\mathrm{H}}$ represent the longitude, latitude, solar radius, differential rotation, meridional circulation, and horizontal diffusion, respectively. The equation is derived from the radial component of the mean-field magnetohydrodynamic induction equation at the solar surface, assuming that the field at the surface is purely vertical (J. Jiang et al. 2014). We use an empirically derived velocity profile for the differential rotation as determined by H. B. Snodgrass \& R. K. Ulrich (1990): $\Omega(\theta)=0.18-2.396 \cos ^{2} \theta-$ $1.787 \cos ^{4} \theta$, where $\Omega(\theta)$ is expressed in units of degrees per day. The constant term in the differential rotation profile is provided in the Carrington frame, which is commonly used in conventional SFT models. For the meridional flow, we use the profile $u(\theta)=-R_{\odot} \Delta_{u} \cos \theta \sin ^{\rho} \theta$, where $\Delta_{u}$ represents the flow divergence at the solar equator. For this study, we adopt $\Delta_{u}=6.9 \times 10^{-8} \mathrm{~s}^{-1}, p=3.87$, and $\eta_{\mathrm{H}}=425 \mathrm{~km}^{2} \mathrm{~s}^{-1}$. For more details, refer to A. R. Yeates et al. (2023); its baseline code is available at https://github.com/antyeates1983/sft_data.

### 3.2. AI-based SFT Model

We employ a deep-learning model based on the Pix2PixCC, as illustrated in Figure 2. Our model consists of three major components: a generator, a discriminator, and an inspector. The generator aims to produce target-like outputs from inputs, guided by objective functions from the discriminator and the inspector during the training. In this study, we replace the generator of the Pix2PixCC proposed by H.-J. Jeong et al. (2022) with the U-Net. The U-Net features a symmetric U-shaped architecture, consisting of encoder convolutional layers and decoder transposed convolutional layers, connected by skip connections (O. Ronneberger et al. 2015). Skip connections provide detailed features from previous encoder
layers to decoder layers, enabling the generation of more precise and representative outputs. The U-Net with skip connections is widely used in video frame prediction tasks (Y. Qiang et al. 2020; B. Q. Bastos et al. 2021), and has been applied to predict the global total electron content of the ionosphere for one-day space-weather forecasting (S. Lee et al. 2021). The discriminator distinguishes between real pairs (input and target data) and generated pairs (input and the generator output), and the inspector calculates correlation coefficients (CCs) between targets and the generator outputs to improve the consistency between target and output.

The discriminator extracts features via convolutional layers, enabling the calculation of a feature-matching loss $\left(\mathcal{L}_{\mathrm{FM}}\right)$, which optimizes the generator. Unlike direct data differences, $\mathcal{L}_{\mathrm{FM}}$ minimizes the discrepancy between feature maps of real and generated pairs. This approach is particularly effective for handling data with a broad dynamic range (A. Rana et al. 2019; D. Marnerides et al. 2021):

$$
\mathcal{L}_{\mathrm{FM}}(G, D)=\sum_{i=1}^{T} \frac{1}{N_{i}}\left\|D^{(i)}(x, y)-D^{(i)}(x, G(x))\right\|
$$

where $G, D, T, i, N_{i}, x$, and $y$ denote the generator, the discriminator, the total number of layers, the serial number of the layers, the number of elements in output feature maps of each layer, the input data, and the target data, respectively. $D^{(i)}$ represents the $i$ th layer of $D$, and $G(x)$ is the generator's output.

For the adversarial process to produce realistic outputs, we adopt the least-squares generative adversarial network (LSGAN) losses (X. Mao et al. 2017), defined as

$$
\begin{aligned}
& \mathcal{L}_{\mathrm{LSGAN}}^{G}(G, D)=\frac{1}{2}(D(x, G(x))-1)^{2} \\
& \mathcal{L}_{\mathrm{LSGAN}}^{D}(G, D)=\frac{1}{2}(D(x, y)-1)^{2}+\frac{1}{2}(D(x, G(x)))^{2}
\end{aligned}
$$

where $D(x, G(x))$ and $D(x, y)$ represent probabilities ranging from 0 (generated) to 1 (real), computed by the discriminator for generated and real pairs, respectively. The generator minimizes the $\mathcal{L}_{\mathrm{LSGAN}}^{G}$, while the discriminator minimizes $\mathcal{L}_{\mathrm{LSGAN}}^{D}$. The competition between the generator and the discriminator contributes to generating realistic outputs. The performance of adversarial objectives has been well demonstrated in image-to-image translation tasks for solar data (E. Park et al. 2019; H.-J. Jeong et al. 2020, 2022).

To stabilize training, we adopt a CC loss $\left(\mathcal{L}_{\mathrm{CC}}\right)$ based on Lin's concordance CC (L. I.-K. Lin 1989), which balances positive and negative polarity fields better than error-based losses (R. Vallejos et al. 2020; B. T. Atmaja \& M. Akagi 2021). The $\mathcal{L}_{\mathrm{CC}}$, computed at multiple scales, is expressed as

$$
\mathcal{L}_{\mathrm{CC}}(G)=\sum_{i=0}^{T} \frac{1}{T+1}\left(1-\mathrm{CC}_{i}(y, G(x))\right)
$$

where $T$ and $i$ denote the total number of downsampling by a factor of 2 and the serial number of the downsampling, respectively. $\mathrm{CC}_{i}$ indicates the CC value between the $2^{i}$ times downsampled target and generated data. The average of the CC values from multiscale targets and AI-generated results guides the model in optimizing its parameters. In addition, with the
![img-1.jpeg](img-1.jpeg)

Figure 2. Flowchart and structures of our deep-learning model based on the Pix2PixCC. The generator produces target-like data from input data. The discriminator trains to distinguish between the real pair (input and target data) and the generated pair (input and AI-generated data). The inspector computes concordance CCs between the target data and generated data. The generator's parameters are updated from the losses calculated by the inspector and discriminator during the model-training process.

help of LCC, we avoid enforcing artificial saturation limits on our model.

The final objective functions are

$$
\begin{aligned}
& \min_{G}\left(\lambda_1 \mathcal{L}_{\text{LSGAN}}^G(G, D) + \lambda_2 \mathcal{L}_{\text{FM}}(G, D) + \lambda_3 \mathcal{L}_{\text{CC}}(G)\right) \\
& \min_{D} \left(\mathcal{L}_{\text{LSGAN}}^D(G, D)\right),
\end{aligned}
$$

where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are hyperparameters that control the importance of $\mathcal{L}_{\text{LSGAN}}^G$, $\mathcal{L}_{\text{FM}}$, and $\mathcal{L}_{\text{CC}}$, respectively. We set $\lambda_1$, $\lambda_2$, and $\lambda_3$ to 2, 10, and 5, respectively, following H.-J. Jeong et al. (2022). We optimize the objective functions using the adaptive moment estimation (Adam; D. P. Kingma & J. Ba 2014) optimizer with a learning rate of 0.0002 over 1,000,000 iterations, and select the best-performing model using validation data sets based on evaluation metrics.

## 4. Results and Discussion

### 4.1. Quantitative Comparison

We evaluate the performance of our deep-learning model using three metrics: rms error (RMSE), feature similarity index measure (FSIM), and pixel-to-pixel Pearson CC between the target data and the model-predicted data in the test data sets. The RMSE measures the average differences between each pair of corresponding pixels in the output and target data. The FSIM quantifies low-level feature similarity using phase congruency and gradient magnitude to identify the local quality of the data (L. Zhang et al. 2011). The phase congruency computes the alignment of phase and amplitude across individual frequency components in the frequency domain, and the gradient magnitude measures contrast variation. The FSIM has been widely used to evaluate how blurry, noisy, or distorted the output is compared to the target data (U. Sara et al. 2019). We compute the pixel-to-pixel CC after 10 × 10 binning, which results in a 10° resolution in longitude, to compare the large-scale structure of magnetic fields on the solar surface (T. Getachew et al. 2019). The FSIM and the pixel-to-pixel CC range from 0 to 1, where 1 represents perfect agreement.

Table 1 shows that the quantitative evaluation results of our AI-based SFT model are similar to those of the conventional SFT model and better than those of the persistent model. The persistent model is based on the assumption that the output data will be the same as the input data. The average RMSE of our model results (17.8 G) is lower than that of the conventional
![img-2.jpeg](img-2.jpeg)

Figure 3. Comparison of pixel-to-pixel magnetic fields between target data and model results. Panels (a), (b), and (c) show scatter plots of magnetic fields from target data and results from our AI-based SFT model, the conventional SFT model, and the persistent model, respectively, in a 10° resolution. Panels (d), (e), and (f) show scatter plots of the same data in a 1° resolution. Gray dashed diagonal lines indicate positive correlations between the target's and the model's magnetic fields.

Table 1

|   | RMSE (G) | FSIM | Pixel-to-pixel CC (10 × 10 Binning)  |
| --- | --- | --- | --- |
|  Our AI-based SFT model results | 17.8 | 0.46 | 0.71  |
|  Conventional SFT model results | 18.0–20.2 | 0.39–0.43 | 0.65–0.68  |
|  Persistent model results | 25.1 | 0.42 | 0.56  |

**Note.** Lower RMSE values and higher FSIM and pixel-to-pixel CC values indicate better results.

SFT model results (18.0 G) and the persistent model results (25.1 G). The average FSIM and pixel-to-pixel CC of our model results (0.46 and 0.71, respectively) are higher than those of the conventional SFT model results (0.39 and 0.68, respectively) and the persistent model results (0.42 and 0.56, respectively). The average FSIM of the conventional SFT model results is lower than that of the persistent model results due to the smoothed magnetic field distributions in the conventional SFT model results. When ηH is reduced from 425 to 250 km² s⁻¹ as suggested by R. Cameron et al. (2010), the conventional SFT model results yield the average RMSE of 19.8 G, FSIM 0.40, and pixel-to-pixel CC of 0.67. When the flux-dependent ηH is implemented, as described in J. Worden & J. Harvey (2000), the conventional SFT model results show the average RMSE of 20.2 G, FSIM of 0.43, and pixel-to-pixel CC of 0.65. As the FSIM of the conventional SFT model results increases through adjustments to the ηH parameter, its RMSE increases and pixel-to-pixel CC decreases. Our model results show higher FSIM and pixel-to-pixel CC and lower RMSE compared to the results of three different ηH configurations for the conventional SFT model.

Figure 3 shows scatter plots of pixel-to-pixel magnetic field strengths between the target data and model results for the same data sets when calculating the average pixel-to-pixel CCs in Table 1. The results of our AI-based model and the conventional model show similar pixel-to-pixel magnetic field distributions in a 10° resolution, as shown in Figures 3(a) and (b). There are several pixels for which all three models cannot produce strong magnetic fields relative to the target data, and we expect that the reason is that these models cannot predict newly emerging magnetic fields during a solar rotation. In the case of magnetic fields generated by our model that are stronger than 200 G, the pixel-to-pixel CC shows a high value of 0.90 with the target data in a 1° resolution (Figure 3(d)). The pixel-to-pixel CCs for strong magnetic field predictions from the conventional model (Figure 3(e)) and the persistent model (Figure 3(f)) in a 1° resolution are 0.48 and 0.18, respectively.

We also train and evaluate our model separately using data sets from HMI, MDI, and GONG, and the average results from
![img-3.jpeg](img-3.jpeg)

Figure 4. Comparison between MDI synoptic maps for CR 2026 (the model input) and CR 2027 (target), a conventional SFT model result, and our AI-generated data for the next solar rotation. (a)–(b) The MDI synoptic maps are input (a) and target data (b) for our deep-learning model. (c) Conventional SFT model result for the next solar rotation from the input data. (d) Our AI-generated data from the input data. Green arrows A1, A2, and A3 in (a)–(d) indicate the transported magnetic fluxes during a solar rotation, respectively. Red boxes in (a)–(d) show the location of a newly emerging active region.

![img-4.jpeg](img-4.jpeg)

Figure 5. Comparison between GONG synoptic maps for CR 2238 (the model input) and CR 2239 (target), a conventional SFT model result, and our AI-generated data for the next solar rotation. Each column, green arrows and red boxes are the same as in Figure 4.
![img-5.jpeg](img-5.jpeg)

Figure 6. A series of synoptic maps, our AI-generated data, and conventional SFT model results for three solar rotations. (a)–(d) HMI synoptic maps from CR 2195 (the model input) to CR 2198 (target). (e) HMI synoptic map for CR 2195, which is input data for our deep-learning model. (f)–(h) Iteratively generated data by our deep-learning model from the input data. (i) HMI synoptic map for CR 2195, which is an input data of the conventional SFT model. (j)–(l) A series of the SFT model results for three solar rotations. Green arrows and red boxes are the same as in Figure 4.

These data sets are mostly consistent with those in Table 1. In addition, we attempt to predict the next solar rotation synoptic maps by including synoptic maps from the last one and two solar rotations as additional model inputs, but this does not lead to significant improvements. The evaluation results of the conventional SFT model might be improved by incorporating the Air Force Data Assimilative Photospheric Flux Transport (K. S. Hickmann et al. 2015) model, which employs stochastic diffusion and data assimilation methods to reproduce more realistic magnetic features, and the Advective Flux Transport (L. Upton & D. H. Hathaway 2013) model, which utilizes vector spherical harmonics to replicate the characteristics of convective flows observed on the Sun.

### 4.2. Qualitative Comparison

We compare the results of our AI-based SFT model with the input and target data and the results of the conventional SFT model. Figure 4 shows the data and model results for MDI synoptic maps in early 2005. The input data corresponds to the MDI synoptic map for CR 2026, and the target data is the MDI synoptic map for the next solar rotation (CR 2027). Our AI-generated data produces the differential rotation and poleward flow of magnetic fields during a solar rotation well, as indicated by the green arrows (A1, A2, and A3) in Figure 4. Compared to the conventional SFT model result, our model result shows that magnetic field distributions and features are much closer to the target data. Our model, the conventional SFT model, and the persistent model cannot predict the appearance of a new active region (red box in Figure 4(b)) during the solar rotation.

Figure 5 shows the results when the model input is a GONG synoptic map for CR 2238 at the end of 2020. However, as in the previous case, none of the models—including ours, the conventional SFT model, and the persistent model—can predict the newly emerging active region (red box in Figure 5). While both our model and the conventional SFT model produce similar large-scale magnetic field distributions, our model generates small-scale magnetic features that are closer to the target data.

We can predict magnetic field data for several solar rotations ahead by running our model iteratively. Figure 6 shows HMI synoptic maps from CR 2195 to CR 2198 and the corresponding model results based on the input data for CR 2195.
![img-6.jpeg](img-6.jpeg)

Figure 7. AI-generated data from synthetic magnetic field data and comparison with the conventional SFT model results. (a) Examples of synthetic 10 bipoles' data from −0.5 to 0.5 sine latitude and from 60° to 300° longitude at regular intervals. (b) AI-generated data from the synthetic input data. (c) Conventional SFT model result for the next solar rotation from the synthetic input data. (d)–(e) Differential rotation and meridional flow speed estimated by our AI-generated data. Blue dashed lines in (b), (d), and (e) show profiles of the SFT by our deep-learning model. Brown lines in (c)–(e) show profiles of the SFT using the conventional SFT model. Black dots and horizontal gray bars in (d)–(e) represent the mean and standard deviation, respectively, of the estimated differential rotation speeds and meridional flow speeds, calculated within 1° latitude bins.

Using our AI-based SFT model, we first generate data for one solar rotation ahead from the model input, as shown in Figure 6(f). Next, we generate data for two solar rotations ahead using the AI-generated data, as shown in Figure 6(g), and then generate data for three solar rotations ahead in a similar manner, as shown in Figure 6(h). The sequence of results from our model shows reasonable agreement with the target data. The magnetic fields of the generated data diffuse and transport toward higher latitudes, similar to those of the target data, as indicated by the green arrows (A1 and A2) in Figure 6. Although neither our model nor the conventional SFT model can predict the appearance of the new active region for CR 2198 (red boxes in Figure 6), our model generates small-sale magnetic features that are closer to the target HMI synoptic data over the three solar rotations.

### 4.3. Comparison Using Synthetic Data

Here we use synthetic input data to confirm that our model represents solar SFT. The synthetic data include bipolar structures with magnetic field strengths ranging from −150 to +150 G. We make 3000 synthetic data at 10° longitude intervals from 60° to 300° and at 1° latitude intervals from −0.5 to +0.5, with two different leading magnetic field polarities for the bipolar structures. Using our AI-based SFT model, we generate data for the next solar rotation from the synthetic input data. We then calculate the longitudinal and latitudinal differences of the magnetic flux-weighted centers between the input and output data. The longitudinal differences represent differential rotation, and the latitudinal differences indicate meridional flow.

Figure 7 shows examples of 10 bipoles as input data, spaced at 60° longitude intervals and 0.1 sine-latitude intervals, and the corresponding model results. We confirm that the differential rotation and meridional flow profiles derived from our AI-generated data exhibit reasonable agreement with the conventional SFT profiles reported in several studies. The blue dashed lines in Figure 7 are SFT profiles computed from our model outputs using all 3000 synthetic input data. The brown solid lines represent profiles from the conventional SFT model parameters described in Section 3.1, providing a comparison with our AI-based SFT model results. Our results show patterns of differential rotation and meridional flow that are similar to those produced by the conventional SFT model, as shown in Figures 7(d) and (e), respectively. The brown dashed line in Figure 7(d) represents the empirical differential rotation profile measured from ephemeral regions observed during solar cycle 24 (A. S. Kutsenko 2021). The brown dashed line in Figure 7(e) shows the meridional flow profile implemented by the SFT model of C. J. Schrijver & A. M. Title (2001) and K. S. Hickmann et al. (2015).

We examine our AI-based SFT model using synthetic input data that include anomalous bipolar structures, which do not obey Hale's polarity law (G. E. Hale & S. B. Nicholson 1925) or Joy's tilt law (G. E. Hale et al. 1919). As shown in Figure 8, our model predicts the overall magnetic field distribution one solar rotation later from the anomalous bipole synthetic data, similar to the results of the conventional SFT model.
![img-7.jpeg](img-7.jpeg)

Figure 8. Comparison of our AI-generated data and the conventional SFT model results from synthetic input data, including bipolar structures with different orientations and polarities in the four configurations (Hale, anti-Hale, Joy, and anti-Joy regions) in each hemisphere. Panels (a), (b), and (c) show the synthetic input data, the results of our AI-based SFT model, and the results of the conventional SFT model, respectively.

Anomalous active regions cause significant changes in large-scale solar polar field buildup and open magnetic flux dynamics during a solar cycle (J. Jiang et al. 2014; S. Pal et al. 2023). We expect that our AI-based SFT model can handle the diversity of observed solar surface field evolution dynamics and is useful for predicting the evolution of magnetic fields in both standard and anomalous bipolar regions.

### 5. Summary and Conclusion

In this study, we have developed an AI-based SFT model to predict the evolution of solar magnetic fields during a CR. We trained and evaluated a deep-learning model based on the Pix2PixCC architecture using SDO/HMI, SOHO/MDI, and NSO/GONG synoptic maps spanning 1996–2023. Our AI-based SFT model generates magnetic field distributions for the next solar rotation, similar to the conventional SFT model and better than the persistent model in quantitative metrics such as RMSE, FSIM, and pixel-to-pixel CC. Our AI-based model offers a useful method for quick predictive assessment. Using test and synthetic data sets, we demonstrated that our model successfully implements key SFT motions, such as differential rotation and meridional flow on the solar surface. Our model also generates small-scale magnetic features better than the conventional SFT models. Our model enables iterative forecasting over three solar rotations, showing reasonable agreement with observed data. However, like the conventional SFT model and the persistent model, predicting the magnetic fields of newly emerging active regions remains challenging for our AI-based SFT model. The predictive capability of the conventional SFT model comes from the memory inherent in the slow solar surface flow processes, assuming no significant new emergences (D. Nandy et al. 2018; S. Dash et al. 2020). We expect that this inherent memory allows our AI-based model, which is trained using more than 25 yr of synoptic maps, to predict the magnetic fields of the next solar rotation similar to the results of the conventional SFT model.

Our results can be used as initial boundary conditions for solar coronal and heliospheric numerical models, potentially enhancing space-weather forecasting from several days to months in advance. Conventional SFT model results have been utilized to predict global coronal structures (D. Nandy et al. 2018), solar extreme ultraviolet irradiance (C. Henney et al. 2015), solar wind speeds (G. Barnes et al. 2023), upcoming solar activity cycles (P. Bhowmik & D. Nandy 2018), etc. We expect that our AI-generated data would provide improved predictive outcomes for these applications. However, we acknowledge the limitations of our method in predicting emerging magnetic fluxes of active regions, which makes it challenging to apply our AI-generated data to solar flare forecasts and extreme space-weather predictions during solar maximum. We expect that using input data with shorter time intervals (L. Bai et al. 2021; F. P. Ramunno et al. 2024) or additional data on convective motion below the solar surface (R. H. Cameron et al. 2012) may enhance our model's predictions of emerging magnetic fluxes. We leave this to future work. Our study shows a possibility that the AI-based model may show better results than existing empirical models. Furthermore, our new methodology, which uses a well-trained AI-based model and synthetic input data, can be applied to estimate empirical profiles from two-dimensional observations.

### Acknowledgments

We acknowledge support from the SpaceAI program (https://spaceai.kasi.re.kr), led by the Korea Astronomy and Space Science Institute (KASI) in partnership with Kyung Hee University, Korea Advanced Institute of Science & Technology (KAIST), and private-sector companies. SpaceAI provides a collaborative framework in which scientists, software engineers, industry experts as well as students/citizens all participate in as various project teams to solve multidisciplinary, community-wide questions in space science and technology with artificial intelligence. This research was supported by a Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2023-00248916), an Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (RS-2023-00234488, Development of solar synoptic magnetograms using deep-learning, 15%), the KASI under the R&D program (Project No. 2024-1-850-02) supervised by the MSIT, and the BK21 FOUR program of Graduate School, Kyung Hee University (GS-1-JO-NON-20242364). We thank the numerous researchers who have contributed to the development of SFT models, with special appreciation to A. R. Yeates et al. (2023). We also acknowledge the community efforts dedicated to developing the open-source packages used in this work.

**Software:** PyTorch (A. Paszke et al. 2019), SunPy (The SunPy Community et al. 2020), Astropy (T. P. Robitaille et al. 2013; A. M. Price-Whelan et al. 2018), SciPy (P. Virtanen et al. 2020), NumPy (C. R. Harris et al. 2020), Matplotlib (J. D. Hunter 2007), Scikit-image (S. Van der Walt et al. 2014).
## ORCID IDs

Hyun-Jin Jeong (1) https://orcid.org/0000-0003-4616-947X Mingyu Jeon (1) https://orcid.org/0009-0004-7798-5052

Daeil Kim (1) https://orcid.org/0009-0008-5566-6084
Youngjae Kim (1) https://orcid.org/0009-0009-2316-3658
Ji-Hye Baek (1) https://orcid.org/0000-0002-0230-4417
Yong-Jae Moon (1) https://orcid.org/0000-0001-6216-6944
Seonghwan Choi (1) https://orcid.org/0000-0002-1946-7327

## References

Athalathil, J. J., Vaidya, B., Kundu, S., Upendran, V., \& Cheung, M. C. 2024, ApJ, 975, 258
Atmaja, B. T., \& Akagi, M. 2021, JPhCS, 1896, 012004
Bai, L., Bi, Y., Yang, B., et al. 2021, RAA, 21, 113
Barnes, G., DeRosa, M. L., Jones, S. I., et al. 2023, ApJ, 946, 105
Bastos, B. Q., Oliveira, F. L. C., \& Milidiu, R. L. 2021, Int. J. Forecast., 37, 949
Bhowmik, P., \& Nandy, D. 2018, NatCo, 9, 5209
Cameron, R., Jiang, J., Schmitt, D., \& Schüssler, M. 2010, ApJ, 719, 264
Cameron, R. H., Schmitt, D., Jiang, J., \& Işık, E. 2012, A\&A, 542, A127
Dash, S., Bhowmik, P., Athira, B., Ghosh, N., \& Nandy, D. 2020, ApJ, 890, 37
Getachew, T., Virtanen, I., \& Mursula, K. 2019, ApJ, 874, 116
Gosain, S., Pevtsov, A., Rudenko, G., \& Anfinogentov, S. 2013, ApJ, 772, 52
Hale, G. E., Ellerman, F., Nicholson, S. B., \& Joy, A. H. 1919, ApJ, 49, 153
Hale, G. E., \& Nicholson, S. B. 1925, ApJ, 62, 270
Harris, C. R., Millman, K. J., Van Der Walt, S. J., et al. 2020, Natur, 585, 357
Henney, C., Hock, R., Schooley, A., et al. 2015, SpWea, 13, 141
Hickmann, K. S., Godinez, H. C., Henney, C. J., \& Arge, C. N. 2015, SoPh, 290, 1105
Hoeksema, J. T., \& Scherrer, P. H. 1986, SoPh, 105, 205
Howard, R., Bumba, V., \& Smith, S. F. 1969, Atlas of Solar Magnetic Fields (Washington, DC: Carnegie Institution)
Hunter, J. D. 2007, CSE, 9, 90
Jeong, H.-J., Moon, Y.-J., Park, E., \& Lee, H. 2020, ApJL, 903, L25
Jeong, H.-J., Moon, Y.-J., Park, E., Lee, H., \& Baek, J.-H. 2022, ApJS, 262, 50
Jiang, J., Hathaway, D., Cameron, R., et al. 2014, SSRv, 186, 491
Kingma, D. P., \& Ba, J. 2014, arXiv:1412.6980
Kutsenko, A. S. 2021, MNRAS, 500, 5159

Lin, L. I.-K. 1989, Biometrics, 45, 255
Lee, S., Ji, E.-Y., Moon, Y.-J., \& Park, E. 2021, SpWea, 19, e2020SW002600
Leighton, R. B. 1964, ApJ, 140, 1547
Mackay, D. H., \& Yeates, A. R. 2012, LRSP, 9, 1
Mao, X., Li, Q., Xie, H., et al. 2017, in 2017 IEEE Int. Conf. on Computer Vision (Piscataway, NJ: IEEE), 2813
Marnerides, D., Bashford-Rogers, T., \& Debattista, K. 2021, Senso, 21, 4032
Nandy, D., Baruah, Y., Bhowmik, P., et al. 2023, JASTP, 248, 106081
Nandy, D., Bhowmik, P., Yeates, A. R., et al. 2018, ApJ, 853, 72
Owens, M. J., Horbury, T., Wicks, R., et al. 2014, SpWea, 12, 395
Pal, S., Bhowmik, P., Mahajan, S. S., \& Nandy, D. 2023, ApJ, 953, 51
Park, E., Moon, Y.-J., Lee, J.-Y., et al. 2019, ApJL, 884, L23
Paszke, A., Gross, S., Massa, F., et al. 2019, in Advances in Neural Information Processing Systems 32, ed. H. Wallach et al. (NeurIPS), https://papers. neurips.cc/paper_files/paper/2019/faah/bdbca288fee7f92f2bfa9f7012727740Abstract.html
Price-Whelan, A. M., Sipőcz, B., Günther, H., et al. 2018, AJ, 156, 123
Qiang, Y., Fei, S., Jiao, Y., \& Li, L. 2020, JPhCS, 1627, 012014
Rahman, S., Shin, S., Jeong, H.-J., et al. 2023, ApJ, 948, 21
Ramunno, F. P., Jeong, H.-J., Hackstein, S., et al. 2024, arXiv:2407.11659
Rana, A., Singh, P., Valenzise, G., et al. 2019, ITIP, 29, 1285
Robitaille, T. P., Tollerud, E. J., Greenfield, P., et al. 2013, A\&A, 558, A33
Ronneberger, O., Fischer, P., \& Brox, T. 2015, Medical Image Computing and Computer-assisted Intervention-MICCAI 2015 (Cham: Springer), 234
Sara, U., Akter, M., \& Uddin, M. S. 2019, J. Comput. Commun., 7, 8
Schrijver, C. J., \& Title, A. M. 2001, ApJ, 551, 1099
Shorten, C., \& Khoshgoftaar, T. M. 2019, J. Big Data, 6, 1
Snodgrass, H. B., \& Ulrich, R. K. 1990, ApJ, 351, 309
Sun, X., Liu, Y., Hoeksema, J., Hayashi, K., \& Zhao, X. 2011, SoPh, 270, 9
The SunPy Community, Barnes, W. T., Bobra, M. G., et al. 2020, ApJ, 890, 68
Upton, L., \& Hathaway, D. H. 2013, ApJ, 780, 5
Vallejos, R., Pérez, J., Ellison, A. M., \& Richardson, A. D. 2020, SpaSt, 40, 100405
Van der Walt, S., Schönberger, J. L., Nunez-Iglesias, J., et al. 2014, PeerJ, 2, e453
Vieira, L. E. A., \& Solanki, S. K. 2010, A\&A, 509, A100
Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020, NatMe, 17, 261
Whitbread, T., Yeates, A., Muñoz-Jaramillo, A., \& Petrie, G. 2017, A\&A, 607, A76
Worden, J., \& Harvey, J. 2000, SoPh, 195, 247
Yeates, A. R., Cheung, M. C., Jiang, J., Petrovay, K., \& Wang, Y.-M. 2023, SSRv, 219, 31
Zhang, L., Zhang, L., Mou, X., \& Zhang, D. 2011, ITIP, 20, 2378"
